#!/usr/bin/env python3
"""
ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‚’æ•´ç†ã—ã¦çµ±åˆæº–å‚™
"""
import chromadb
from chromadb.config import Settings
from pathlib import Path
import json
from datetime import datetime

def organize_and_merge_collections():
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ•´ç†ã—ã¦çµ±åˆ"""
    
    source_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB"
    target_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_v4"
    
    print("ğŸ”„ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•´ç†ãƒ»çµ±åˆå‡¦ç†é–‹å§‹")
    print("=" * 60)
    print(f"ğŸ“‚ ç§»è¡Œå…ƒ: {source_db_path}")
    print(f"ğŸ“‚ ç§»è¡Œå…ˆ: {target_db_path}")
    print()
    
    try:
        # ç§»è¡Œå…ƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        source_client = chromadb.PersistentClient(
            path=source_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ç§»è¡Œå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        target_client = chromadb.PersistentClient(
            path=target_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ç§»è¡Œå…ƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç¢ºèª
        source_collections = source_client.list_collections()
        print(f"ğŸ“Š ç§»è¡Œå…ƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(source_collections)}")
        
        # æœ‰åŠ¹ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š
        valid_collections = []
        for collection in source_collections:
            doc_count = collection.count()
            print(f"   ğŸ“ {collection.name}: {doc_count}ä»¶")
            
            if doc_count > 0:
                valid_collections.append(collection)
                print(f"      âœ… æœ‰åŠ¹")
            else:
                print(f"      âŒ ç©ºã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ - ã‚¹ã‚­ãƒƒãƒ—")
        
        print(f"\nğŸ¯ çµ±åˆå¯¾è±¡: {len(valid_collections)}å€‹ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
        
        if len(valid_collections) < 2:
            print("âŒ çµ±åˆã«ã¯æœ€ä½2å€‹ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™")
            return False
        
        # çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
        merged_collection_name = "merged_iruka_knowledge"
        
        # æ—¢å­˜ã®çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°å‰Šé™¤
        try:
            target_client.delete_collection(merged_collection_name)
            print(f"ğŸ—‘ï¸  æ—¢å­˜ã®çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤")
        except:
            pass
        
        # æ–°ã—ã„çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
        print(f"ğŸ†• çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ: {merged_collection_name}")
        merged_collection = target_client.create_collection(
            name=merged_collection_name,
            metadata={
                "description": "çµ±åˆã•ã‚ŒãŸIrukaãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹",
                "created_at": datetime.now().isoformat(),
                "source_collections": ", ".join([c.name for c in valid_collections]),
                "migration_version": "v4.0"
            }
        )
        
        # å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»çµ±åˆ
        total_added = 0
        
        for collection in valid_collections:
            collection_name = collection.name
            print(f"\nğŸ“¥ '{collection_name}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
            
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆembeddingsãªã—ï¼‰
            all_data = collection.get(include=['documents', 'metadatas'])
            
            ids = all_data.get('ids', [])
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            
            print(f"   ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿: {len(ids)}ä»¶")
            
            if not ids:
                print(f"   âš ï¸  ãƒ‡ãƒ¼ã‚¿ãªã— - ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            # IDã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ ã—ã¦é‡è¤‡å›é¿
            # temp_repairã®åå‰ã‚’æ­£è¦åŒ–
            if "temp_repair" in collection_name:
                if "sister_chat_history" in collection_name:
                    source_name = "sister_chat_history"
                elif "my_sister_context" in collection_name:
                    source_name = "my_sister_context"
                else:
                    source_name = collection_name
            else:
                source_name = collection_name
            
            prefixed_ids = [f"{source_name}_{doc_id}" for doc_id in ids]
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å…ƒã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
            enhanced_metadatas = []
            for i, metadata in enumerate(metadatas):
                if metadata is None:
                    metadata = {}
                
                enhanced_metadata = {}
                
                # æ—¢å­˜ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å‹ãƒã‚§ãƒƒã‚¯ã¨å¤‰æ›
                for key, value in metadata.items():
                    if isinstance(value, (str, int, float, bool)):
                        enhanced_metadata[key] = value
                    elif isinstance(value, list):
                        enhanced_metadata[key] = ", ".join(str(v) for v in value)
                    else:
                        enhanced_metadata[key] = str(value)
                
                # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                enhanced_metadata.update({
                    'source_collection': source_name,
                    'migration_timestamp': datetime.now().isoformat(),
                    'original_id': str(ids[i])
                })
                enhanced_metadatas.append(enhanced_metadata)
            
            # ãƒãƒƒãƒã§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            batch_size = 20
            for i in range(0, len(prefixed_ids), batch_size):
                end_idx = min(i + batch_size, len(prefixed_ids))
                
                batch_ids = prefixed_ids[i:end_idx]
                batch_documents = documents[i:end_idx]
                batch_metadatas = enhanced_metadatas[i:end_idx]
                
                try:
                    merged_collection.add(
                        ids=batch_ids,
                        documents=batch_documents,
                        metadatas=batch_metadatas
                    )
                    
                    print(f"   âœ… ãƒãƒƒãƒ {i//batch_size + 1}: {len(batch_ids)}ä»¶è¿½åŠ ")
                    total_added += len(batch_ids)
                    
                except Exception as e:
                    print(f"   âŒ ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        # çµ±åˆçµæœç¢ºèª
        final_count = merged_collection.count()
        print(f"\nğŸ‰ çµ±åˆå®Œäº†!")
        print(f"ğŸ“Š ç·è¿½åŠ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_added}")
        print(f"ğŸ” æœ€çµ‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä»¶æ•°: {final_count}")
        
        # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        try:
            search_test = merged_collection.query(
                query_texts=["test search"],
                n_results=3
            )
            
            if search_test and search_test.get('documents'):
                print(f"âœ… çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ¤œç´¢ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
                print(f"ğŸ“‹ æ¤œç´¢çµæœ: {len(search_test.get('documents', []))}ä»¶")
            else:
                print(f"âŒ çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ¤œç´¢ãƒ†ã‚¹ãƒˆ: å¤±æ•—")
        
        except Exception as search_error:
            print(f"âŒ æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {search_error}")
        
        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        sample = merged_collection.get(limit=3)
        print(f"\nğŸ“„ çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒ«:")
        for i, (doc_id, document, metadata) in enumerate(zip(
            sample.get('ids', []), 
            sample.get('documents', []), 
            sample.get('metadatas', [])
        )):
            print(f"   {i+1}. ID: {doc_id}")
            if document:
                preview = document[:80] + "..." if len(document) > 80 else document
                print(f"      å†…å®¹: {preview}")
            if metadata:
                source = metadata.get('source_collection', 'Unknown')
                print(f"      å…ƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {source}")
            print()
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report = {
            "success": True,
            "migration_timestamp": datetime.now().isoformat(),
            "source_db": source_db_path,
            "target_db": target_db_path,
            "merged_collection_name": merged_collection_name,
            "source_collections": [c.name for c in valid_collections],
            "total_documents_migrated": total_added,
            "final_collection_count": final_count
        }
        
        report_file = Path(__file__).parent / f"merge_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = organize_and_merge_collections()
    
    if success:
        print("\nğŸ¯ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
    else:
        print("\nğŸ’¥ çµ±åˆå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
