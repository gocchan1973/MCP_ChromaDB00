#!/usr/bin/env python3
"""
ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ã¨è¨­å®šçµ±ä¸€ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Universal Configã‚’ä½¿ç”¨ã—ã¦ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®šã‚’çµ±ä¸€
"""

import os
import sys
import re
from pathlib import Path
from typing import List, Dict, Tuple
import shutil

# Universal Configå°å…¥
sys.path.append(str(Path(__file__).parent / "src" / "config"))
from universal_config import UniversalConfig

class HardcodingRemover:
    """ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ãƒ„ãƒ¼ãƒ«"""
    
    def __init__(self):
        self.workspace_root = UniversalConfig.WORKSPACE_ROOT
        self.replacement_patterns = self._setup_patterns()
        self.processed_files = []
        self.backup_dir = self.workspace_root / "hardcoding_removal_backup"
        
    def _setup_patterns(self) -> List[Tuple[str, str, str]]:
        """ç½®æ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨­å®š"""
        return [
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: (æ­£è¦è¡¨ç¾, ç½®æ›æ–‡å­—åˆ—, èª¬æ˜)
            (
                r'f:/å‰¯æ¥­/VSC_WorkSpace/IrukaWorkspace/shared__ChromaDB_v4',
                'str(UniversalConfig.get_chromadb_path("shared_v4"))',
                'SharedDB V4ãƒ‘ã‚¹'
            ),
            (
                r'f:/å‰¯æ¥­/VSC_WorkSpace/IrukaWorkspace/shared__ChromaDB',
                'str(UniversalConfig.get_chromadb_path("shared_v4"))',
                'SharedDBãƒ‘ã‚¹'
            ),
            (
                r'f:/å‰¯æ¥­/VSC_WorkSpace/IrukaWorkspace/shared_Chromadb/chromadb_data',
                'str(UniversalConfig.get_chromadb_path("shared_legacy"))',
                'Legacy ChromaDBãƒ‘ã‚¹'
            ),
            (
                r'f:/å‰¯æ¥­/VSC_WorkSpace/MySisterDB/chromadb_data',
                'str(UniversalConfig.get_chromadb_path("mysister_local"))',
                'MySisterDB ChromaDBãƒ‘ã‚¹'
            ),
            (
                r'f:/å‰¯æ¥­/VSC_WorkSpace/MCP_ChromaDB00/src/enhanced_main\.py',
                'str(UniversalConfig.get_workspace_path("MCP_ChromaDB00", "src", "enhanced_main.py"))',
                'MCP Server ãƒ‘ã‚¹'
            ),
            (
                r'f:/å‰¯æ¥­/VSC_WorkSpace/MySisterDB',
                'str(UniversalConfig.get_workspace_path("MySisterDB"))',
                'MySisterDBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
            ),
            (
                r'f:/å‰¯æ¥­/VSC_WorkSpace/IrukaProjectII',
                'str(UniversalConfig.get_workspace_path("IrukaProjectII"))',
                'IrukaProjectIIãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
            ),
            (
                r'"sister_chat_history_V4"',
                'UniversalConfig.get_collection_name()',
                'ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å V4'
            ),
            (
                r'"sister_chat_history"',
                'UniversalConfig.get_collection_name("legacy")',
                'ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å Legacy'
            ),
            (
                r'sister_chat_history_v4',
                'UniversalConfig.get_collection_name()',
                'ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å v4'
            )
        ]
    
    def create_backup(self, file_path: Path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ"""
        self.backup_dir.mkdir(exist_ok=True)
        
        # ç›¸å¯¾ãƒ‘ã‚¹ã‚’ä½œæˆ
        rel_path = file_path.relative_to(self.workspace_root)
        backup_path = self.backup_dir / rel_path
        backup_path.parent.mkdir(parents=True, exist_ok=True)
        
        shutil.copy2(file_path, backup_path)
        print(f"ğŸ“‹ Backup created: {backup_path}")
    
    def add_universal_config_import(self, content: str, file_path: Path) -> str:
        """Universal Configã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ """
        # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã®ã¿
        if not file_path.suffix == '.py':
            return content
        
        # æ—¢ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if 'universal_config' in content or 'UniversalConfig' in content:
            return content
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆéƒ¨åˆ†ã‚’è¦‹ã¤ã‘ã¦è¿½åŠ 
        lines = content.split('\n')
        import_insert_line = 0
        
        # shebang, docstring, encodingç­‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹
        for i, line in enumerate(lines):
            if line.strip().startswith('import ') or line.strip().startswith('from '):
                import_insert_line = i
                break
            elif line.strip() and not line.strip().startswith('#') and not line.strip().startswith('"""') and not line.strip().startswith("'''"):
                import_insert_line = i
                break
        
        # Universal Configã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
        import_lines = [
            "",
            "# Universal Configå°å…¥",
            "import sys",
            "from pathlib import Path",
            "sys.path.append(str(Path(__file__).resolve().parents[2] / 'MCP_ChromaDB00' / 'src' / 'config'))",
            "from universal_config import UniversalConfig",
            ""
        ]
        
        # é©åˆ‡ãªä½ç½®ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æŒ¿å…¥
        for idx, import_line in enumerate(import_lines):
            lines.insert(import_insert_line + idx, import_line)
        
        return '\n'.join(lines)
    
    def process_file(self, file_path: Path) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é™¤å»"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            modifications = []
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã¨ç½®æ›
            for pattern, replacement, description in self.replacement_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    content = re.sub(pattern, replacement, content)
                    modifications.append(f"  - {description}: {len(matches)} ç®‡æ‰€")
            
            # å¤‰æ›´ãŒã‚ã£ãŸå ´åˆ
            if content != original_content:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
                self.create_backup(file_path)
                
                # Universal Configã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
                content = self.add_universal_config_import(content, file_path)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print(f"âœ… ä¿®æ­£å®Œäº†: {file_path}")
                for mod in modifications:
                    print(mod)
                
                self.processed_files.append({
                    'file': str(file_path),
                    'modifications': modifications
                })
                
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return False
    
    def scan_and_process(self) -> Dict[str, int]:
        """ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦å‡¦ç†"""
        stats = {
            'scanned': 0,
            'modified': 0,
            'skipped': 0,
            'errors': 0
        }
        
        # å‡¦ç†å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        target_dirs = [
            "MySisterDB",
            "IrukaProjectII", 
            "MCP_ChromaDB00"
        ]
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            "**/__pycache__/**",
            "**/node_modules/**",
            "**/.git/**",
            "**/backup/**",
            "**/hardcoding_removal_backup/**",
            "**/*.pyc",
            "**/*.log"
        ]
        
        for dir_name in target_dirs:
            dir_path = self.workspace_root / dir_name
            if not dir_path.exists():
                continue
                
            print(f"\nğŸ” å‡¦ç†ä¸­: {dir_name}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
            for file_path in dir_path.rglob("*"):
                if not file_path.is_file():
                    continue
                
                # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                if any(file_path.match(pattern) for pattern in exclude_patterns):
                    continue
                
                # å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ« (.py, .json, .md)
                if file_path.suffix not in ['.py', '.json', '.md']:
                    continue
                
                stats['scanned'] += 1
                
                try:
                    if self.process_file(file_path):
                        stats['modified'] += 1
                    else:
                        stats['skipped'] += 1
                except Exception as e:
                    stats['errors'] += 1
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        return stats
    
    def generate_report(self, stats: Dict[str, int]):
        """å‡¦ç†çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print(f"\n{'='*60}")
        print("ğŸ“Š ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»çµæœ")
        print(f"{'='*60}")
        print(f"ã‚¹ã‚­ãƒ£ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['scanned']}")
        print(f"ä¿®æ­£ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['modified']}")
        print(f"ã‚¹ã‚­ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['skipped']}")
        print(f"ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['errors']}")
        print(f"\nğŸ“‚ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.backup_dir}")
        
        if self.processed_files:
            print(f"\nğŸ“ ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
            for item in self.processed_files:
                print(f"  - {item['file']}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_path = self.workspace_root / "hardcoding_removal_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("="*50 + "\n\n")
            f.write(f"å‡¦ç†æ—¥æ™‚: {import datetime; datetime.datetime.now()}\n")
            f.write(f"ã‚¹ã‚­ãƒ£ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['scanned']}\n")
            f.write(f"ä¿®æ­£ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['modified']}\n")
            f.write(f"ã‚¹ã‚­ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['skipped']}\n")
            f.write(f"ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['errors']}\n\n")
            
            f.write("ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\n")
            for item in self.processed_files:
                f.write(f"  - {item['file']}\n")
                for mod in item['modifications']:
                    f.write(f"    {mod}\n")
        
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é™¤å»ãƒ„ãƒ¼ãƒ«é–‹å§‹")
    print("Universal Configçµ±åˆãƒ—ãƒ­ã‚»ã‚¹")
    print("="*60)
    
    # Universal Configè¨­å®šç¢ºèª
    print(f"ğŸ“‚ ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ«ãƒ¼ãƒˆ: {UniversalConfig.WORKSPACE_ROOT}")
    print(f"ğŸ—„ï¸  ç¾åœ¨ã®ChrmaDBãƒ‘ã‚¹: {UniversalConfig.get_chromadb_path()}")
    print(f"ğŸ“‹ ç¾åœ¨ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {UniversalConfig.get_collection_name()}")
    
    # å‡¦ç†å®Ÿè¡Œ
    remover = HardcodingRemover()
    stats = remover.scan_and_process()
    remover.generate_report(stats)
    
    print(f"\nâœ… å‡¦ç†å®Œäº†!")
    print("âš ï¸  ä¿®æ­£å¾Œã¯å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()
