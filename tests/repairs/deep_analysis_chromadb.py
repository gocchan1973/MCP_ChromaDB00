#!/usr/bin/env python3
"""
ChromaDBçµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ·±å±¤ç²¾æŸ»ãƒ„ãƒ¼ãƒ«
åŸºæœ¬æƒ…å ±ã‹ã‚‰SQLiteå†…éƒ¨æ§‹é€ ã€embeddingsè©³ç´°ã¾ã§å…¨å±¤ã‚’ç²¾æŸ»
"""
import chromadb
from chromadb.config import Settings
from pathlib import Path
import json
from datetime import datetime
import sqlite3
import numpy as np
import hashlib
import os

def deep_analysis_chromadb():
    """ChromaDBã®æ·±å±¤ç²¾æŸ»å®Ÿè¡Œ"""
    
    db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_v4"
    
    print("ğŸ”¬ ChromaDBçµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ·±å±¤ç²¾æŸ»é–‹å§‹")
    print("=" * 80)
    print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {db_path}")
    print(f"ğŸ•’ ç²¾æŸ»é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    analysis_results = {
        "analysis_timestamp": datetime.now().isoformat(),
        "database_path": db_path,
        "layers": {}
    }
    
    try:
        # Layer 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤ã®åˆ†æ
        print("ğŸ“ Layer 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤åˆ†æ")
        print("-" * 60)
        
        fs_analysis = analyze_filesystem_layer(db_path)
        analysis_results["layers"]["filesystem"] = fs_analysis
        print()
        
        # Layer 2: ChromaDB APIå±¤ã®åˆ†æ
        print("ğŸ”Œ Layer 2: ChromaDB APIå±¤åˆ†æ")
        print("-" * 60)
        
        api_analysis = analyze_chromadb_api_layer(db_path)
        analysis_results["layers"]["chromadb_api"] = api_analysis
        print()
        
        # Layer 3: SQLiteå†…éƒ¨æ§‹é€ åˆ†æ
        print("ğŸ—„ï¸ Layer 3: SQLiteå†…éƒ¨æ§‹é€ åˆ†æ")
        print("-" * 60)
        
        sqlite_analysis = analyze_sqlite_internal_layer(db_path)
        analysis_results["layers"]["sqlite_internal"] = sqlite_analysis
        print()
        
        # Layer 4: ãƒ™ã‚¯ãƒˆãƒ«ãƒ»embeddingsè©³ç´°åˆ†æ
        print("ğŸ§® Layer 4: ãƒ™ã‚¯ãƒˆãƒ«ãƒ»embeddingsè©³ç´°åˆ†æ")
        print("-" * 60)
        
        vector_analysis = analyze_vector_embeddings_layer(db_path)
        analysis_results["layers"]["vector_embeddings"] = vector_analysis
        print()
        
        # Layer 5: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ»å“è³ªåˆ†æ
        print("âœ… Layer 5: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ»å“è³ªåˆ†æ")
        print("-" * 60)
        
        integrity_analysis = analyze_data_integrity_layer(db_path)
        analysis_results["layers"]["data_integrity"] = integrity_analysis
        print()
        
        # ç·åˆè¨ºæ–­çµæœ
        print("ğŸ¯ ç·åˆè¨ºæ–­çµæœ")
        print("=" * 60)
        
        overall_health = generate_overall_health_report(analysis_results)
        analysis_results["overall_health"] = overall_health
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = Path(__file__).parent / f"deep_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        
        return analysis_results
        
    except Exception as e:
        print(f"âŒ æ·±å±¤ç²¾æŸ»ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None

def analyze_filesystem_layer(db_path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤ã®åˆ†æ"""
    
    fs_analysis = {}
    
    try:
        db_dir = Path(db_path)
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {db_dir}")
        
        if not db_dir.exists():
            print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return {"error": "Directory does not exist"}
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹ã®è©³ç´°åˆ†æ
        files = list(db_dir.iterdir())
        print(f"ğŸ“‹ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
        
        file_details = []
        total_size = 0
        
        for file_path in files:
            if file_path.is_file():
                size = file_path.stat().st_size
                total_size += size
                modified = datetime.fromtimestamp(file_path.stat().st_mtime)
                
                file_info = {
                    "name": file_path.name,
                    "size_bytes": size,
                    "size_mb": round(size / (1024 * 1024), 2),
                    "modified": modified.isoformat(),
                    "extension": file_path.suffix
                }
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ï¼ˆå°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
                if size < 50 * 1024 * 1024:  # 50MBæœªæº€
                    try:
                        with open(file_path, 'rb') as f:
                            file_hash = hashlib.md5(f.read()).hexdigest()
                        file_info["md5_hash"] = file_hash
                    except:
                        file_info["md5_hash"] = "calculation_failed"
                
                file_details.append(file_info)
                print(f"   ğŸ“„ {file_path.name}: {file_info['size_mb']} MB")
        
        fs_analysis = {
            "directory_exists": True,
            "total_files": len(files),
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "files": file_details
        }
        
        print(f"ğŸ“Š ç·ãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚º: {fs_analysis['total_size_mb']} MB")
        
        # æ¨©é™ãƒã‚§ãƒƒã‚¯
        try:
            test_file = db_dir / "test_permissions.tmp"
            test_file.write_text("test")
            test_file.unlink()
            fs_analysis["write_permissions"] = True
            print(f"âœ… æ›¸ãè¾¼ã¿æ¨©é™: æ­£å¸¸")
        except:
            fs_analysis["write_permissions"] = False
            print(f"âŒ æ›¸ãè¾¼ã¿æ¨©é™: ã‚¨ãƒ©ãƒ¼")
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        fs_analysis = {"error": str(e)}
    
    return fs_analysis

def analyze_chromadb_api_layer(db_path):
    """ChromaDB APIå±¤ã®åˆ†æ"""
    
    api_analysis = {}
    
    try:
        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶š
        client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        print(f"âœ… ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶š: æˆåŠŸ")
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—
        collections = client.list_collections()
        print(f"ğŸ“š ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(collections)}")
        
        collections_info = []
        
        for collection in collections:
            print(f"\nğŸ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åˆ†æ: {collection.name}")
            
            collection_info = {
                "name": collection.name,
                "id": str(collection.id),
                "metadata": collection.metadata
            }
            
            # åŸºæœ¬çµ±è¨ˆ
            doc_count = collection.count()
            print(f"   ğŸ“Š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count}")
            collection_info["document_count"] = doc_count
            
            if doc_count > 0:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚µãƒ³ãƒ—ãƒ«åˆ†æ
                sample_size = min(10, doc_count)
                sample_data = collection.get(
                    include=['documents', 'metadatas'],
                    limit=sample_size
                )
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé•·çµ±è¨ˆ
                doc_lengths = [len(doc) for doc in sample_data.get('documents', [])]
                if doc_lengths:
                    collection_info["document_stats"] = {
                        "sample_size": len(doc_lengths),
                        "min_length": min(doc_lengths),
                        "max_length": max(doc_lengths),
                        "avg_length": round(sum(doc_lengths) / len(doc_lengths), 2)
                    }
                    print(f"   ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé•·: {min(doc_lengths)}-{max(doc_lengths)} (å¹³å‡: {collection_info['document_stats']['avg_length']})")
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ
                metadatas = sample_data.get('metadatas', [])
                if metadatas:
                    all_keys = set()
                    for meta in metadatas:
                        if meta:
                            all_keys.update(meta.keys())
                    
                    collection_info["metadata_keys"] = sorted(list(all_keys))
                    print(f"   ğŸ·ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼æ•°: {len(all_keys)}")
                    print(f"   ğŸ”‘ ã‚­ãƒ¼ä¸€è¦§: {', '.join(sorted(list(all_keys))[:5])}{'...' if len(all_keys) > 5 else ''}")
                
                # æ¤œç´¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
                try:
                    search_results = collection.query(
                        query_texts=["test search functionality"],
                        n_results=min(3, doc_count)
                    )
                    
                    if search_results and search_results.get('documents'):
                        collection_info["search_functional"] = True
                        collection_info["search_results_count"] = len(search_results.get('documents', []))
                        print(f"   âœ… æ¤œç´¢æ©Ÿèƒ½: æ­£å¸¸ ({collection_info['search_results_count']}ä»¶)")
                        
                        # æ¤œç´¢è·é›¢åˆ†æ
                        distances = search_results.get('distances', [])
                        if distances and distances[0]:
                            dist_stats = {
                                "min_distance": float(min(distances[0])),
                                "max_distance": float(max(distances[0])),
                                "avg_distance": float(sum(distances[0]) / len(distances[0]))
                            }
                            collection_info["search_distance_stats"] = dist_stats
                            print(f"   ğŸ“ æ¤œç´¢è·é›¢: {dist_stats['min_distance']:.4f}-{dist_stats['max_distance']:.4f}")
                    else:
                        collection_info["search_functional"] = False
                        print(f"   âŒ æ¤œç´¢æ©Ÿèƒ½: ç•°å¸¸")
                
                except Exception as search_error:
                    collection_info["search_functional"] = False
                    collection_info["search_error"] = str(search_error)
                    print(f"   âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {search_error}")
            
            collections_info.append(collection_info)
        
        api_analysis = {
            "connection_successful": True,
            "collections_count": len(collections),
            "collections": collections_info
        }
        
    except Exception as e:
        print(f"âŒ ChromaDB APIåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        api_analysis = {"connection_successful": False, "error": str(e)}
    
    return api_analysis

def analyze_sqlite_internal_layer(db_path):
    """SQLiteå†…éƒ¨æ§‹é€ ã®åˆ†æ"""
    
    sqlite_analysis = {}
    
    try:
        sqlite_file = Path(db_path) / "chroma.sqlite3"
        
        if not sqlite_file.exists():
            print(f"âŒ SQLiteãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sqlite_file}")
            return {"error": "SQLite file not found"}
        
        print(f"ğŸ“Š SQLiteãƒ•ã‚¡ã‚¤ãƒ«: {sqlite_file}")
        
        with sqlite3.connect(sqlite_file) as conn:
            cursor = conn.cursor()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±
            cursor.execute("PRAGMA database_list;")
            db_info = cursor.fetchall()
            print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±: {db_info}")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã¨è©³ç´°
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = [row[0] for row in cursor.fetchall()]
            print(f"ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
            
            tables_info = {}
            
            for table in tables:
                print(f"\nğŸ” ãƒ†ãƒ¼ãƒ–ãƒ«åˆ†æ: {table}")
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ 
                cursor.execute(f"PRAGMA table_info([{table}]);")
                columns = cursor.fetchall()
                column_info = [
                    {
                        "cid": col[0],
                        "name": col[1],
                        "type": col[2],
                        "notnull": bool(col[3]),
                        "default_value": col[4],
                        "pk": bool(col[5])
                    }
                    for col in columns
                ]
                
                # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
                cursor.execute(f"SELECT COUNT(*) FROM [{table}];")
                record_count = cursor.fetchone()[0]
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºï¼ˆæ¨å®šï¼‰
                cursor.execute(f"SELECT COUNT(*) FROM pragma_table_info('{table}');")
                column_count = cursor.fetchone()[0]
                
                table_info = {
                    "record_count": record_count,
                    "column_count": column_count,
                    "columns": column_info
                }
                
                print(f"   ğŸ“Š ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {record_count}")
                print(f"   ğŸ›ï¸ ã‚«ãƒ©ãƒ æ•°: {column_count}")
                
                # é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã®è©³ç´°åˆ†æ
                if table in ['collections', 'embeddings', 'embedding_metadata']:
                    print(f"   ğŸ”¬ è©³ç´°åˆ†æä¸­...")
                    
                    if table == 'collections':
                        cursor.execute("SELECT id, name, dimension FROM collections;")
                        collection_details = cursor.fetchall()
                        table_info["collection_details"] = [
                            {"id": row[0], "name": row[1], "dimension": row[2]}
                            for row in collection_details
                        ]
                        print(f"      ğŸ“š ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è©³ç´°: {len(collection_details)}ä»¶")
                    
                    elif table == 'embeddings':
                        cursor.execute("SELECT segment_id, COUNT(*) FROM embeddings GROUP BY segment_id;")
                        embedding_distribution = cursor.fetchall()
                        table_info["embedding_distribution"] = [
                            {"segment_id": row[0], "count": row[1]}
                            for row in embedding_distribution
                        ]
                        print(f"      ğŸ§® ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥åˆ†å¸ƒ: {len(embedding_distribution)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
                    
                    elif table == 'embedding_metadata':
                        cursor.execute("SELECT key, COUNT(*) FROM embedding_metadata GROUP BY key LIMIT 10;")
                        metadata_distribution = cursor.fetchall()
                        table_info["metadata_key_distribution"] = [
                            {"key": row[0], "count": row[1]}
                            for row in metadata_distribution
                        ]
                        print(f"      ğŸ·ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼åˆ†å¸ƒ: {len(metadata_distribution)}ç¨®é¡")
                
                tables_info[table] = table_info
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ†æ
            cursor.execute("SELECT name, tbl_name, sql FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%';")
            indexes = cursor.fetchall()
            
            indexes_info = []
            for index in indexes:
                index_info = {
                    "name": index[0],
                    "table": index[1],
                    "sql": index[2]
                }
                indexes_info.append(index_info)
            
            print(f"\nğŸ” ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(indexes_info)}")
            for idx in indexes_info:
                print(f"   ğŸ“‡ {idx['name']} -> {idx['table']}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ
            cursor.execute("PRAGMA page_count;")
            page_count = cursor.fetchone()[0]
            
            cursor.execute("PRAGMA page_size;")
            page_size = cursor.fetchone()[0]
            
            total_size = page_count * page_size
            print(f"\nğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ:")
            print(f"   ğŸ“„ ãƒšãƒ¼ã‚¸æ•°: {page_count}")
            print(f"   ğŸ“ ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {page_size} bytes")
            print(f"   ğŸ’¾ ç·ã‚µã‚¤ã‚º: {total_size / (1024*1024):.2f} MB")
            
            sqlite_analysis = {
                "file_exists": True,
                "file_size_mb": round(sqlite_file.stat().st_size / (1024*1024), 2),
                "page_count": page_count,
                "page_size": page_size,
                "total_size_mb": round(total_size / (1024*1024), 2),
                "tables_count": len(tables),
                "tables": tables_info,
                "indexes_count": len(indexes_info),
                "indexes": indexes_info
            }
    
    except Exception as e:
        print(f"âŒ SQLiteåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        sqlite_analysis = {"error": str(e)}
    
    return sqlite_analysis

def analyze_vector_embeddings_layer(db_path):
    """ãƒ™ã‚¯ãƒˆãƒ«ãƒ»embeddingsè©³ç´°åˆ†æ"""
    
    vector_analysis = {}
    
    try:
        client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        collections = client.list_collections()
        
        for collection in collections:
            print(f"ğŸ§® ãƒ™ã‚¯ãƒˆãƒ«åˆ†æ: {collection.name}")
            
            collection_vector_info = {}
            doc_count = collection.count()
            
            if doc_count > 0:
                # å®‰å…¨ãªembeddingsåˆ†æï¼ˆç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã‚’é¿ã‘ã‚‹ï¼‰
                print(f"   ğŸ” ãƒ™ã‚¯ãƒˆãƒ«æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆä¸­...")
                
                # 1. æ¤œç´¢ãƒ†ã‚¹ãƒˆã§ãƒ™ã‚¯ãƒˆãƒ«æ©Ÿèƒ½ç¢ºèª
                try:
                    # ç•°ãªã‚‹ã‚¯ã‚¨ãƒªã§ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆ
                    test_queries = [
                        "Python programming",
                        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ",
                        "æŠ€è¡“æ–‡æ›¸",
                        "ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º"
                    ]
                    
                    search_results = []
                    for query in test_queries:
                        try:
                            result = collection.query(
                                query_texts=[query],
                                n_results=min(5, doc_count)
                            )
                            
                            if result and result.get('documents'):
                                search_results.append({
                                    "query": query,
                                    "results_count": len(result.get('documents', [])),
                                    "distances": result.get('distances', [[]])[0] if result.get('distances') else []
                                })
                        except Exception as query_error:
                            search_results.append({
                                "query": query,
                                "error": str(query_error)
                            })
                    
                    collection_vector_info["search_tests"] = search_results
                    successful_searches = sum(1 for r in search_results if "error" not in r)
                    print(f"   âœ… æ¤œç´¢ãƒ†ã‚¹ãƒˆæˆåŠŸç‡: {successful_searches}/{len(test_queries)}")
                    
                    # 2. è·é›¢çµ±è¨ˆåˆ†æ
                    all_distances = []
                    for result in search_results:
                        if "distances" in result and result["distances"]:
                            all_distances.extend(result["distances"])
                    
                    if all_distances:
                        distance_stats = {
                            "count": len(all_distances),
                            "min": float(min(all_distances)),
                            "max": float(max(all_distances)),
                            "mean": float(sum(all_distances) / len(all_distances)),
                            "median": float(sorted(all_distances)[len(all_distances)//2])
                        }
                        collection_vector_info["distance_statistics"] = distance_stats
                        print(f"   ğŸ“Š è·é›¢çµ±è¨ˆ: å¹³å‡={distance_stats['mean']:.4f}, ç¯„å›²={distance_stats['min']:.4f}-{distance_stats['max']:.4f}")
                    
                    # 3. é¡ä¼¼åº¦åˆ†å¸ƒãƒ†ã‚¹ãƒˆ
                    print(f"   ğŸ”„ é¡ä¼¼åº¦åˆ†å¸ƒãƒ†ã‚¹ãƒˆä¸­...")
                    
                    # ãƒ©ãƒ³ãƒ€ãƒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                    sample_data = collection.get(limit=min(10, doc_count))
                    sample_docs = sample_data.get('documents', [])
                    
                    if sample_docs:
                        similarity_matrix = []
                        for i, doc in enumerate(sample_docs[:5]):  # æœ€åˆã®5ä»¶ã§é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
                            try:
                                similar_results = collection.query(
                                    query_texts=[doc[:200]],  # æœ€åˆã®200æ–‡å­—ã§æ¤œç´¢
                                    n_results=min(doc_count, 10)
                                )
                                
                                if similar_results and similar_results.get('distances'):
                                    distances = similar_results.get('distances', [[]])[0]
                                    similarity_matrix.append(distances[:5])  # ä¸Šä½5ä»¶ã®è·é›¢
                            except Exception as sim_error:
                                print(f"   âš ï¸  é¡ä¼¼åº¦ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {sim_error}")
                        
                        if similarity_matrix:
                            collection_vector_info["similarity_matrix_sample"] = similarity_matrix
                            print(f"   ğŸ“ˆ é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹: {len(similarity_matrix)}x{len(similarity_matrix[0]) if similarity_matrix else 0}")
                    
                    # 4. ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ¨å®šï¼ˆé–“æ¥çš„ï¼‰
                    try:
                        # æ¤œç´¢çµæœã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒã‚’æ¨å®š
                        # ã“ã‚Œã¯ç›´æ¥çš„ã§ã¯ãªã„ãŒã€ChromaDBã®å†…éƒ¨å‹•ä½œã‹ã‚‰æ¨å®šå¯èƒ½
                        print(f"   ğŸ”¢ ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ¨å®šä¸­...")
                        
                        # SQLiteã‹ã‚‰ç›´æ¥ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±ã‚’å–å¾—ï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰
                        sqlite_file = Path(db_path) / "chroma.sqlite3"
                        if sqlite_file.exists():
                            with sqlite3.connect(sqlite_file) as conn:
                                cursor = conn.cursor()
                                
                                # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³IDã‚’å–å¾—
                                cursor.execute("SELECT id, dimension FROM collections WHERE name = ?", (collection.name,))
                                collection_db_info = cursor.fetchone()
                                
                                if collection_db_info:
                                    collection_vector_info["collection_id"] = collection_db_info[0]
                                    collection_vector_info["vector_dimension"] = collection_db_info[1]
                                    print(f"   ğŸ“ ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {collection_db_info[1]}")
                                
                                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±
                                cursor.execute("SELECT id, type FROM segments WHERE collection = ?", (collection_db_info[0],))
                                segments = cursor.fetchall()
                                collection_vector_info["segments"] = [
                                    {"id": seg[0], "type": seg[1]} for seg in segments
                                ]
                                print(f"   ğŸ§© ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments)}")
                    
                    except Exception as dim_error:
                        print(f"   âš ï¸  æ¬¡å…ƒæ¨å®šã‚¨ãƒ©ãƒ¼: {dim_error}")
                
                except Exception as vector_error:
                    print(f"   âŒ ãƒ™ã‚¯ãƒˆãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {vector_error}")
                    collection_vector_info["error"] = str(vector_error)
            
            else:
                collection_vector_info["empty_collection"] = True
                print(f"   ğŸ“­ ç©ºã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
            
            vector_analysis[collection.name] = collection_vector_info
    
    except Exception as e:
        print(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        vector_analysis = {"error": str(e)}
    
    return vector_analysis

def analyze_data_integrity_layer(db_path):
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ»å“è³ªåˆ†æ"""
    
    integrity_analysis = {}
    
    try:
        client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        collections = client.list_collections()
        print(f"ğŸ” ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å¯¾è±¡: {len(collections)}ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
        
        for collection in collections:
            print(f"\nâœ… æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: {collection.name}")
            
            collection_integrity = {}
            doc_count = collection.count()
            
            if doc_count > 0:
                # 1. IDé‡è¤‡ãƒã‚§ãƒƒã‚¯
                all_data = collection.get()
                ids = all_data.get('ids', [])
                documents = all_data.get('documents', [])
                metadatas = all_data.get('metadatas', [])
                
                unique_ids = set(ids)
                collection_integrity["id_duplicates"] = len(ids) - len(unique_ids)
                print(f"   ğŸ†” IDé‡è¤‡: {collection_integrity['id_duplicates']}ä»¶")
                
                # 2. ãƒ‡ãƒ¼ã‚¿æ¬ æãƒã‚§ãƒƒã‚¯
                missing_documents = sum(1 for doc in documents if not doc or doc.strip() == "")
                missing_metadata = sum(1 for meta in metadatas if meta is None)
                
                collection_integrity["missing_documents"] = missing_documents
                collection_integrity["missing_metadata"] = missing_metadata
                print(f"   ğŸ“„ ç©ºãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {missing_documents}ä»¶")
                print(f"   ğŸ·ï¸  æ¬ æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {missing_metadata}ä»¶")
                
                # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                metadata_keys_per_doc = []
                source_collection_distribution = {}
                
                for meta in metadatas:
                    if meta:
                        metadata_keys_per_doc.append(len(meta.keys()))
                        source = meta.get('source_collection', 'unknown')
                        source_collection_distribution[source] = source_collection_distribution.get(source, 0) + 1
                
                if metadata_keys_per_doc:
                    collection_integrity["metadata_consistency"] = {
                        "min_keys": min(metadata_keys_per_doc),
                        "max_keys": max(metadata_keys_per_doc),
                        "avg_keys": sum(metadata_keys_per_doc) / len(metadata_keys_per_doc)
                    }
                    print(f"   ğŸ”‘ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼æ•°: {collection_integrity['metadata_consistency']['min_keys']}-{collection_integrity['metadata_consistency']['max_keys']}")
                
                collection_integrity["source_distribution"] = source_collection_distribution
                print(f"   ğŸ“Š å…ƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åˆ†å¸ƒ: {source_collection_distribution}")
                
                # 4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå“è³ªãƒã‚§ãƒƒã‚¯
                doc_quality = {
                    "very_short": 0,  # < 50æ–‡å­—
                    "short": 0,       # 50-200æ–‡å­—
                    "medium": 0,      # 200-1000æ–‡å­—
                    "long": 0,        # 1000-5000æ–‡å­—
                    "very_long": 0    # > 5000æ–‡å­—
                }
                
                for doc in documents:
                    if doc:
                        length = len(doc)
                        if length < 50:
                            doc_quality["very_short"] += 1
                        elif length < 200:
                            doc_quality["short"] += 1
                        elif length < 1000:
                            doc_quality["medium"] += 1
                        elif length < 5000:
                            doc_quality["long"] += 1
                        else:
                            doc_quality["very_long"] += 1
                
                collection_integrity["document_quality_distribution"] = doc_quality
                print(f"   ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé•·åˆ†å¸ƒ: çŸ­={doc_quality['very_short']}, ä¸­={doc_quality['medium']}, é•·={doc_quality['long']}")
                
                # 5. æ¤œç´¢å“è³ªãƒ†ã‚¹ãƒˆ
                search_quality_tests = []
                test_cases = [
                    ("exact_match", documents[0][:100] if documents else ""),
                    ("partial_match", documents[0][:50] if documents else ""),
                    ("semantic_search", "æŠ€è¡“ ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°"),
                    ("empty_query", ""),
                    ("long_query", "ã“ã‚Œã¯éå¸¸ã«é•·ã„ã‚¯ã‚¨ãƒªã§ã™ã€‚" * 10)
                ]
                
                for test_name, query in test_cases:
                    if query:  # ç©ºã§ãªã„å ´åˆã®ã¿ãƒ†ã‚¹ãƒˆ
                        try:
                            result = collection.query(
                                query_texts=[query],
                                n_results=min(3, doc_count)
                            )
                            
                            search_quality_tests.append({
                                "test": test_name,
                                "success": bool(result and result.get('documents')),
                                "results_count": len(result.get('documents', [])) if result else 0
                            })
                        except Exception as search_error:
                            search_quality_tests.append({
                                "test": test_name,
                                "success": False,
                                "error": str(search_error)
                            })
                
                collection_integrity["search_quality_tests"] = search_quality_tests
                successful_tests = sum(1 for t in search_quality_tests if t.get('success', False))
                print(f"   ğŸ” æ¤œç´¢å“è³ªãƒ†ã‚¹ãƒˆ: {successful_tests}/{len(search_quality_tests)} æˆåŠŸ")
                
                # 6. ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
                total_score = 100
                
                # ãƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—
                if collection_integrity["id_duplicates"] > 0:
                    total_score -= 20
                
                if collection_integrity["missing_documents"] > doc_count * 0.1:  # 10%ä»¥ä¸ŠãŒç©º
                    total_score -= 15
                
                if collection_integrity["missing_metadata"] > doc_count * 0.1:
                    total_score -= 10
                
                search_success_rate = successful_tests / len(search_quality_tests) if search_quality_tests else 0
                if search_success_rate < 0.8:  # 80%æœªæº€ã®æˆåŠŸç‡
                    total_score -= 20
                
                collection_integrity["quality_score"] = max(0, total_score)
                print(f"   ğŸ¯ å“è³ªã‚¹ã‚³ã‚¢: {collection_integrity['quality_score']}/100")
            
            else:
                collection_integrity["empty_collection"] = True
                collection_integrity["quality_score"] = 0
                print(f"   ğŸ“­ ç©ºã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
            
            integrity_analysis[collection.name] = collection_integrity
    
    except Exception as e:
        print(f"âŒ æ•´åˆæ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        integrity_analysis = {"error": str(e)}
    
    return integrity_analysis

def generate_overall_health_report(analysis_results):
    """ç·åˆãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    health_report = {}
    
    try:
        # å„å±¤ã®å¥å…¨æ€§è©•ä¾¡
        layer_scores = {}
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤
        fs_layer = analysis_results["layers"].get("filesystem", {})
        if fs_layer.get("write_permissions") and not fs_layer.get("error"):
            layer_scores["filesystem"] = 100
        else:
            layer_scores["filesystem"] = 50
        
        # ChromaDB APIå±¤
        api_layer = analysis_results["layers"].get("chromadb_api", {})
        if api_layer.get("connection_successful"):
            api_score = 80
            collections = api_layer.get("collections", [])
            functional_collections = sum(1 for c in collections if c.get("search_functional", False))
            if collections and functional_collections == len(collections):
                api_score = 100
            layer_scores["chromadb_api"] = api_score
        else:
            layer_scores["chromadb_api"] = 0
        
        # SQLiteå†…éƒ¨å±¤
        sqlite_layer = analysis_results["layers"].get("sqlite_internal", {})
        if sqlite_layer.get("file_exists") and not sqlite_layer.get("error"):
            layer_scores["sqlite_internal"] = 90
        else:
            layer_scores["sqlite_internal"] = 20
        
        # ãƒ™ã‚¯ãƒˆãƒ«ãƒ»embeddingså±¤
        vector_layer = analysis_results["layers"].get("vector_embeddings", {})
        if not vector_layer.get("error"):
            vector_score = 70
            # æ¤œç´¢ãƒ†ã‚¹ãƒˆã®æˆåŠŸç‡ã«åŸºã¥ã„ã¦èª¿æ•´
            for collection_name, collection_info in vector_layer.items():
                search_tests = collection_info.get("search_tests", [])
                if search_tests:
                    success_rate = sum(1 for t in search_tests if "error" not in t) / len(search_tests)
                    if success_rate > 0.8:
                        vector_score = 100
                    break
            layer_scores["vector_embeddings"] = vector_score
        else:
            layer_scores["vector_embeddings"] = 30
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤
        integrity_layer = analysis_results["layers"].get("data_integrity", {})
        if not integrity_layer.get("error"):
            integrity_scores = []
            for collection_name, collection_info in integrity_layer.items():
                if "quality_score" in collection_info:
                    integrity_scores.append(collection_info["quality_score"])
            
            if integrity_scores:
                layer_scores["data_integrity"] = sum(integrity_scores) / len(integrity_scores)
            else:
                layer_scores["data_integrity"] = 50
        else:
            layer_scores["data_integrity"] = 20
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = sum(layer_scores.values()) / len(layer_scores)
        
        # å¥å…¨æ€§ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        if overall_score >= 90:
            health_level = "EXCELLENT"
            health_icon = "ğŸŸ¢"
        elif overall_score >= 80:
            health_level = "GOOD"
            health_icon = "ğŸŸ¡"
        elif overall_score >= 60:
            health_level = "FAIR"
            health_icon = "ğŸŸ "
        else:
            health_level = "POOR"
            health_icon = "ğŸ”´"
        
        health_report = {
            "overall_score": round(overall_score, 2),
            "health_level": health_level,
            "health_icon": health_icon,
            "layer_scores": layer_scores,
            "recommendations": generate_recommendations(analysis_results, layer_scores)
        }
        
        print(f"{health_icon} ç·åˆãƒ˜ãƒ«ã‚¹: {health_level} ({overall_score:.1f}/100)")
        print(f"ğŸ“Š å±¤åˆ¥ã‚¹ã‚³ã‚¢:")
        for layer, score in layer_scores.items():
            print(f"   {layer}: {score:.1f}/100")
        
        if health_report["recommendations"]:
            print(f"ğŸ’¡ æ¨å¥¨äº‹é …:")
            for rec in health_report["recommendations"]:
                print(f"   â€¢ {rec}")
    
    except Exception as e:
        print(f"âŒ ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        health_report = {"error": str(e)}
    
    return health_report

def generate_recommendations(analysis_results, layer_scores):
    """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
    
    recommendations = []
    
    # å„å±¤ã®ã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨å¥¨äº‹é …
    if layer_scores.get("filesystem", 0) < 80:
        recommendations.append("ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    if layer_scores.get("chromadb_api", 0) < 80:
        recommendations.append("ChromaDB APIã®æ¥ç¶šæ€§ã‚’æ”¹å–„ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
    
    if layer_scores.get("sqlite_internal", 0) < 80:
        recommendations.append("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    if layer_scores.get("vector_embeddings", 0) < 80:
        recommendations.append("ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿèƒ½ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
    
    if layer_scores.get("data_integrity", 0) < 80:
        recommendations.append("ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡é™¤å»ã¨å“è³ªæ”¹å–„ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„")
    
    # å…·ä½“çš„ãªå•é¡Œã«åŸºã¥ãæ¨å¥¨äº‹é …
    integrity_layer = analysis_results["layers"].get("data_integrity", {})
    for collection_name, collection_info in integrity_layer.items():
        if isinstance(collection_info, dict):
            if collection_info.get("id_duplicates", 0) > 0:
                recommendations.append(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³'{collection_name}'ã®IDé‡è¤‡ã‚’è§£æ±ºã—ã¦ãã ã•ã„")
            
            if collection_info.get("missing_documents", 0) > 0:
                recommendations.append(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³'{collection_name}'ã®ç©ºãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã¾ãŸã¯è£œå®Œã—ã¦ãã ã•ã„")
    
    return recommendations

if __name__ == "__main__":
    result = deep_analysis_chromadb()
    
    if result:
        print(f"\nğŸ‰ æ·±å±¤ç²¾æŸ»ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        health = result.get("overall_health", {})
        if health:
            print(f"ğŸ¥ ç·åˆãƒ˜ãƒ«ã‚¹: {health.get('health_icon', 'â“')} {health.get('health_level', 'UNKNOWN')} ({health.get('overall_score', 0)}/100)")
    else:
        print(f"\nâŒ æ·±å±¤ç²¾æŸ»ã«å¤±æ•—ã—ã¾ã—ãŸ")
