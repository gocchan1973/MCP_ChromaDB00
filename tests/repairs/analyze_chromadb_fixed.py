#!/usr/bin/env python3
"""
ChromaDBã®è©³ç´°åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹å«ã‚€ãƒ»ä¿®æ­£ç‰ˆï¼‰
"""
import chromadb
from chromadb.config import Settings
from pathlib import Path
import json
from datetime import datetime
import sqlite3
import numpy as np

def analyze_chromadb_with_index(db_path: str):
    """ChromaDBã‚’è©³ç´°åˆ†æï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹å«ã‚€ï¼‰"""
    print(f"ğŸ” ChromaDBåˆ†æé–‹å§‹ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹å«ã‚€ï¼‰: {db_path}")
    print("=" * 70)
    
    try:
        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶š
        client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # åŸºæœ¬æƒ…å ±
        collections = client.list_collections()
        print(f"ğŸ“Š ç·ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(collections)}")
        print()
        
        total_documents = 0
        index_info = {}
        
        # å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ
        for i, collection in enumerate(collections, 1):
            print(f"ğŸ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ {i}: {collection.name}")
            print(f"   ID: {collection.id}")
            print(f"   ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {collection.metadata}")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            doc_count = collection.count()
            total_documents += doc_count
            print(f"   ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count}")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹åˆ†æ
            print(f"   ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹:")
            collection_index_info = {}
            
            if doc_count > 0:
                try:
                    # ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿æƒ…å ±ï¼ˆå®‰å…¨ãªå–å¾—ï¼‰
                    embeddings_data = collection.get(include=['embeddings'], limit=1)
                    embeddings_list = embeddings_data.get('embeddings', [])
                    
                    # é…åˆ—ã®å­˜åœ¨ç¢ºèªã‚’å®‰å…¨ã«è¡Œã†
                    has_valid_embedding = False
                    embedding_dim = 0
                    
                    if embeddings_list:  # ãƒªã‚¹ãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹
                        if len(embeddings_list) > 0:  # ãƒªã‚¹ãƒˆãŒç©ºã§ãªã„ã‹
                            first_embedding = embeddings_list[0]
                            if first_embedding is not None:  # æœ€åˆã®è¦ç´ ãŒNoneã§ãªã„ã‹
                                # numpyé…åˆ—ã®å ´åˆã‚‚å®‰å…¨ã«å‡¦ç†
                                try:
                                    if hasattr(first_embedding, '__len__'):
                                        embedding_dim = len(first_embedding)
                                        has_valid_embedding = True
                                    elif hasattr(first_embedding, 'shape'):
                                        # numpyé…åˆ—ã®å ´åˆ
                                        embedding_dim = first_embedding.shape[0] if len(first_embedding.shape) > 0 else 0
                                        has_valid_embedding = embedding_dim > 0
                                except (TypeError, AttributeError, IndexError):
                                    # é•·ã•ã‚’å–å¾—ã§ããªã„å ´åˆ
                                    has_valid_embedding = False
                    
                    if has_valid_embedding:
                        print(f"      âœ… ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿: æœ‰åŠ¹")
                        print(f"      ğŸ“ ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°: {embedding_dim}")
                        collection_index_info['has_embeddings'] = True
                        collection_index_info['embedding_dimension'] = embedding_dim
                        
                        # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–çŠ¶æ³ç¢ºèª
                        try:
                            all_embeddings = collection.get(include=['embeddings'])
                            all_embeddings_list = all_embeddings.get('embeddings', [])
                            
                            # å®‰å…¨ã«ãƒ™ã‚¯ãƒˆãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                            vectorized_count = 0
                            for emb in all_embeddings_list:
                                if emb is not None:
                                    try:
                                        # ãƒ™ã‚¯ãƒˆãƒ«ãŒæœ‰åŠ¹ãªé•·ã•ã‚’æŒã¤ã‹ãƒã‚§ãƒƒã‚¯
                                        if hasattr(emb, '__len__'):
                                            if len(emb) > 0:
                                                vectorized_count += 1
                                        elif hasattr(emb, 'shape'):
                                            if len(emb.shape) > 0 and emb.shape[0] > 0:
                                                vectorized_count += 1
                                    except:
                                        # é•·ã•ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                                        continue
                            
                            print(f"      ğŸ“Š ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¸ˆã¿: {vectorized_count}/{doc_count} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
                            collection_index_info['vectorized_documents'] = vectorized_count
                            collection_index_info['vectorization_ratio'] = vectorized_count / doc_count if doc_count > 0 else 0
                        except Exception as vector_error:
                            print(f"      âš ï¸  ãƒ™ã‚¯ãƒˆãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {vector_error}")
                            collection_index_info['vectorized_documents'] = 0
                            collection_index_info['vectorization_ratio'] = 0
                    else:
                        print(f"      âŒ ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿: ãªã—")
                        collection_index_info['has_embeddings'] = False
                        collection_index_info['vectorized_documents'] = 0
                        collection_index_info['vectorization_ratio'] = 0
                        
                except Exception as e:
                    print(f"      âš ï¸  ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    collection_index_info['error'] = str(e)
                    collection_index_info['has_embeddings'] = False
                    collection_index_info['vectorized_documents'] = 0
                
                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¡¨ç¤º
                sample = collection.get(limit=2)
                print(f"   ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:")
                for j, (doc_id, document, metadata) in enumerate(zip(
                    sample.get('ids', []), 
                    sample.get('documents', []), 
                    sample.get('metadatas', [])
                )):
                    print(f"     - ID: {doc_id}")
                    if document:
                        preview = document[:80] + "..." if len(document) > 80 else document
                        print(f"       å†…å®¹: {preview}")
                    if metadata:
                        print(f"       ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata}")
                    print()
                    
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ
                all_docs = collection.get()
                metadatas = all_docs.get('metadatas', [])
                if metadatas:
                    metadata_keys = set()
                    for meta in metadatas:
                        if meta:
                            metadata_keys.update(meta.keys())
                    print(f"   ğŸ·ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼: {list(metadata_keys)}")
                    collection_index_info['metadata_keys'] = list(metadata_keys)
            else:
                print(f"      ğŸ“­ ç©ºã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
                collection_index_info['has_embeddings'] = False
                collection_index_info['vectorized_documents'] = 0
            
            index_info[collection.name] = collection_index_info
            print("-" * 50)
            print()
        
        # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®è©³ç´°åˆ†æ
        print("ğŸ—„ï¸  SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©³ç´°åˆ†æ")
        print("=" * 50)
        sqlite_file = Path(db_path) / "chroma.sqlite3"
        sqlite_info = {}
        
        if sqlite_file.exists():
            try:
                size_mb = sqlite_file.stat().st_size / (1024 * 1024)
                print(f"ğŸ“¦ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {size_mb:.2f} MB")
                sqlite_info['file_size_mb'] = size_mb
                
                with sqlite3.connect(sqlite_file) as conn:
                    cursor = conn.cursor()
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
                    tables = cursor.fetchall()
                    table_names = [table[0] for table in tables]
                    print(f"ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§: {table_names}")
                    sqlite_info['tables'] = table_names
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§
                    cursor.execute("SELECT name, tbl_name, sql FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%';")
                    indexes = cursor.fetchall()
                    print(f"ğŸ“Š ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(indexes)}")
                    
                    index_details = []
                    for index_name, table_name, sql in indexes:
                        print(f"   ğŸ” {index_name} (ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name})")
                        if sql:
                            print(f"      SQL: {sql}")
                        index_details.append({
                            'name': index_name,
                            'table': table_name,
                            'sql': sql
                        })
                    sqlite_info['indexes'] = index_details
                    
                    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®è©³ç´°æƒ…å ±
                    table_details = {}
                    for table in table_names:
                        try:
                            cursor.execute(f"SELECT COUNT(*) FROM [{table}];")
                            count = cursor.fetchone()[0]
                            
                            cursor.execute(f"PRAGMA table_info([{table}]);")
                            columns = cursor.fetchall()
                            column_info = [{'name': col[1], 'type': col[2], 'not_null': col[3], 'pk': col[5]} for col in columns]
                            
                            print(f"   ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ« '{table}': {count} ãƒ¬ã‚³ãƒ¼ãƒ‰")
                            print(f"      ã‚«ãƒ©ãƒ : {[col['name'] for col in column_info]}")
                            
                            table_details[table] = {
                                'record_count': count,
                                'columns': column_info
                            }
                        except Exception as e:
                            print(f"      âš ï¸  ãƒ†ãƒ¼ãƒ–ãƒ« '{table}' åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    
                    sqlite_info['table_details'] = table_details
                    
            except Exception as e:
                print(f"âŒ SQLiteåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                sqlite_info['error'] = str(e)
        else:
            print(f"âŒ SQLiteãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sqlite_file}")
            sqlite_info['error'] = "SQLite file not found"
        
        # ç·æ‹¬
        print("=" * 50)
        print("ğŸ“ˆ åˆ†æç·æ‹¬")
        print(f"   ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_documents}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {db_path}")
        print(f"   åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹ã‚µãƒãƒªãƒ¼
        vectorized_collections = sum(1 for info in index_info.values() if info.get('has_embeddings', False))
        total_vectorized = sum(info.get('vectorized_documents', 0) for info in index_info.values())
        
        print(f"   ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¸ˆã¿ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {vectorized_collections}/{len(collections)}")
        print(f"   ç·ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {total_vectorized}/{total_documents}")
        if total_documents > 0:
            overall_ratio = total_vectorized / total_documents * 100
            print(f"   å…¨ä½“ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç‡: {overall_ratio:.1f}%")
        
        return {
            "success": True,
            "collections_count": len(collections),
            "total_documents": total_documents,
            "vectorized_collections": vectorized_collections,
            "total_vectorized_documents": total_vectorized,
            "collections": [{"name": c.name, "id": str(c.id), "count": c.count()} for c in collections],
            "index_info": index_info,
            "sqlite_info": sqlite_info,
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    # æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‚’åˆ†æ
    target_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB"
    result = analyze_chromadb_with_index(target_path)
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚ä¿å­˜
    output_file = Path(__file__).parent / f"chromadb_index_analysis_fixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ åˆ†æçµæœã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
