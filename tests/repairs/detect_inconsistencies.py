#!/usr/bin/env python3
"""
ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ä¸æ•´åˆæ¤œå‡ºãƒ„ãƒ¼ãƒ«
"""
import chromadb
from chromadb.config import Settings
from pathlib import Path
import json
from datetime import datetime
import sqlite3

def detect_collection_inconsistencies(db_path: str):
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ä¸æ•´åˆã‚’æ¤œå‡º"""
    print(f"ğŸ” ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸æ•´åˆæ¤œå‡º: {db_path}")
    print("=" * 70)
    
    try:
        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶š
        client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        collections = client.list_collections()
        print(f"ğŸ“Š å¯¾è±¡ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(collections)}")
        print()
        
        inconsistencies = {}
        
        # å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ
        for i, collection in enumerate(collections, 1):
            print(f"ğŸ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ {i}: {collection.name}")
            collection_issues = {}
            
            doc_count = collection.count()
            print(f"   ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count}")
            
            if doc_count > 0:
                # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
                print(f"   ğŸ” ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æ:")
                all_docs = collection.get()
                metadatas = all_docs.get('metadatas', [])
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã®çµ±è¨ˆ
                metadata_key_stats = {}
                missing_metadata_count = 0
                
                for j, metadata in enumerate(metadatas):
                    if metadata is None:
                        missing_metadata_count += 1
                        continue
                    
                    for key in metadata.keys():
                        if key not in metadata_key_stats:
                            metadata_key_stats[key] = 0
                        metadata_key_stats[key] += 1
                
                print(f"      ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {missing_metadata_count}/{doc_count}")
                print(f"      ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ä½¿ç”¨é »åº¦:")
                for key, count in metadata_key_stats.items():
                    coverage = (count / doc_count) * 100
                    status = "âœ…" if coverage == 100 else "âš ï¸" if coverage > 50 else "âŒ"
                    print(f"        {status} '{key}': {count}/{doc_count} ({coverage:.1f}%)")
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã®ä¸æ•´åˆæ¤œå‡º
                expected_keys = set(metadata_key_stats.keys())
                inconsistent_docs = []
                
                for j, (doc_id, metadata) in enumerate(zip(all_docs.get('ids', []), metadatas)):
                    if metadata is None:
                        inconsistent_docs.append({
                            'doc_id': doc_id,
                            'issue': 'ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—'
                        })
                        continue
                    
                    current_keys = set(metadata.keys())
                    missing_keys = expected_keys - current_keys
                    extra_keys = current_keys - expected_keys
                    
                    if missing_keys or extra_keys:
                        inconsistent_docs.append({
                            'doc_id': doc_id,
                            'missing_keys': list(missing_keys),
                            'extra_keys': list(extra_keys)
                        })
                
                if inconsistent_docs:
                    print(f"      âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(inconsistent_docs)}")
                    for issue in inconsistent_docs[:3]:  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
                        print(f"        - {issue['doc_id']}: {issue.get('issue', 'æ§‹é€ ä¸æ•´åˆ')}")
                else:
                    print(f"      âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ : ä¸€è²«æ€§ã‚ã‚Š")
                
                collection_issues['metadata_inconsistencies'] = inconsistent_docs
                collection_issues['metadata_coverage'] = metadata_key_stats
                
                # 2. ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                print(f"   ğŸ” ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿åˆ†æ:")
                try:
                    # å°‘æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒã‚’ãƒã‚§ãƒƒã‚¯
                    sample_embeddings = collection.get(include=['embeddings'], limit=min(5, doc_count))
                    embeddings = sample_embeddings.get('embeddings', [])
                    
                    if embeddings:
                        dimensions = []
                        null_count = 0
                        
                        for emb in embeddings:
                            if emb is None:
                                null_count += 1
                            else:
                                dimensions.append(len(emb))
                        
                        if dimensions:
                            unique_dims = set(dimensions)
                            if len(unique_dims) == 1:
                                dim = dimensions[0]
                                print(f"      âœ… ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {dim} (ä¸€è²«æ€§ã‚ã‚Š)")
                                collection_issues['vector_dimension'] = dim
                                collection_issues['vector_dimension_consistent'] = True
                            else:
                                print(f"      âŒ ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒä¸æ•´åˆ: {unique_dims}")
                                collection_issues['vector_dimensions'] = list(unique_dims)
                                collection_issues['vector_dimension_consistent'] = False
                        
                        if null_count > 0:
                            print(f"      âš ï¸  Nullãƒ™ã‚¯ãƒˆãƒ«: {null_count}/{len(embeddings)} ã‚µãƒ³ãƒ—ãƒ«")
                            collection_issues['null_vectors'] = null_count
                    else:
                        print(f"      âŒ ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãªã—")
                        collection_issues['has_vectors'] = False
                        
                except Exception as e:
                    print(f"      âŒ ãƒ™ã‚¯ãƒˆãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    collection_issues['vector_error'] = str(e)
                
                # 3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                print(f"   ğŸ” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹åˆ†æ:")
                documents = all_docs.get('documents', [])
                
                empty_docs = sum(1 for doc in documents if not doc or doc.strip() == "")
                avg_length = sum(len(doc) for doc in documents if doc) / len([d for d in documents if d]) if documents else 0
                
                print(f"      ç©ºãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {empty_docs}/{doc_count}")
                print(f"      å¹³å‡æ–‡å­—æ•°: {avg_length:.1f}")
                
                collection_issues['empty_documents'] = empty_docs
                collection_issues['average_document_length'] = avg_length
            
            else:
                print(f"   ğŸ“­ ç©ºã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
                collection_issues['is_empty'] = True
            
            inconsistencies[collection.name] = collection_issues
            print("-" * 50)
            print()
        
        # SQLiteãƒ¬ãƒ™ãƒ«ã§ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        print("ğŸ—„ï¸  SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§åˆ†æ")
        print("=" * 50)
        
        sqlite_file = Path(db_path) / "chroma.sqlite3"
        db_issues = {}
        
        if sqlite_file.exists():
            with sqlite3.connect(sqlite_file) as conn:
                cursor = conn.cursor()
                
                # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ•´åˆæ€§
                cursor.execute("SELECT id, name, dimension FROM collections;")
                db_collections = cursor.fetchall()
                
                print(f"ğŸ“‹ SQLiteã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•´åˆæ€§:")
                for col_id, col_name, dimension in db_collections:
                    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ãƒã‚§ãƒƒã‚¯
                    cursor.execute("SELECT COUNT(*) FROM segments WHERE collection = ?;", (col_id,))
                    segment_count = cursor.fetchone()[0]
                    
                    # åŸ‹ã‚è¾¼ã¿æ•°ãƒã‚§ãƒƒã‚¯
                    cursor.execute("SELECT COUNT(*) FROM embeddings WHERE segment_id IN (SELECT id FROM segments WHERE collection = ?);", (col_id,))
                    embedding_count = cursor.fetchone()[0]
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯
                    cursor.execute("SELECT COUNT(*) FROM embedding_metadata WHERE id IN (SELECT id FROM embeddings WHERE segment_id IN (SELECT id FROM segments WHERE collection = ?));", (col_id,))
                    metadata_count = cursor.fetchone()[0]
                    
                    print(f"   ğŸ“ {col_name}:")
                    print(f"      è¨­å®šæ¬¡å…ƒæ•°: {dimension}")
                    print(f"      ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {segment_count}")
                    print(f"      åŸ‹ã‚è¾¼ã¿ãƒ¬ã‚³ãƒ¼ãƒ‰: {embedding_count}")
                    print(f"      ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚³ãƒ¼ãƒ‰: {metadata_count}")
                    
                    # ä¸æ•´åˆæ¤œå‡º
                    expected_segments = 2  # é€šå¸¸ã¯VECTORã¨METADATAã®2ã¤
                    if segment_count != expected_segments:
                        print(f"      âš ï¸  ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ç•°å¸¸: æœŸå¾…å€¤{expected_segments}, å®Ÿéš›{segment_count}")
                    
                    db_issues[col_name] = {
                        'dimension': dimension,
                        'segments': segment_count,
                        'embeddings': embedding_count,
                        'metadata_records': metadata_count
                    }
                
                # å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                print(f"\nğŸ” å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰æ¤œå‡º:")
                
                # å­¤ç«‹ã—ãŸåŸ‹ã‚è¾¼ã¿
                cursor.execute("SELECT COUNT(*) FROM embeddings WHERE segment_id NOT IN (SELECT id FROM segments);")
                orphaned_embeddings = cursor.fetchone()[0]
                
                # å­¤ç«‹ã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                cursor.execute("SELECT COUNT(*) FROM embedding_metadata WHERE id NOT IN (SELECT id FROM embeddings);")
                orphaned_metadata = cursor.fetchone()[0]
                
                if orphaned_embeddings > 0:
                    print(f"   âŒ å­¤ç«‹ã—ãŸåŸ‹ã‚è¾¼ã¿: {orphaned_embeddings}")
                if orphaned_metadata > 0:
                    print(f"   âŒ å­¤ç«‹ã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {orphaned_metadata}")
                
                if orphaned_embeddings == 0 and orphaned_metadata == 0:
                    print(f"   âœ… å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãªã—")
                
                db_issues['orphaned_embeddings'] = orphaned_embeddings
                db_issues['orphaned_metadata'] = orphaned_metadata
        
        # ç·åˆè©•ä¾¡
        print("\nğŸ“ˆ ä¸æ•´åˆåˆ†æçµæœ")
        print("=" * 50)
        
        total_issues = 0
        for col_name, issues in inconsistencies.items():
            col_issues = 0
            if issues.get('metadata_inconsistencies'):
                col_issues += len(issues['metadata_inconsistencies'])
            if not issues.get('vector_dimension_consistent', True):
                col_issues += 1
            if issues.get('empty_documents', 0) > 0:
                col_issues += 1
            
            status = "âœ… æ­£å¸¸" if col_issues == 0 else f"âš ï¸ {col_issues}ä»¶ã®å•é¡Œ"
            print(f"   {col_name}: {status}")
            total_issues += col_issues
        
        if total_issues == 0:
            print(f"\nğŸ‰ å…¨ä½“è©•ä¾¡: ä¸æ•´åˆãªã— - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯æ­£å¸¸ã§ã™")
        else:
            print(f"\nâš ï¸ å…¨ä½“è©•ä¾¡: {total_issues}ä»¶ã®ä¸æ•´åˆã‚’æ¤œå‡º")
        
        return {
            "success": True,
            "total_issues": total_issues,
            "collection_inconsistencies": inconsistencies,
            "database_issues": db_issues,
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    target_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB"
    result = detect_collection_inconsistencies(target_path)
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = Path(__file__).parent / f"collection_inconsistency_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è©³ç´°åˆ†æçµæœã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
