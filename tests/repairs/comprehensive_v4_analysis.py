#!/usr/bin/env python3
"""
ChromaDB v4 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ãªæ·±å±¤åˆ†æžãƒ„ãƒ¼ãƒ«
å…¨ã¦ã®å±¤ã‚’è©³ç´°ã«åˆ†æžã—ã¦å®Œå…¨ãªå¥å…¨æ€§å ±å‘Šã‚’ç”Ÿæˆ
"""

import chromadb
import os
import json
import sqlite3
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Tuple
import hashlib
import time
from pathlib import Path

class ChromaDBv4Analyzer:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.client = None
        self.collection = None
        self.analysis_results = {}
        
    def connect_to_database(self) -> bool:
        """v4ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŽ¥ç¶š"""
        try:
            self.client = chromadb.PersistentClient(path=self.db_path)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šæˆåŠŸ: {self.db_path}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def analyze_filesystem_layer(self) -> Dict[str, Any]:
        """ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤ã®åˆ†æž"""
        print("\nðŸ” ç¬¬1å±¤åˆ†æž: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤")
        
        results = {
            "layer": "filesystem",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
            db_exists = os.path.exists(self.db_path)
            results["details"]["database_exists"] = db_exists
            
            if not db_exists:
                results["status"] = "failed"
                results["details"]["error"] = "Database directory does not exist"
                return results
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®åˆ†æž
            directory_structure = {}
            total_size = 0
            file_count = 0
            
            for root, dirs, files in os.walk(self.db_path):
                rel_path = os.path.relpath(root, self.db_path)
                directory_structure[rel_path] = []
                
                for file in files:
                    file_path = os.path.join(root, file)
                    file_size = os.path.getsize(file_path)
                    file_info = {
                        "name": file,
                        "size": file_size,
                        "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
                    }
                    directory_structure[rel_path].append(file_info)
                    total_size += file_size
                    file_count += 1
            
            results["details"]["structure"] = directory_structure
            results["details"]["total_size_bytes"] = total_size
            results["details"]["total_size_mb"] = round(total_size / (1024 * 1024), 2)
            results["details"]["file_count"] = file_count
            
            # æ¨©é™ç¢ºèª
            permissions = {
                "readable": os.access(self.db_path, os.R_OK),
                "writable": os.access(self.db_path, os.W_OK),
                "executable": os.access(self.db_path, os.X_OK)
            }
            results["details"]["permissions"] = permissions
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            if all(permissions.values()) and total_size > 0:
                results["status"] = "excellent"
                results["score"] = 100
            elif permissions["readable"] and permissions["writable"]:
                results["status"] = "good"
                results["score"] = 85
            else:
                results["status"] = "warning"
                results["score"] = 60
                
            print(f"   ðŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : {len(directory_structure)} ãƒ•ã‚©ãƒ«ãƒ€, {file_count} ãƒ•ã‚¡ã‚¤ãƒ«")
            print(f"   ðŸ’¾ ç·ã‚µã‚¤ã‚º: {results['details']['total_size_mb']} MB")
            print(f"   ðŸ”’ æ¨©é™: {permissions}")
            
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_chromadb_api_layer(self) -> Dict[str, Any]:
        """ç¬¬2å±¤: ChromaDB APIå±¤ã®åˆ†æž"""
        print("\nðŸ” ç¬¬2å±¤åˆ†æž: ChromaDB APIå±¤")
        
        results = {
            "layer": "chromadb_api",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—
            collections = self.client.list_collections()
            results["details"]["collections_count"] = len(collections)
            results["details"]["collections"] = []
            
            for collection in collections:
                coll_info = {
                    "name": collection.name,
                    "id": collection.id,
                    "metadata": collection.metadata
                }
                results["details"]["collections"].append(coll_info)
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®š
                if collection.name == "sister_chat_history_v4":
                    self.collection = collection
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æž
            if self.collection:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
                doc_count = self.collection.count()
                results["details"]["main_collection"] = {
                    "name": self.collection.name,
                    "document_count": doc_count,
                    "metadata": self.collection.metadata
                }
                
                # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆ
                try:
                    search_result = self.collection.query(
                        query_texts=["ãƒ†ã‚¹ãƒˆ"],
                        n_results=1
                    )
                    results["details"]["search_test"] = {
                        "success": True,
                        "result_count": len(search_result["documents"][0]) if search_result["documents"] else 0
                    }
                except Exception as e:
                    results["details"]["search_test"] = {
                        "success": False,
                        "error": str(e)
                    }
                
                print(f"   ðŸ“Š ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {self.collection.name}")
                print(f"   ðŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count}")
                print(f"   ðŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆ: {'âœ…' if results['details']['search_test']['success'] else 'âŒ'}")
                
                # ã‚¹ã‚³ã‚¢åˆ¤å®š
                if doc_count > 0 and results["details"]["search_test"]["success"]:
                    results["status"] = "excellent"
                    results["score"] = 100
                else:
                    results["status"] = "warning"
                    results["score"] = 70
            else:
                results["status"] = "failed"
                results["details"]["error"] = "Main collection not found"
                results["score"] = 0
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_sqlite_internal_layer(self) -> Dict[str, Any]:
        """ç¬¬3å±¤: SQLiteå†…éƒ¨æ§‹é€ å±¤ã®åˆ†æž"""
        print("\nðŸ” ç¬¬3å±¤åˆ†æž: SQLiteå†…éƒ¨æ§‹é€ å±¤")
        
        results = {
            "layer": "sqlite_internal",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŽ¢ã™
            sqlite_files = []
            for root, dirs, files in os.walk(self.db_path):
                for file in files:
                    if file.endswith('.sqlite') or file.endswith('.db'):
                        sqlite_files.append(os.path.join(root, file))
            
            results["details"]["sqlite_files"] = sqlite_files
            print(f"   ðŸ—„ï¸  SQLiteãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(sqlite_files)}")
            
            if not sqlite_files:
                results["status"] = "warning"
                results["details"]["error"] = "No SQLite files found"
                results["score"] = 50
                return results
            
            # ãƒ¡ã‚¤ãƒ³ã®SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æž
            main_db = sqlite_files[0]  # é€šå¸¸ã¯æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ¡ã‚¤ãƒ³
            
            with sqlite3.connect(main_db) as conn:
                cursor = conn.cursor()
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
                tables = [row[0] for row in cursor.fetchall()]
                results["details"]["tables"] = tables
                results["details"]["table_count"] = len(tables)
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§
                cursor.execute("SELECT name FROM sqlite_master WHERE type='index';")
                indexes = [row[0] for row in cursor.fetchall()]
                results["details"]["indexes"] = indexes
                results["details"]["index_count"] = len(indexes)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ
                cursor.execute("PRAGMA database_list;")
                db_info = cursor.fetchall()
                results["details"]["database_info"] = db_info
                
                # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®è©³ç´°æƒ…å ±
                table_details = {}
                for table in tables:
                    try:
                        cursor.execute(f"SELECT COUNT(*) FROM `{table}`;")
                        row_count = cursor.fetchone()[0]
                        
                        cursor.execute(f"PRAGMA table_info(`{table}`);")
                        schema = cursor.fetchall()
                        
                        table_details[table] = {
                            "row_count": row_count,
                            "schema": schema
                        }
                    except Exception as e:
                        table_details[table] = {"error": str(e)}
                
                results["details"]["table_details"] = table_details
                
                print(f"   ðŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
                print(f"   ðŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(indexes)}")
                
                # ã‚¹ã‚³ã‚¢åˆ¤å®š
                if len(tables) >= 10 and len(indexes) > 0:
                    results["status"] = "excellent"
                    results["score"] = 95
                elif len(tables) >= 5:
                    results["status"] = "good"
                    results["score"] = 80
                else:
                    results["status"] = "warning"
                    results["score"] = 60
                    
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_vector_embeddings_layer(self) -> Dict[str, Any]:
        """ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤ã®åˆ†æž"""
        print("\nðŸ” ç¬¬4å±¤åˆ†æž: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤")
        
        results = {
            "layer": "vector_embeddings",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒ™ã‚¯ã‚¿ãƒ¼åˆ†æž
            sample_data = self.collection.get(limit=5, include=['embeddings', 'metadatas', 'documents'])
            
            if not sample_data['embeddings']:
                results["status"] = "failed"
                results["details"]["error"] = "No embeddings found"
                results["score"] = 0
                return results
            
            embeddings = sample_data['embeddings']
            results["details"]["sample_count"] = len(embeddings)
            
            # ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒåˆ†æž
            if embeddings:
                vector_dimensions = len(embeddings[0])
                results["details"]["vector_dimensions"] = vector_dimensions
                
                # å…¨ãƒ™ã‚¯ã‚¿ãƒ¼ã®çµ±è¨ˆ
                all_vectors = np.array(embeddings)
                results["details"]["vector_stats"] = {
                    "mean": float(np.mean(all_vectors)),
                    "std": float(np.std(all_vectors)),
                    "min": float(np.min(all_vectors)),
                    "max": float(np.max(all_vectors)),
                    "shape": all_vectors.shape
                }
                
                # ãƒ™ã‚¯ã‚¿ãƒ¼å“è³ªãƒã‚§ãƒƒã‚¯
                # 1. ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯
                zero_vectors = np.sum(np.all(all_vectors == 0, axis=1))
                results["details"]["zero_vectors"] = int(zero_vectors)
                
                # 2. æ­£è¦åŒ–ãƒã‚§ãƒƒã‚¯
                norms = np.linalg.norm(all_vectors, axis=1)
                results["details"]["vector_norms"] = {
                    "mean": float(np.mean(norms)),
                    "std": float(np.std(norms)),
                    "min": float(np.min(norms)),
                    "max": float(np.max(norms))
                }
                
                # 3. é¡žä¼¼åº¦åˆ†å¸ƒ
                if len(embeddings) > 1:
                    similarities = []
                    for i in range(min(3, len(embeddings))):
                        for j in range(i+1, min(5, len(embeddings))):
                            sim = np.dot(embeddings[i], embeddings[j]) / (
                                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])
                            )
                            similarities.append(float(sim))
                    
                    if similarities:
                        results["details"]["similarity_stats"] = {
                            "mean": float(np.mean(similarities)),
                            "std": float(np.std(similarities)),
                            "min": float(np.min(similarities)),
                            "max": float(np.max(similarities))
                        }
                
                print(f"   ðŸ”¢ ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒ: {vector_dimensions}")
                print(f"   ðŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(embeddings)}")
                print(f"   ðŸŽ¯ ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {zero_vectors}")
                
                # ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ãƒ†ã‚¹ãƒˆ
                try:
                    test_query = "ãƒ†ã‚¹ãƒˆæ¤œç´¢"
                    search_results = self.collection.query(
                        query_texts=[test_query],
                        n_results=3
                    )
                    
                    search_quality = len(search_results['documents'][0]) if search_results['documents'] else 0
                    results["details"]["search_quality"] = search_quality
                    
                    print(f"   ðŸ” æ¤œç´¢å“è³ª: {search_quality}/3")
                    
                except Exception as e:
                    results["details"]["search_error"] = str(e)
                    print(f"   âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                
                # ã‚¹ã‚³ã‚¢åˆ¤å®š
                quality_score = 100
                if zero_vectors > 0:
                    quality_score -= 10
                if results["details"]["vector_norms"]["std"] > 0.5:
                    quality_score -= 10
                if "search_error" in results["details"]:
                    quality_score -= 20
                
                results["score"] = max(0, quality_score)
                
                if quality_score >= 90:
                    results["status"] = "excellent"
                elif quality_score >= 70:
                    results["status"] = "good"
                else:
                    results["status"] = "warning"
                    
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_data_integrity_layer(self) -> Dict[str, Any]:
        """ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤ã®åˆ†æž"""
        print("\nðŸ” ç¬¬5å±¤åˆ†æž: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤")
        
        results = {
            "layer": "data_integrity",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå¤§ããªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å ´åˆã¯åˆ¶é™ï¼‰
            total_count = self.collection.count()
            sample_limit = min(total_count, 1000)  # æœ€å¤§1000ä»¶ã¾ã§åˆ†æž
            
            all_data = self.collection.get(
                limit=sample_limit,
                include=['metadatas', 'documents', 'embeddings']
            )
            
            results["details"]["analyzed_documents"] = len(all_data['documents'])
            results["details"]["total_documents"] = total_count
            
            # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            metadata_issues = []
            if all_data['metadatas']:
                required_fields = ['source', 'timestamp']  # æœŸå¾…ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                
                for i, metadata in enumerate(all_data['metadatas']):
                    if metadata is None:
                        metadata_issues.append(f"Document {i}: Null metadata")
                        continue
                    
                    for field in required_fields:
                        if field not in metadata:
                            metadata_issues.append(f"Document {i}: Missing field '{field}'")
            
            results["details"]["metadata_issues"] = metadata_issues
            results["details"]["metadata_integrity_score"] = max(0, 100 - len(metadata_issues))
            
            # 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯
            if all_data['documents']:
                doc_hashes = []
                duplicates = []
                
                for i, doc in enumerate(all_data['documents']):
                    if doc:
                        doc_hash = hashlib.md5(doc.encode('utf-8')).hexdigest()
                        if doc_hash in doc_hashes:
                            duplicates.append(i)
                        doc_hashes.append(doc_hash)
                
                results["details"]["duplicates"] = duplicates
                results["details"]["duplicate_count"] = len(duplicates)
                results["details"]["uniqueness_ratio"] = (len(all_data['documents']) - len(duplicates)) / len(all_data['documents'])
            
            # 3. åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ã‚¿ãƒ¼æ•´åˆæ€§
            embedding_issues = []
            if all_data['embeddings']:
                expected_dim = len(all_data['embeddings'][0]) if all_data['embeddings'] else 0
                
                for i, embedding in enumerate(all_data['embeddings']):
                    if embedding is None:
                        embedding_issues.append(f"Document {i}: Null embedding")
                    elif len(embedding) != expected_dim:
                        embedding_issues.append(f"Document {i}: Dimension mismatch")
            
            results["details"]["embedding_issues"] = embedding_issues
            results["details"]["embedding_integrity_score"] = max(0, 100 - len(embedding_issues))
            
            # 4. IDæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            if all_data['ids']:
                id_set = set(all_data['ids'])
                duplicate_ids = len(all_data['ids']) - len(id_set)
                results["details"]["duplicate_ids"] = duplicate_ids
            
            # 5. ç·åˆæ¤œç´¢å“è³ªãƒ†ã‚¹ãƒˆ
            search_tests = [
                "å§‰å¦¹", "ä¼šè©±", "è³ªå•", "å›žç­”", "ã‚·ã‚¹ãƒ†ãƒ "
            ]
            search_quality_scores = []
            
            for test_query in search_tests:
                try:
                    search_result = self.collection.query(
                        query_texts=[test_query],
                        n_results=5
                    )
                    if search_result['documents'] and search_result['documents'][0]:
                        search_quality_scores.append(len(search_result['documents'][0]))
                    else:
                        search_quality_scores.append(0)
                except:
                    search_quality_scores.append(0)
            
            results["details"]["search_quality_tests"] = {
                "queries": search_tests,
                "scores": search_quality_scores,
                "average_score": np.mean(search_quality_scores) if search_quality_scores else 0
            }
            
            print(f"   ðŸ“Š åˆ†æžãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(all_data['documents'])}/{total_count}")
            print(f"   ðŸ” ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å•é¡Œ: {len(metadata_issues)}")
            print(f"   ðŸ”„ é‡è¤‡æ•°: {len(duplicates) if 'duplicates' in locals() else 0}")
            print(f"   ðŸŽ¯ æ¤œç´¢å“è³ªå¹³å‡: {results['details']['search_quality_tests']['average_score']:.2f}/5")
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            total_score = 100
            total_score -= min(50, len(metadata_issues) * 2)  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å•é¡Œ
            total_score -= min(30, len(duplicates) * 2 if 'duplicates' in locals() else 0)  # é‡è¤‡
            total_score -= min(20, len(embedding_issues) * 5)  # åŸ‹ã‚è¾¼ã¿å•é¡Œ
            
            results["score"] = max(0, total_score)
            
            if total_score >= 95:
                results["status"] = "excellent"
            elif total_score >= 80:
                results["status"] = "good"
            elif total_score >= 60:
                results["status"] = "warning"
            else:
                results["status"] = "failed"
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªåˆ†æžãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\n" + "="*80)
        print("ðŸ”¬ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æžãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*80)
        
        # å…¨å±¤ã®åˆ†æžã‚’å®Ÿè¡Œ
        layer_results = []
        
        # ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
        layer_results.append(self.analyze_filesystem_layer())
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶š
        if self.connect_to_database():
            # ç¬¬2å±¤: ChromaDB API
            layer_results.append(self.analyze_chromadb_api_layer())
            
            # ç¬¬3å±¤: SQLiteå†…éƒ¨
            layer_results.append(self.analyze_sqlite_internal_layer())
            
            # ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿
            layer_results.append(self.analyze_vector_embeddings_layer())
            
            # ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            layer_results.append(self.analyze_data_integrity_layer())
        
        # ç·åˆè©•ä¾¡
        total_score = np.mean([result.get("score", 0) for result in layer_results])
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if total_score >= 95:
            overall_status = "EXCELLENT"
            status_emoji = "ðŸŒŸ"
        elif total_score >= 85:
            overall_status = "GOOD"
            status_emoji = "âœ…"
        elif total_score >= 70:
            overall_status = "WARNING"
            status_emoji = "âš ï¸"
        else:
            overall_status = "CRITICAL"
            status_emoji = "âŒ"
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        final_report = {
            "analysis_timestamp": datetime.now().isoformat(),
            "database_path": self.db_path,
            "overall_status": overall_status,
            "overall_score": round(total_score, 2),
            "layer_count": len(layer_results),
            "layer_results": layer_results,
            "summary": {
                "status_emoji": status_emoji,
                "recommendations": self.generate_recommendations(layer_results),
                "health_metrics": self.calculate_health_metrics(layer_results)
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        print(f"\n{status_emoji} ç·åˆè©•ä¾¡: {overall_status} ({total_score:.1f}/100)")
        print(f"ðŸ“Š åˆ†æžå±¤æ•°: {len(layer_results)}")
        print(f"â° åˆ†æžæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"\nðŸ“‹ å±¤åˆ¥ã‚¹ã‚³ã‚¢:")
        for result in layer_results:
            status_icon = {
                "excellent": "ðŸŒŸ",
                "good": "âœ…", 
                "warning": "âš ï¸",
                "failed": "âŒ",
                "unknown": "â“"
            }.get(result.get("status", "unknown"), "â“")
            
            layer_name = result.get("layer", "Unknown")
            score = result.get("score", 0)
            print(f"   {status_icon} {layer_name}: {score}/100")
        
        # æŽ¨å¥¨äº‹é …
        recommendations = final_report["summary"]["recommendations"]
        if recommendations:
            print(f"\nðŸ’¡ æŽ¨å¥¨äº‹é …:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        return final_report
    
    def generate_recommendations(self, layer_results: List[Dict]) -> List[str]:
        """åˆ†æžçµæžœã«åŸºã¥ãæŽ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        for result in layer_results:
            if result.get("status") == "failed":
                recommendations.append(f"{result.get('layer', 'Unknown')}å±¤ã®ä¿®å¾©ãŒå¿…è¦ã§ã™")
            elif result.get("status") == "warning":
                recommendations.append(f"{result.get('layer', 'Unknown')}å±¤ã®æœ€é©åŒ–ã‚’æ¤œè¨Žã—ã¦ãã ã•ã„")
        
        if not recommendations:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯è‰¯å¥½ãªçŠ¶æ…‹ã§ã™ã€‚å®šæœŸçš„ãªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚")
        
        return recommendations
    
    def calculate_health_metrics(self, layer_results: List[Dict]) -> Dict[str, Any]:
        """å¥å…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        scores = [result.get("score", 0) for result in layer_results]
        
        return {
            "average_score": round(np.mean(scores), 2),
            "min_score": min(scores),
            "max_score": max(scores),
            "score_variance": round(np.var(scores), 2),
            "healthy_layers": sum(1 for score in scores if score >= 80),
            "critical_layers": sum(1 for score in scores if score < 60)
        }
    
    def save_report(self, report: Dict[str, Any], output_file: str = None):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"chromadb_v4_analysis_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"\nðŸ’¾ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_file}")
        except Exception as e:
            print(f"\nâŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ChromaDB v4ã®ãƒ‘ã‚¹
    v4_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_v4"
    
    print("ðŸš€ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æžã‚’é–‹å§‹")
    print(f"ðŸ“‚ å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {v4_db_path}")
    
    # åˆ†æžå™¨ã‚’åˆæœŸåŒ–
    analyzer = ChromaDBv4Analyzer(v4_db_path)
    
    # åŒ…æ‹¬çš„ãªåˆ†æžã‚’å®Ÿè¡Œ
    report = analyzer.generate_comprehensive_report()
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    output_file = r"f:\å‰¯æ¥­\VSC_WorkSpace\MCP_ChromaDB00\chromadb_v4_comprehensive_report.json"
    analyzer.save_report(report, output_file)
    
    print("\nðŸŽ‰ åˆ†æžå®Œäº†!")
    return report

if __name__ == "__main__":
    report = main()
