#!/usr/bin/env python3
"""
ChromaDB v4 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ãªæ·±å±¤åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆä¿®æ­£ç‰ˆï¼‰
numpyé…åˆ—ã®çœŸå½å€¤ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã—ã€å®‰å…¨ãªåˆ†æã‚’å®Ÿè¡Œ
"""

import chromadb
import os
import json
import sqlite3
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Tuple
import hashlib
import time
from pathlib import Path
import uuid

class SafeChromaDBv4Analyzer:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.client = None
        self.collection = None
        self.analysis_results = {}
        
    def connect_to_database(self) -> bool:
        """v4ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š"""
        try:
            self.client = chromadb.PersistentClient(path=self.db_path)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ: {self.db_path}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def safe_array_check(self, arr, condition_func):
        """numpyé…åˆ—ã®å®‰å…¨ãªãƒã‚§ãƒƒã‚¯"""
        try:
            if isinstance(arr, np.ndarray):
                return condition_func(arr)
            else:
                return condition_func(np.array(arr))
        except Exception as e:
            print(f"   âš ï¸ é…åˆ—ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def analyze_filesystem_layer(self) -> Dict[str, Any]:
        """ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬1å±¤åˆ†æ: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤")
        
        results = {
            "layer": "filesystem",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
            db_exists = os.path.exists(self.db_path)
            results["details"]["database_exists"] = db_exists
            
            if not db_exists:
                results["status"] = "failed"
                results["details"]["error"] = "Database directory does not exist"
                return results
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®åˆ†æ
            directory_structure = {}
            total_size = 0
            file_count = 0
            
            for root, dirs, files in os.walk(self.db_path):
                rel_path = os.path.relpath(root, self.db_path)
                directory_structure[rel_path] = []
                
                for file in files:
                    file_path = os.path.join(root, file)
                    file_size = os.path.getsize(file_path)
                    file_info = {
                        "name": file,
                        "size": file_size,
                        "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
                    }
                    directory_structure[rel_path].append(file_info)
                    total_size += file_size
                    file_count += 1
            
            results["details"]["structure"] = directory_structure
            results["details"]["total_size_bytes"] = total_size
            results["details"]["total_size_mb"] = round(total_size / (1024 * 1024), 2)
            results["details"]["file_count"] = file_count
            
            # æ¨©é™ç¢ºèª
            permissions = {
                "readable": os.access(self.db_path, os.R_OK),
                "writable": os.access(self.db_path, os.W_OK),
                "executable": os.access(self.db_path, os.X_OK)
            }
            results["details"]["permissions"] = permissions
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            if all(permissions.values()) and total_size > 0:
                results["status"] = "excellent"
                results["score"] = 100
            elif permissions["readable"] and permissions["writable"]:
                results["status"] = "good"
                results["score"] = 85
            else:
                results["status"] = "warning"
                results["score"] = 60
                
            print(f"   ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : {len(directory_structure)} ãƒ•ã‚©ãƒ«ãƒ€, {file_count} ãƒ•ã‚¡ã‚¤ãƒ«")
            print(f"   ğŸ’¾ ç·ã‚µã‚¤ã‚º: {results['details']['total_size_mb']} MB")
            print(f"   ğŸ”’ æ¨©é™: {permissions}")
            
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_chromadb_api_layer(self) -> Dict[str, Any]:
        """ç¬¬2å±¤: ChromaDB APIå±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬2å±¤åˆ†æ: ChromaDB APIå±¤")
        
        results = {
            "layer": "chromadb_api",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—
            collections = self.client.list_collections()
            results["details"]["collections_count"] = len(collections)
            results["details"]["collections"] = []
            
            for collection in collections:
                coll_info = {
                    "name": collection.name,
                    "id": str(collection.id),  # UUIDã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                    "metadata": collection.metadata
                }
                results["details"]["collections"].append(coll_info)
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®š
                if collection.name == "sister_chat_history_v4":
                    self.collection = collection
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ
            if self.collection:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
                doc_count = self.collection.count()
                results["details"]["main_collection"] = {
                    "name": self.collection.name,
                    "document_count": doc_count,
                    "metadata": self.collection.metadata
                }
                
                # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆ
                try:
                    search_result = self.collection.query(
                        query_texts=["ãƒ†ã‚¹ãƒˆ"],
                        n_results=1
                    )
                    results["details"]["search_test"] = {
                        "success": True,
                        "result_count": len(search_result["documents"][0]) if search_result["documents"] else 0
                    }
                except Exception as e:
                    results["details"]["search_test"] = {
                        "success": False,
                        "error": str(e)
                    }
                
                print(f"   ğŸ“Š ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {self.collection.name}")
                print(f"   ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count}")
                print(f"   ğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆ: {'âœ…' if results['details']['search_test']['success'] else 'âŒ'}")
                
                # ã‚¹ã‚³ã‚¢åˆ¤å®š
                if doc_count > 0 and results["details"]["search_test"]["success"]:
                    results["status"] = "excellent"
                    results["score"] = 100
                else:
                    results["status"] = "warning"
                    results["score"] = 70
            else:
                results["status"] = "failed"
                results["details"]["error"] = "Main collection not found"
                results["score"] = 0
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_sqlite_internal_layer(self) -> Dict[str, Any]:
        """ç¬¬3å±¤: SQLiteå†…éƒ¨æ§‹é€ å±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬3å±¤åˆ†æ: SQLiteå†…éƒ¨æ§‹é€ å±¤")
        
        results = {
            "layer": "sqlite_internal",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            sqlite_files = []
            for root, dirs, files in os.walk(self.db_path):
                for file in files:
                    if file.endswith('.sqlite') or file.endswith('.db') or 'chroma' in file.lower():
                        sqlite_files.append(os.path.join(root, file))
            
            results["details"]["sqlite_files"] = sqlite_files
            print(f"   ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(sqlite_files)}")
            
            if not sqlite_files:
                # ChromaDBã®å†…éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã‚’æ¢ã™
                chroma_files = []
                for root, dirs, files in os.walk(self.db_path):
                    for file in files:
                        if any(ext in file.lower() for ext in ['sqlite', 'db', 'data', 'index']):
                            chroma_files.append(os.path.join(root, file))
                
                results["details"]["chroma_internal_files"] = chroma_files
                print(f"   ğŸ“ ChromaDBå†…éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(chroma_files)}")
                
                if len(chroma_files) > 0:
                    results["status"] = "good"
                    results["score"] = 80
                    results["details"]["note"] = "ChromaDBå†…éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã‚’æ¤œå‡º"
                else:
                    results["status"] = "warning"
                    results["details"]["error"] = "No database files found"
                    results["score"] = 50
                return results
            
            # ãƒ¡ã‚¤ãƒ³ã®SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
            main_db = sqlite_files[0]
            print(f"   ğŸ—„ï¸  ãƒ¡ã‚¤ãƒ³DB: {os.path.basename(main_db)}")
            
            try:
                with sqlite3.connect(main_db) as conn:
                    cursor = conn.cursor()
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
                    tables = [row[0] for row in cursor.fetchall()]
                    results["details"]["tables"] = tables
                    results["details"]["table_count"] = len(tables)
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='index';")
                    indexes = [row[0] for row in cursor.fetchall()]
                    results["details"]["indexes"] = indexes
                    results["details"]["index_count"] = len(indexes)
                    
                    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®è©³ç´°æƒ…å ±
                    table_details = {}
                    for table in tables[:10]:  # æœ€åˆã®10ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿
                        try:
                            cursor.execute(f"SELECT COUNT(*) FROM `{table}`;")
                            row_count = cursor.fetchone()[0]
                            table_details[table] = {"row_count": row_count}
                        except Exception as e:
                            table_details[table] = {"error": str(e)}
                    
                    results["details"]["table_details"] = table_details
                    
                    print(f"   ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
                    print(f"   ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(indexes)}")
                    
                    # ã‚¹ã‚³ã‚¢åˆ¤å®š
                    if len(tables) >= 10 and len(indexes) > 0:
                        results["status"] = "excellent"
                        results["score"] = 95
                    elif len(tables) >= 5:
                        results["status"] = "good"
                        results["score"] = 80
                    else:
                        results["status"] = "warning"
                        results["score"] = 60
                        
            except Exception as e:
                results["status"] = "warning"
                results["details"]["sqlite_error"] = str(e)
                results["score"] = 70
                print(f"   âš ï¸ SQLiteåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_vector_embeddings_layer(self) -> Dict[str, Any]:
        """ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤ã®åˆ†æï¼ˆå®‰å…¨ç‰ˆï¼‰"""
        print("\nğŸ” ç¬¬4å±¤åˆ†æ: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤")
        
        results = {
            "layer": "vector_embeddings",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # å°‘æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            sample_data = self.collection.get(limit=3, include=['embeddings', 'metadatas', 'documents'])
            
            if not sample_data['embeddings'] or len(sample_data['embeddings']) == 0:
                results["status"] = "failed"
                results["details"]["error"] = "No embeddings found"
                results["score"] = 0
                return results
            
            embeddings = sample_data['embeddings']
            results["details"]["sample_count"] = len(embeddings)
            
            # ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒåˆ†æ
            if embeddings and len(embeddings) > 0:
                vector_dimensions = len(embeddings[0]) if embeddings[0] else 0
                results["details"]["vector_dimensions"] = vector_dimensions
                
                # å®‰å…¨ãªãƒ™ã‚¯ã‚¿ãƒ¼çµ±è¨ˆ
                try:
                    first_vector = np.array(embeddings[0])
                    results["details"]["vector_stats"] = {
                        "dimensions": vector_dimensions,
                        "sample_vector_norm": float(np.linalg.norm(first_vector)),
                        "sample_vector_mean": float(np.mean(first_vector)),
                        "sample_vector_std": float(np.std(first_vector))
                    }
                    
                    # ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ç‰ˆï¼‰
                    zero_count = 0
                    for emb in embeddings:
                        emb_array = np.array(emb)
                        if np.allclose(emb_array, 0):
                            zero_count += 1
                    
                    results["details"]["zero_vectors"] = zero_count
                    results["details"]["non_zero_vectors"] = len(embeddings) - zero_count
                    
                    print(f"   ğŸ”¢ ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒ: {vector_dimensions}")
                    print(f"   ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(embeddings)}")
                    print(f"   ğŸ¯ ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {zero_count}")
                    
                except Exception as e:
                    print(f"   âš ï¸ ãƒ™ã‚¯ã‚¿ãƒ¼çµ±è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
                    results["details"]["stats_error"] = str(e)
                
                # ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ãƒ†ã‚¹ãƒˆ
                try:
                    test_queries = ["ãƒ†ã‚¹ãƒˆæ¤œç´¢", "å§‰å¦¹", "ä¼šè©±"]
                    search_successes = 0
                    
                    for query in test_queries:
                        try:
                            search_results = self.collection.query(
                                query_texts=[query],
                                n_results=2
                            )
                            
                            if search_results['documents'] and len(search_results['documents'][0]) > 0:
                                search_successes += 1
                        except:
                            continue
                    
                    results["details"]["search_success_rate"] = search_successes / len(test_queries)
                    print(f"   ğŸ” æ¤œç´¢æˆåŠŸç‡: {search_successes}/{len(test_queries)}")
                    
                except Exception as e:
                    results["details"]["search_error"] = str(e)
                    print(f"   âŒ æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                
                # ã‚¹ã‚³ã‚¢åˆ¤å®š
                quality_score = 100
                
                if vector_dimensions < 100:
                    quality_score -= 10
                if zero_count > 0:
                    quality_score -= 20
                if "search_error" in results["details"]:
                    quality_score -= 30
                elif results["details"].get("search_success_rate", 0) < 0.5:
                    quality_score -= 20
                
                results["score"] = max(0, quality_score)
                
                if quality_score >= 90:
                    results["status"] = "excellent"
                elif quality_score >= 70:
                    results["status"] = "good"
                else:
                    results["status"] = "warning"
            else:
                results["status"] = "failed"
                results["details"]["error"] = "No valid embeddings found"
                results["score"] = 0
                    
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_data_integrity_layer(self) -> Dict[str, Any]:
        """ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤ã®åˆ†æï¼ˆå®‰å…¨ç‰ˆï¼‰"""
        print("\nğŸ” ç¬¬5å±¤åˆ†æ: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤")
        
        results = {
            "layer": "data_integrity",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # é©åº¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§åˆ†æ
            total_count = self.collection.count()
            sample_limit = min(total_count, 50)  # æœ€å¤§50ä»¶ã§åˆ†æ
            
            all_data = self.collection.get(
                limit=sample_limit,
                include=['metadatas', 'documents', 'embeddings']
            )
            
            results["details"]["analyzed_documents"] = len(all_data['documents']) if all_data['documents'] else 0
            results["details"]["total_documents"] = total_count
            
            # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            metadata_issues = []
            if all_data['metadatas']:
                for i, metadata in enumerate(all_data['metadatas']):
                    if metadata is None:
                        metadata_issues.append(f"Document {i}: Null metadata")
                    elif not isinstance(metadata, dict):
                        metadata_issues.append(f"Document {i}: Invalid metadata type")
            
            results["details"]["metadata_issues"] = len(metadata_issues)
            results["details"]["metadata_integrity_score"] = max(0, 100 - len(metadata_issues) * 5)
            
            # 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡å˜ç‰ˆï¼‰
            duplicate_count = 0
            if all_data['documents']:
                doc_texts = [doc for doc in all_data['documents'] if doc]
                unique_docs = set(doc_texts)
                duplicate_count = len(doc_texts) - len(unique_docs)
            
            results["details"]["duplicate_count"] = duplicate_count
            results["details"]["uniqueness_ratio"] = (
                (results["details"]["analyzed_documents"] - duplicate_count) / 
                max(1, results["details"]["analyzed_documents"])
            )
            
            # 3. åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ã‚¿ãƒ¼æ•´åˆæ€§
            embedding_issues = 0
            if all_data['embeddings']:
                expected_dim = len(all_data['embeddings'][0]) if all_data['embeddings'][0] else 0
                
                for embedding in all_data['embeddings']:
                    if embedding is None:
                        embedding_issues += 1
                    elif len(embedding) != expected_dim:
                        embedding_issues += 1
            
            results["details"]["embedding_issues"] = embedding_issues
            results["details"]["embedding_integrity_score"] = max(0, 100 - embedding_issues * 10)
            
            # 4. åŸºæœ¬æ¤œç´¢å“è³ªãƒ†ã‚¹ãƒˆ
            search_tests = ["å§‰å¦¹", "ä¼šè©±", "è³ªå•"]
            successful_searches = 0
            
            for test_query in search_tests:
                try:
                    search_result = self.collection.query(
                        query_texts=[test_query],
                        n_results=3
                    )
                    if search_result['documents'] and search_result['documents'][0]:
                        successful_searches += 1
                except:
                    continue
            
            search_quality_score = (successful_searches / len(search_tests)) * 100
            results["details"]["search_quality_score"] = search_quality_score
            
            print(f"   ğŸ“Š åˆ†æãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {results['details']['analyzed_documents']}/{total_count}")
            print(f"   ğŸ” ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å•é¡Œ: {len(metadata_issues)}")
            print(f"   ğŸ”„ é‡è¤‡æ•°: {duplicate_count}")
            print(f"   ğŸ¯ æ¤œç´¢å“è³ª: {search_quality_score:.1f}/100")
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            total_score = 100
            total_score -= min(30, len(metadata_issues) * 5)  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å•é¡Œ
            total_score -= min(20, duplicate_count * 10)  # é‡è¤‡
            total_score -= min(30, embedding_issues * 10)  # åŸ‹ã‚è¾¼ã¿å•é¡Œ
            total_score = (total_score + search_quality_score) / 2  # æ¤œç´¢å“è³ªã‚’å¹³å‡
            
            results["score"] = max(0, int(total_score))
            
            if total_score >= 95:
                results["status"] = "excellent"
            elif total_score >= 80:
                results["status"] = "good"
            elif total_score >= 60:
                results["status"] = "warning"
            else:
                results["status"] = "failed"
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆå®‰å…¨ç‰ˆï¼‰"""
        print("\n" + "="*80)
        print("ğŸ”¬ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰")
        print("="*80)
        
        # å…¨å±¤ã®åˆ†æã‚’å®Ÿè¡Œ
        layer_results = []
        
        # ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
        layer_results.append(self.analyze_filesystem_layer())
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        if self.connect_to_database():
            # ç¬¬2å±¤: ChromaDB API
            layer_results.append(self.analyze_chromadb_api_layer())
            
            # ç¬¬3å±¤: SQLiteå†…éƒ¨
            layer_results.append(self.analyze_sqlite_internal_layer())
            
            # ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿
            layer_results.append(self.analyze_vector_embeddings_layer())
            
            # ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            layer_results.append(self.analyze_data_integrity_layer())
        
        # ç·åˆè©•ä¾¡
        scores = [result.get("score", 0) for result in layer_results]
        total_score = sum(scores) / len(scores) if scores else 0
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if total_score >= 95:
            overall_status = "EXCELLENT"
            status_emoji = "ğŸŒŸ"
        elif total_score >= 85:
            overall_status = "GOOD"
            status_emoji = "âœ…"
        elif total_score >= 70:
            overall_status = "WARNING"
            status_emoji = "âš ï¸"
        else:
            overall_status = "CRITICAL"
            status_emoji = "âŒ"
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        final_report = {
            "analysis_timestamp": datetime.now().isoformat(),
            "database_path": self.db_path,
            "overall_status": overall_status,
            "overall_score": round(total_score, 2),
            "layer_count": len(layer_results),
            "layer_results": layer_results,
            "summary": {
                "status_emoji": status_emoji,
                "recommendations": self.generate_recommendations(layer_results),
                "health_metrics": self.calculate_health_metrics(layer_results)
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        print(f"\n{status_emoji} ç·åˆè©•ä¾¡: {overall_status} ({total_score:.1f}/100)")
        print(f"ğŸ“Š åˆ†æå±¤æ•°: {len(layer_results)}")
        print(f"â° åˆ†ææ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"\nğŸ“‹ å±¤åˆ¥ã‚¹ã‚³ã‚¢:")
        for result in layer_results:
            status_icon = {
                "excellent": "ğŸŒŸ",
                "good": "âœ…", 
                "warning": "âš ï¸",
                "failed": "âŒ",
                "unknown": "â“"
            }.get(result.get("status", "unknown"), "â“")
            
            layer_name = result.get("layer", "Unknown")
            score = result.get("score", 0)
            print(f"   {status_icon} {layer_name}: {score}/100")
        
        # æ¨å¥¨äº‹é …
        recommendations = final_report["summary"]["recommendations"]
        if recommendations:
            print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        return final_report
    
    def generate_recommendations(self, layer_results: List[Dict]) -> List[str]:
        """åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        for result in layer_results:
            status = result.get("status", "unknown")
            layer = result.get("layer", "Unknown")
            
            if status == "failed":
                recommendations.append(f"{layer}å±¤ã®ä¿®å¾©ãŒå¿…è¦ã§ã™")
            elif status == "warning":
                recommendations.append(f"{layer}å±¤ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        if not recommendations:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯è‰¯å¥½ãªçŠ¶æ…‹ã§ã™ã€‚å®šæœŸçš„ãªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚")
        
        return recommendations
    
    def calculate_health_metrics(self, layer_results: List[Dict]) -> Dict[str, Any]:
        """å¥å…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        scores = [result.get("score", 0) for result in layer_results]
        
        return {
            "average_score": round(sum(scores) / len(scores) if scores else 0, 2),
            "min_score": min(scores) if scores else 0,
            "max_score": max(scores) if scores else 0,
            "healthy_layers": sum(1 for score in scores if score >= 80),
            "critical_layers": sum(1 for score in scores if score < 60),
            "total_layers": len(scores)
        }
    
    def save_report(self, report: Dict[str, Any], output_file: str = None):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"chromadb_v4_analysis_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)  # default=strã‚’è¿½åŠ 
            print(f"\nğŸ’¾ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_file}")
        except Exception as e:
            print(f"\nâŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ChromaDB v4ã®ãƒ‘ã‚¹
    v4_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_v4"
    
    print("ğŸš€ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æã‚’é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    print(f"ğŸ“‚ å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {v4_db_path}")
    
    # åˆ†æå™¨ã‚’åˆæœŸåŒ–
    analyzer = SafeChromaDBv4Analyzer(v4_db_path)
    
    # åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ
    report = analyzer.generate_comprehensive_report()
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    output_file = r"f:\å‰¯æ¥­\VSC_WorkSpace\MCP_ChromaDB00\chromadb_v4_comprehensive_report_safe.json"
    analyzer.save_report(report, output_file)
    
    print("\nğŸ‰ åˆ†æå®Œäº†!")
    return report

if __name__ == "__main__":
    report = main()
