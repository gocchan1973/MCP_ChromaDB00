#!/usr/bin/env python3
"""
ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç§»è¡Œãƒ»çµ±åˆãƒ„ãƒ¼ãƒ«ï¼ˆä¿®æ­£ç‰ˆï¼‰
æ—¢å­˜ã®2ã¤ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ–°ã—ã„DBã«ç§»è¡Œã—ã€çµ±åˆã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
"""
import chromadb
from chromadb.config import Settings
from pathlib import Path
import json
from datetime import datetime

def migrate_and_merge_collections():
    """æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ–°DBã«ç§»è¡Œã—ã€çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    
    # ãƒ‘ã‚¹è¨­å®š
    source_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB"
    target_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_v4"
    
    print("ğŸš€ ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç§»è¡Œãƒ»çµ±åˆé–‹å§‹")
    print("=" * 60)
    print(f"ğŸ“‚ ç§»è¡Œå…ƒ: {source_db_path}")
    print(f"ğŸ“‚ ç§»è¡Œå…ˆ: {target_db_path}")
    print()
    
    try:
        # ç§»è¡Œå…ƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        print("ğŸ“¥ ç§»è¡Œå…ƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šä¸­...")
        source_client = chromadb.PersistentClient(
            path=source_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ç§»è¡Œå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šï¼ˆæ–°è¦ä½œæˆï¼‰
        print("ğŸ“¤ ç§»è¡Œå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ä¸­...")
        target_client = chromadb.PersistentClient(
            path=target_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ç§»è¡Œå…ƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç¢ºèª
        source_collections = source_client.list_collections()
        print(f"âœ… ç§»è¡Œå…ƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(source_collections)}")
        
        collection_data = {}
        
        # å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        for collection in source_collections:
            print(f"\nğŸ“‹ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection.name}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            all_data = collection.get(
                include=['documents', 'metadatas', 'embeddings']
            )
            
            ids = all_data.get('ids', [])
            documents = all_data.get('documents', [])
            metadatas = all_data.get('metadatas', [])
            embeddings = all_data.get('embeddings', [])
            
            print(f"   ğŸ“Š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(ids)}")
            print(f"   ğŸ”¢ IDæ•°: {len(ids)}")
            print(f"   ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")
            print(f"   ğŸ·ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•°: {len(metadatas)}")
            print(f"   ğŸ§® ãƒ™ã‚¯ãƒˆãƒ«æ•°: {len(embeddings)}")
            
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            collection_data[collection.name] = {
                'ids': ids,
                'documents': documents,
                'metadatas': metadatas,
                'embeddings': embeddings,
                'original_metadata': collection.metadata
            }
        
        print(f"\nğŸ”„ ç§»è¡Œå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆé–‹å§‹...")
        
        # çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
        merged_collection_name = "merged_iruka_knowledge"
        print(f"ğŸ“š çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: {merged_collection_name}")
        
        # æ—¢å­˜ã®çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°å‰Šé™¤
        try:
            existing_collection = target_client.get_collection(merged_collection_name)
            target_client.delete_collection(merged_collection_name)
            print("   ğŸ—‘ï¸  æ—¢å­˜ã®çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        except:
            pass
        
        # æ–°ã—ã„çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯æ–‡å­—åˆ—ã®ã¿ï¼‰
        merged_collection = target_client.create_collection(
            name=merged_collection_name,
            metadata={
                "description": "çµ±åˆã•ã‚ŒãŸIrukaãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹",
                "created_at": datetime.now().isoformat(),
                "source_collections": ", ".join(list(collection_data.keys())),
                "migration_version": "v4.0"
            }
        )
        
        print("âœ… çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã—ãŸ")
        
        # ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆãƒ»è¿½åŠ 
        total_added = 0
        
        for collection_name, data in collection_data.items():
            print(f"\nğŸ“¥ '{collection_name}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­...")
            
            ids = data['ids']
            documents = data['documents']
            metadatas = data['metadatas']
            embeddings = data['embeddings']
            
            if not ids:
                print("   âš ï¸  ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
            
            # IDã®é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
            prefixed_ids = [f"{collection_name}_{doc_id}" for doc_id in ids]
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å…ƒã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
            enhanced_metadatas = []
            for i, metadata in enumerate(metadatas):
                if metadata is None:
                    metadata = {}
                
                enhanced_metadata = {}
                
                # æ—¢å­˜ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å‹ãƒã‚§ãƒƒã‚¯ã¨å¤‰æ›
                for key, value in metadata.items():
                    if isinstance(value, (str, int, float, bool)):
                        enhanced_metadata[key] = value
                    elif isinstance(value, list):
                        # ãƒªã‚¹ãƒˆã¯æ–‡å­—åˆ—ã«å¤‰æ›
                        enhanced_metadata[key] = ", ".join(str(v) for v in value)
                    else:
                        # ãã®ä»–ã®å‹ã¯æ–‡å­—åˆ—ã«å¤‰æ›
                        enhanced_metadata[key] = str(value)
                
                # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨ã¦æ–‡å­—åˆ—å‹ï¼‰
                enhanced_metadata.update({
                    'source_collection': collection_name,
                    'migration_timestamp': datetime.now().isoformat(),
                    'original_id': str(ids[i])
                })
                enhanced_metadatas.append(enhanced_metadata)
            
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã§åˆ†å‰²ã—ã¦è¿½åŠ 
            batch_size = 50
            total_docs = len(prefixed_ids)
            
            for i in range(0, total_docs, batch_size):
                end_idx = min(i + batch_size, total_docs)
                
                batch_ids = prefixed_ids[i:end_idx]
                batch_documents = documents[i:end_idx]
                batch_metadatas = enhanced_metadatas[i:end_idx]                # embeddingsã®å®‰å…¨ãªå‡¦ç†
                batch_embeddings = None
                has_embeddings = False
                
                # embeddingsé…åˆ—ã®å­˜åœ¨ç¢ºèªã‚’å®‰å…¨ã«è¡Œã†
                if embeddings is not None:
                    try:
                        # ãƒªã‚¹ãƒˆã‹ã©ã†ã‹ç¢ºèª
                        if hasattr(embeddings, '__len__'):
                            if len(embeddings) > 0:
                                batch_embeddings = embeddings[i:end_idx]
                                has_embeddings = True
                    except (ValueError, TypeError):
                        # numpyé…åˆ—ã®åˆ¤å®šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
                        has_embeddings = False
                
                try:
                    # ãƒ™ã‚¯ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã§å‡¦ç†ã‚’åˆ†ã‘ã‚‹
                    if has_embeddings and batch_embeddings:
                        merged_collection.add(
                            ids=batch_ids,
                            documents=batch_documents,
                            metadatas=batch_metadatas,
                            embeddings=batch_embeddings
                        )
                    else:
                        merged_collection.add(
                            ids=batch_ids,
                            documents=batch_documents,
                            metadatas=batch_metadatas
                        )
                    
                    print(f"   âœ… ãƒãƒƒãƒ {i//batch_size + 1}: {len(batch_ids)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ")
                    total_added += len(batch_ids)
                    
                except Exception as e:
                    print(f"   âŒ ãƒãƒƒãƒ {i//batch_size + 1} ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        print(f"\nğŸ‰ çµ±åˆå®Œäº†!")
        print(f"ğŸ“Š ç·è¿½åŠ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_added}")
        
        # çµæœç¢ºèª
        final_count = merged_collection.count()
        print(f"ğŸ” æœ€çµ‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä»¶æ•°: {final_count}")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¡¨ç¤º
        sample = merged_collection.get(limit=3)
        print(f"\nğŸ“„ çµ±åˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒ«:")
        for i, (doc_id, document, metadata) in enumerate(zip(
            sample.get('ids', []), 
            sample.get('documents', []), 
            sample.get('metadatas', [])
        )):
            print(f"   {i+1}. ID: {doc_id}")
            if document:
                preview = document[:100] + "..." if len(document) > 100 else document
                print(f"      å†…å®¹: {preview}")
            if metadata:
                print(f"      ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata}")
            print()
        
        # çµ±åˆæƒ…å ±ã‚’JSONã§ä¿å­˜
        migration_report = {
            "success": True,
            "migration_timestamp": datetime.now().isoformat(),
            "source_db": source_db_path,
            "target_db": target_db_path,
            "merged_collection_name": merged_collection_name,
            "source_collections": list(collection_data.keys()),
            "total_documents_migrated": total_added,
            "final_collection_count": final_count,
            "source_collection_details": {
                name: {
                    "document_count": len(data['ids']),
                    "has_embeddings": bool(data['embeddings'] and len(data['embeddings']) > 0)
                }
                for name, data in collection_data.items()
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_file = Path(__file__).parent / f"migration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(migration_report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_file}")
        
        return migration_report
        
    except Exception as e:
        print(f"âŒ ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    result = migrate_and_merge_collections()
    
    if result.get("success"):
        print("\nğŸ¯ ç§»è¡Œãƒ»çµ±åˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
    else:
        print(f"\nğŸ’¥ ç§»è¡Œãƒ»çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ: {result.get('error')}")
