#!/usr/bin/env python3
"""
ChromaDB v4 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ãªæ·±å±¤åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆæœ€çµ‚å®Œå…¨ç‰ˆï¼‰
numpyä½¿ç”¨ã‚’æœ€å°é™ã«æŠ‘ãˆã€å…¨å±¤ã®å®Œå…¨åˆ†æã‚’å®Ÿè¡Œ
"""

import chromadb
import os
import json
import sqlite3
from datetime import datetime
from typing import Dict, List, Any, Tuple
import hashlib
import time
from pathlib import Path
import uuid
import math

class FinalChromaDBv4Analyzer:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.client = None
        self.collection = None
        self.analysis_results = {}
        
    def connect_to_database(self) -> bool:
        """v4ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š"""
        try:
            self.client = chromadb.PersistentClient(path=self.db_path)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ: {self.db_path}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def safe_math_stats(self, values: List[float]) -> Dict[str, float]:
        """æ•°å­¦çµ±è¨ˆã‚’å®‰å…¨ã«è¨ˆç®—ï¼ˆnumpyã‚’ä½¿ç”¨ã—ãªã„ï¼‰"""
        if not values:
            return {"mean": 0, "std": 0, "min": 0, "max": 0, "count": 0}
        
        n = len(values)
        mean_val = sum(values) / n
        variance = sum((x - mean_val) ** 2 for x in values) / n
        std_val = math.sqrt(variance)
        
        return {
            "mean": round(mean_val, 6),
            "std": round(std_val, 6),
            "min": round(min(values), 6),
            "max": round(max(values), 6),
            "count": n
        }
    
    def analyze_filesystem_layer(self) -> Dict[str, Any]:
        """ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬1å±¤åˆ†æ: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤")
        
        results = {
            "layer": "filesystem",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
            db_exists = os.path.exists(self.db_path)
            results["details"]["database_exists"] = db_exists
            
            if not db_exists:
                results["status"] = "failed"
                results["details"]["error"] = "Database directory does not exist"
                results["score"] = 0
                return results
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®åˆ†æ
            directory_structure = {}
            total_size = 0
            file_count = 0
            
            for root, dirs, files in os.walk(self.db_path):
                rel_path = os.path.relpath(root, self.db_path)
                directory_structure[rel_path] = []
                
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        file_size = os.path.getsize(file_path)
                        file_info = {
                            "name": file,
                            "size": file_size,
                            "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
                        }
                        directory_structure[rel_path].append(file_info)
                        total_size += file_size
                        file_count += 1
                    except Exception as e:
                        print(f"   âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {file}: {e}")
            
            results["details"]["structure"] = directory_structure
            results["details"]["total_size_bytes"] = total_size
            results["details"]["total_size_mb"] = round(total_size / (1024 * 1024), 2)
            results["details"]["file_count"] = file_count
            
            # æ¨©é™ç¢ºèª
            permissions = {
                "readable": os.access(self.db_path, os.R_OK),
                "writable": os.access(self.db_path, os.W_OK),
                "executable": os.access(self.db_path, os.X_OK)
            }
            results["details"]["permissions"] = permissions
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            if all(permissions.values()) and total_size > 0:
                results["status"] = "excellent"
                results["score"] = 100
            elif permissions["readable"] and permissions["writable"]:
                results["status"] = "good"
                results["score"] = 85
            else:
                results["status"] = "warning"
                results["score"] = 60
                
            print(f"   ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : {len(directory_structure)} ãƒ•ã‚©ãƒ«ãƒ€, {file_count} ãƒ•ã‚¡ã‚¤ãƒ«")
            print(f"   ğŸ’¾ ç·ã‚µã‚¤ã‚º: {results['details']['total_size_mb']} MB")
            print(f"   ğŸ”’ æ¨©é™: {permissions}")
            
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_chromadb_api_layer(self) -> Dict[str, Any]:
        """ç¬¬2å±¤: ChromaDB APIå±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬2å±¤åˆ†æ: ChromaDB APIå±¤")
        
        results = {
            "layer": "chromadb_api",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—
            collections = self.client.list_collections()
            results["details"]["collections_count"] = len(collections)
            results["details"]["collections"] = []
            
            for collection in collections:
                coll_info = {
                    "name": collection.name,
                    "id": str(collection.id),  # UUIDã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                    "metadata": collection.metadata
                }
                results["details"]["collections"].append(coll_info)
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®š
                if collection.name == "sister_chat_history_v4":
                    self.collection = collection
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ
            if self.collection:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
                doc_count = self.collection.count()
                results["details"]["main_collection"] = {
                    "name": self.collection.name,
                    "document_count": doc_count,
                    "metadata": self.collection.metadata
                }
                
                # è¤‡æ•°ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆæ®µéšçš„ï¼‰
                search_tests = [
                    ("åŸºæœ¬æ¤œç´¢", "ãƒ†ã‚¹ãƒˆ"),
                    ("æ—¥æœ¬èªæ¤œç´¢", "å§‰å¦¹"),
                    ("è¤‡åˆæ¤œç´¢", "ä¼šè©± ã‚·ã‚¹ãƒ†ãƒ "),
                    ("è‹±èªæ¤œç´¢", "system"),
                    ("é•·æ–‡æ¤œç´¢", "äººå·¥çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿèƒ½ã«ã¤ã„ã¦")
                ]
                
                search_results = {}
                successful_searches = 0
                
                for test_name, query in search_tests:
                    try:
                        search_result = self.collection.query(
                            query_texts=[query],
                            n_results=2
                        )
                        
                        result_count = 0
                        if (search_result and 'documents' in search_result and 
                            search_result['documents'] and len(search_result['documents']) > 0):
                            result_count = len(search_result['documents'][0])
                        
                        search_results[test_name] = {
                            "success": result_count > 0,
                            "result_count": result_count,
                            "query": query
                        }
                        
                        if result_count > 0:
                            successful_searches += 1
                            
                    except Exception as e:
                        search_results[test_name] = {
                            "success": False,
                            "error": str(e)[:100],
                            "query": query
                        }
                
                results["details"]["search_tests"] = search_results
                results["details"]["search_success_rate"] = successful_searches / len(search_tests)
                
                print(f"   ğŸ“Š ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {self.collection.name}")
                print(f"   ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count}")
                print(f"   ğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆæˆåŠŸ: {successful_searches}/{len(search_tests)}")
                
                # ã‚¹ã‚³ã‚¢åˆ¤å®š
                score = 60  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
                
                if doc_count > 0:
                    score += 20
                if doc_count >= 100:
                    score += 10
                    
                success_rate = successful_searches / len(search_tests)
                if success_rate >= 1.0:
                    score += 10
                elif success_rate >= 0.8:
                    score += 7
                elif success_rate >= 0.6:
                    score += 3
                
                results["score"] = min(100, score)
                
                if results["score"] >= 95:
                    results["status"] = "excellent"
                elif results["score"] >= 80:
                    results["status"] = "good"
                else:
                    results["status"] = "warning"
            else:
                results["status"] = "failed"
                results["details"]["error"] = "Main collection not found"
                results["score"] = 0
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_sqlite_internal_layer(self) -> Dict[str, Any]:
        """ç¬¬3å±¤: SQLiteå†…éƒ¨æ§‹é€ å±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬3å±¤åˆ†æ: SQLiteå†…éƒ¨æ§‹é€ å±¤")
        
        results = {
            "layer": "sqlite_internal",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            sqlite_files = []
            for root, dirs, files in os.walk(self.db_path):
                for file in files:
                    if (file.endswith('.sqlite') or file.endswith('.sqlite3') or 
                        file.endswith('.db') or 'chroma' in file.lower()):
                        sqlite_files.append(os.path.join(root, file))
            
            results["details"]["sqlite_files"] = [os.path.basename(f) for f in sqlite_files]
            results["details"]["sqlite_file_count"] = len(sqlite_files)
            print(f"   ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(sqlite_files)}")
            
            if not sqlite_files:
                results["status"] = "warning"
                results["details"]["error"] = "No SQLite files found"
                results["score"] = 50
                return results
            
            # ãƒ¡ã‚¤ãƒ³ã®SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
            main_db = sqlite_files[0]
            print(f"   ğŸ—„ï¸  ãƒ¡ã‚¤ãƒ³DB: {os.path.basename(main_db)}")
            
            try:
                with sqlite3.connect(main_db) as conn:
                    cursor = conn.cursor()
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
                    tables = [row[0] for row in cursor.fetchall()]
                    results["details"]["tables"] = tables
                    results["details"]["table_count"] = len(tables)
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='index';")
                    indexes = [row[0] for row in cursor.fetchall()]
                    # ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é™¤å¤–
                    custom_indexes = [idx for idx in indexes if not idx.startswith('sqlite_')]
                    results["details"]["indexes"] = custom_indexes
                    results["details"]["index_count"] = len(custom_indexes)
                    
                    # ãƒ“ãƒ¥ãƒ¼ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='view';")
                    views = [row[0] for row in cursor.fetchall()]
                    results["details"]["views"] = views
                    results["details"]["view_count"] = len(views)
                    
                    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆé‡è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã®åˆ†æ
                    segment_tables = [t for t in tables if 'segment' in t.lower()]
                    collection_tables = [t for t in tables if 'collection' in t.lower()]
                    embedding_tables = [t for t in tables if 'embedding' in t.lower()]
                    
                    results["details"]["key_tables"] = {
                        "segments": segment_tables,
                        "collections": collection_tables,
                        "embeddings": embedding_tables
                    }
                    
                    # é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œæ•°ãƒã‚§ãƒƒã‚¯
                    table_stats = {}
                    important_tables = segment_tables + collection_tables + embedding_tables
                    
                    for table in important_tables[:5]:  # æœ€å¤§5ãƒ†ãƒ¼ãƒ–ãƒ«
                        try:
                            cursor.execute(f"SELECT COUNT(*) FROM `{table}`;")
                            row_count = cursor.fetchone()[0]
                            table_stats[table] = {"row_count": row_count}
                        except Exception as e:
                            table_stats[table] = {"error": str(e)[:50]}
                    
                    results["details"]["table_stats"] = table_stats
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
                    try:
                        cursor.execute("PRAGMA page_count;")
                        page_count = cursor.fetchone()[0]
                        cursor.execute("PRAGMA page_size;")
                        page_size = cursor.fetchone()[0]
                        db_size_bytes = page_count * page_size
                        
                        results["details"]["database_size"] = {
                            "pages": page_count,
                            "page_size": page_size,
                            "total_bytes": db_size_bytes,
                            "total_mb": round(db_size_bytes / (1024 * 1024), 2)
                        }
                    except Exception as e:
                        results["details"]["size_error"] = str(e)
                    
                    print(f"   ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
                    print(f"   ğŸ” ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(custom_indexes)}")
                    print(f"   ğŸ”§ é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ={len(segment_tables)}, ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³={len(collection_tables)}")
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—
                    score = 50  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ãƒœãƒ¼ãƒŠã‚¹
                    if len(tables) >= 20:
                        score += 20
                    elif len(tables) >= 15:
                        score += 15
                    elif len(tables) >= 10:
                        score += 10
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°ãƒœãƒ¼ãƒŠã‚¹
                    if len(custom_indexes) >= 10:
                        score += 15
                    elif len(custom_indexes) >= 5:
                        score += 10
                    elif len(custom_indexes) >= 3:
                        score += 5
                    
                    # é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ãƒœãƒ¼ãƒŠã‚¹
                    if len(segment_tables) > 0:
                        score += 5
                    if len(collection_tables) > 0:
                        score += 5
                    if len(embedding_tables) > 0:
                        score += 5
                    
                    results["score"] = min(100, score)
                    
                    if results["score"] >= 90:
                        results["status"] = "excellent"
                    elif results["score"] >= 75:
                        results["status"] = "good"
                    else:
                        results["status"] = "warning"
                        
            except Exception as e:
                results["status"] = "warning"
                results["details"]["sqlite_error"] = str(e)
                results["score"] = 60
                print(f"   âš ï¸ SQLiteåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_vector_embeddings_layer(self) -> Dict[str, Any]:
        """ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤ã®åˆ†æï¼ˆnumpyå®Œå…¨å›é¿ç‰ˆï¼‰"""
        print("\nğŸ” ç¬¬4å±¤åˆ†æ: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤")
        
        results = {
            "layer": "vector_embeddings",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # æ®µéšçš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            try:
                # 1. æœ€åˆã®1ä»¶ã§embeddingã®å­˜åœ¨ç¢ºèª
                single_sample = self.collection.get(limit=1, include=['embeddings'])
                
                if (not single_sample or 'embeddings' not in single_sample or 
                    not single_sample['embeddings'] or len(single_sample['embeddings']) == 0):
                    results["status"] = "failed"
                    results["details"]["error"] = "No embeddings found in collection"
                    results["score"] = 0
                    return results
                
                first_embedding = single_sample['embeddings'][0]
                if first_embedding is None:
                    results["status"] = "failed"
                    results["details"]["error"] = "First embedding is null"
                    results["score"] = 0
                    return results
                
                # åŸºæœ¬æƒ…å ±
                vector_dimensions = len(first_embedding)
                results["details"]["vector_dimensions"] = vector_dimensions
                results["details"]["first_vector_available"] = True
                
                print(f"   ğŸ”¢ ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒ: {vector_dimensions}")
                
                # 2. å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ã®çµ±è¨ˆï¼ˆãƒªã‚¹ãƒˆã¨ã—ã¦å‡¦ç†ï¼‰
                sample_data = self.collection.get(limit=3, include=['embeddings'])
                embeddings_list = sample_data['embeddings']
                
                if embeddings_list and len(embeddings_list) > 0:
                    vector_analysis = {
                        "total_samples": len(embeddings_list),
                        "valid_vectors": 0,
                        "null_vectors": 0,
                        "zero_vectors": 0,
                        "dimension_mismatches": 0,
                        "sample_norms": []
                    }
                    
                    # å„ãƒ™ã‚¯ã‚¿ãƒ¼ã‚’å€‹åˆ¥ã«åˆ†æï¼ˆnumpyä½¿ç”¨å›é¿ï¼‰
                    for i, embedding in enumerate(embeddings_list):
                        if embedding is None:
                            vector_analysis["null_vectors"] += 1
                            continue
                        
                        if len(embedding) != vector_dimensions:
                            vector_analysis["dimension_mismatches"] += 1
                            continue
                        
                        vector_analysis["valid_vectors"] += 1
                        
                        # ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆæ‰‹å‹•ï¼‰
                        is_zero = True
                        norm_squared = 0.0
                        for val in embedding:
                            if abs(val) > 1e-10:
                                is_zero = False
                            norm_squared += val * val
                        
                        if is_zero:
                            vector_analysis["zero_vectors"] += 1
                        
                        # ãƒãƒ«ãƒ è¨ˆç®—ï¼ˆæ‰‹å‹•ï¼‰
                        norm = math.sqrt(norm_squared)
                        vector_analysis["sample_norms"].append(norm)
                    
                    results["details"]["vector_analysis"] = vector_analysis
                    
                    # çµ±è¨ˆæƒ…å ±ï¼ˆå®‰å…¨è¨ˆç®—ï¼‰
                    if vector_analysis["sample_norms"]:
                        norm_stats = self.safe_math_stats(vector_analysis["sample_norms"])
                        results["details"]["norm_statistics"] = norm_stats
                    
                    print(f"   ğŸ“Š æœ‰åŠ¹ãƒ™ã‚¯ã‚¿ãƒ¼: {vector_analysis['valid_vectors']}/{vector_analysis['total_samples']}")
                    print(f"   ğŸ¯ ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼: {vector_analysis['zero_vectors']}")
                    print(f"   âœ… æ¬¡å…ƒæ•´åˆæ€§: {vector_analysis['dimension_mismatches'] == 0}")
                
                # 3. æ¤œç´¢å“è³ªãƒ†ã‚¹ãƒˆï¼ˆæ®µéšçš„ï¼‰
                search_quality_tests = [
                    ("åŸºæœ¬æ¤œç´¢", "ãƒ†ã‚¹ãƒˆ"),
                    ("æ—¥æœ¬èª", "å§‰å¦¹"),
                    ("è¤‡åˆèª", "ä¼šè©±ã‚·ã‚¹ãƒ†ãƒ "),
                    ("è‹±èª", "conversation"),
                    ("å°‚é–€ç”¨èª", "äººå·¥çŸ¥èƒ½")
                ]
                
                search_results = {}
                successful_searches = 0
                total_results = 0
                
                for test_name, query in search_quality_tests:
                    try:
                        search_result = self.collection.query(
                            query_texts=[query],
                            n_results=3
                        )
                        
                        result_count = 0
                        if (search_result and 'documents' in search_result and 
                            search_result['documents'] and len(search_result['documents']) > 0):
                            result_count = len(search_result['documents'][0])
                        
                        search_results[test_name] = {
                            "success": result_count > 0,
                            "result_count": result_count
                        }
                        
                        if result_count > 0:
                            successful_searches += 1
                            total_results += result_count
                            
                    except Exception as e:
                        search_results[test_name] = {
                            "success": False,
                            "error": str(e)[:50]
                        }
                
                search_success_rate = successful_searches / len(search_quality_tests)
                avg_results_per_query = total_results / max(1, successful_searches)
                
                results["details"]["search_quality"] = {
                    "test_results": search_results,
                    "successful_searches": successful_searches,
                    "total_tests": len(search_quality_tests),
                    "success_rate": search_success_rate,
                    "avg_results_per_query": avg_results_per_query
                }
                
                print(f"   ğŸ” æ¤œç´¢æˆåŠŸç‡: {successful_searches}/{len(search_quality_tests)} ({search_success_rate:.1%})")
                
                # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆè©³ç´°ï¼‰
                score = 50  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
                
                # æ¬¡å…ƒãƒœãƒ¼ãƒŠã‚¹
                if vector_dimensions >= 384:
                    score += 15
                elif vector_dimensions >= 256:
                    score += 10
                elif vector_dimensions >= 128:
                    score += 5
                
                # ãƒ™ã‚¯ã‚¿ãƒ¼å“è³ªãƒœãƒ¼ãƒŠã‚¹
                if "vector_analysis" in results["details"]:
                    va = results["details"]["vector_analysis"]
                    if va["valid_vectors"] == va["total_samples"]:
                        score += 10
                    elif va["valid_vectors"] > 0:
                        score += 5
                    
                    if va["zero_vectors"] == 0:
                        score += 10
                    elif va["zero_vectors"] <= 1:
                        score += 5
                    
                    if va["dimension_mismatches"] == 0:
                        score += 5
                
                # æ¤œç´¢å“è³ªãƒœãƒ¼ãƒŠã‚¹
                if search_success_rate >= 1.0:
                    score += 10
                elif search_success_rate >= 0.8:
                    score += 8
                elif search_success_rate >= 0.6:
                    score += 5
                
                if avg_results_per_query >= 2.5:
                    score += 5
                
                results["score"] = min(100, score)
                
                if results["score"] >= 90:
                    results["status"] = "excellent"
                elif results["score"] >= 75:
                    results["status"] = "good"
                else:
                    results["status"] = "warning"
                    
            except Exception as e:
                results["status"] = "failed"
                results["details"]["error"] = f"Vector analysis failed: {str(e)}"
                results["score"] = 0
                print(f"   âŒ ãƒ™ã‚¯ã‚¿ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_data_integrity_layer(self) -> Dict[str, Any]:
        """ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤ã®åˆ†æï¼ˆæœ€çµ‚ç‰ˆï¼‰"""
        print("\nğŸ” ç¬¬5å±¤åˆ†æ: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤")
        
        results = {
            "layer": "data_integrity",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # ç·æ•°ã¨ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºæ±ºå®š
            total_count = self.collection.count()
            sample_limit = min(total_count, 25)  # ä¸­ç¨‹åº¦ã®ã‚µãƒ³ãƒ—ãƒ«
            
            # æ®µéšçš„ãƒ‡ãƒ¼ã‚¿å–å¾—
            all_data = self.collection.get(
                limit=sample_limit,
                include=['metadatas', 'documents', 'ids']
            )
            
            results["details"]["analyzed_documents"] = sample_limit
            results["details"]["total_documents"] = total_count
            results["details"]["analysis_coverage"] = round((sample_limit / total_count) * 100, 1)
            
            # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´åˆæ€§åˆ†æ
            doc_integrity = {
                "total_docs": len(all_data['documents']) if all_data['documents'] else 0,
                "valid_docs": 0,
                "empty_docs": 0,
                "null_docs": 0,
                "avg_doc_length": 0,
                "doc_lengths": []
            }
            
            if all_data['documents']:
                total_length = 0
                for doc in all_data['documents']:
                    if doc is None:
                        doc_integrity["null_docs"] += 1
                    elif doc == "":
                        doc_integrity["empty_docs"] += 1
                    else:
                        doc_integrity["valid_docs"] += 1
                        doc_length = len(doc)
                        doc_integrity["doc_lengths"].append(doc_length)
                        total_length += doc_length
                
                if doc_integrity["valid_docs"] > 0:
                    doc_integrity["avg_doc_length"] = total_length / doc_integrity["valid_docs"]
            
            results["details"]["document_integrity"] = doc_integrity
            
            # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§åˆ†æ
            meta_integrity = {
                "total_metadata": len(all_data['metadatas']) if all_data['metadatas'] else 0,
                "valid_metadata": 0,
                "null_metadata": 0,
                "invalid_metadata": 0,
                "missing_fields": 0,
                "common_fields": {}
            }
            
            if all_data['metadatas']:
                field_counts = {}
                for metadata in all_data['metadatas']:
                    if metadata is None:
                        meta_integrity["null_metadata"] += 1
                    elif not isinstance(metadata, dict):
                        meta_integrity["invalid_metadata"] += 1
                    else:
                        meta_integrity["valid_metadata"] += 1
                        
                        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰çµ±è¨ˆ
                        for field in metadata.keys():
                            field_counts[field] = field_counts.get(field, 0) + 1
                        
                        # é‡è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
                        important_fields = ['source', 'timestamp', 'type']
                        missing_count = sum(1 for field in important_fields if field not in metadata)
                        if missing_count > 0:
                            meta_integrity["missing_fields"] += missing_count
                
                # å…±é€šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ç‰¹å®š
                if meta_integrity["valid_metadata"] > 0:
                    for field, count in field_counts.items():
                        meta_integrity["common_fields"][field] = {
                            "count": count,
                            "percentage": round((count / meta_integrity["valid_metadata"]) * 100, 1)
                        }
            
            results["details"]["metadata_integrity"] = meta_integrity
            
            # 3. IDæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            id_integrity = {
                "total_ids": len(all_data['ids']) if all_data['ids'] else 0,
                "unique_ids": 0,
                "duplicate_ids": 0,
                "null_ids": 0
            }
            
            if all_data['ids']:
                id_set = set()
                for doc_id in all_data['ids']:
                    if doc_id is None:
                        id_integrity["null_ids"] += 1
                    else:
                        if doc_id in id_set:
                            id_integrity["duplicate_ids"] += 1
                        id_set.add(doc_id)
                
                id_integrity["unique_ids"] = len(id_set)
            
            results["details"]["id_integrity"] = id_integrity
            
            # 4. é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œå‡ºï¼ˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
            duplication_analysis = {
                "total_analyzed": doc_integrity["valid_docs"],
                "unique_documents": 0,
                "duplicate_documents": 0,
                "similarity_groups": 0
            }
            
            if all_data['documents'] and doc_integrity["valid_docs"] > 0:
                doc_hashes = {}
                for i, doc in enumerate(all_data['documents']):
                    if doc and doc.strip():
                        doc_hash = hashlib.md5(doc.encode('utf-8')).hexdigest()
                        if doc_hash in doc_hashes:
                            duplication_analysis["duplicate_documents"] += 1
                        else:
                            doc_hashes[doc_hash] = i
                
                duplication_analysis["unique_documents"] = len(doc_hashes)
                duplication_analysis["similarity_groups"] = len(doc_hashes)
            
            results["details"]["duplication_analysis"] = duplication_analysis
            
            # 5. ç·åˆæ¤œç´¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
            search_integrity_tests = [
                ("åŸºæœ¬æ©Ÿèƒ½", "ãƒ†ã‚¹ãƒˆ"),
                ("æ—¥æœ¬èªå¯¾å¿œ", "å§‰å¦¹ãƒãƒ£ãƒƒãƒˆ"),
                ("è¤‡åˆæ¤œç´¢", "äººå·¥çŸ¥èƒ½ ä¼šè©±"),
                ("çŸ­æ–‡æ¤œç´¢", "AI"),
                ("é•·æ–‡æ¤œç´¢", "äººå·¥çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹è‡ªç„¶è¨€èªå‡¦ç†æ©Ÿèƒ½"),
                ("è‹±èªæ¤œç´¢", "artificial intelligence"),
                ("è¨˜å·æ¤œç´¢", "AIãƒ»ML")
            ]
            
            search_test_results = {}
            successful_tests = 0
            total_result_count = 0
            
            for test_name, query in search_integrity_tests:
                try:
                    result = self.collection.query(
                        query_texts=[query],
                        n_results=3
                    )
                    
                    result_count = 0
                    if (result and 'documents' in result and result['documents'] and 
                        len(result['documents']) > 0 and result['documents'][0]):
                        result_count = len(result['documents'][0])
                    
                    search_test_results[test_name] = {
                        "success": result_count > 0,
                        "result_count": result_count,
                        "query_length": len(query)
                    }
                    
                    if result_count > 0:
                        successful_tests += 1
                        total_result_count += result_count
                        
                except Exception as e:
                    search_test_results[test_name] = {
                        "success": False,
                        "error": str(e)[:50],
                        "query_length": len(query)
                    }
            
            search_integrity = {
                "test_results": search_test_results,
                "successful_tests": successful_tests,
                "total_tests": len(search_integrity_tests),
                "success_rate": successful_tests / len(search_integrity_tests),
                "avg_results_per_test": total_result_count / max(1, successful_tests)
            }
            
            results["details"]["search_integrity"] = search_integrity
            
            print(f"   ğŸ“Š åˆ†æã‚«ãƒãƒ¼ç‡: {results['details']['analysis_coverage']}%")
            print(f"   ğŸ“„ æœ‰åŠ¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {doc_integrity['valid_docs']}/{doc_integrity['total_docs']}")
            print(f"   ğŸ·ï¸  æœ‰åŠ¹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {meta_integrity['valid_metadata']}/{meta_integrity['total_metadata']}")
            print(f"   ğŸ”„ é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {duplication_analysis['duplicate_documents']}")
            print(f"   ğŸ” æ¤œç´¢æˆåŠŸç‡: {successful_tests}/{len(search_integrity_tests)} ({search_integrity['success_rate']:.1%})")
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            score = 60  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå“è³ªã‚¹ã‚³ã‚¢
            if doc_integrity["total_docs"] > 0:
                doc_quality = doc_integrity["valid_docs"] / doc_integrity["total_docs"]
                score += doc_quality * 15
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢
            if meta_integrity["total_metadata"] > 0:
                meta_quality = meta_integrity["valid_metadata"] / meta_integrity["total_metadata"]
                score += meta_quality * 10
            
            # IDæ•´åˆæ€§ã‚¹ã‚³ã‚¢
            if id_integrity["total_ids"] > 0:
                id_quality = (id_integrity["total_ids"] - id_integrity["duplicate_ids"] - id_integrity["null_ids"]) / id_integrity["total_ids"]
                score += id_quality * 5
            
            # é‡è¤‡ãƒšãƒŠãƒ«ãƒ†ã‚£
            if duplication_analysis["total_analyzed"] > 0:
                dup_rate = duplication_analysis["duplicate_documents"] / duplication_analysis["total_analyzed"]
                score -= dup_rate * 10
            
            # æ¤œç´¢å“è³ªãƒœãƒ¼ãƒŠã‚¹
            score += search_integrity["success_rate"] * 10
            
            results["score"] = max(0, min(100, int(score)))
            
            if results["score"] >= 95:
                results["status"] = "excellent"
            elif results["score"] >= 85:
                results["status"] = "good"
            elif results["score"] >= 70:
                results["status"] = "warning"
            else:
                results["status"] = "failed"
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆæœ€çµ‚ç‰ˆï¼‰"""
        print("\n" + "="*80)
        print("ğŸ”¬ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆæœ€çµ‚å®Œå…¨ç‰ˆï¼‰")
        print("="*80)
        
        # å…¨å±¤ã®åˆ†æã‚’å®Ÿè¡Œ
        layer_results = []
        
        print("\nğŸš€ 5å±¤æ·±å±¤åˆ†æã‚’é–‹å§‹...")
        
        # ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
        layer_results.append(self.analyze_filesystem_layer())
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        if self.connect_to_database():
            # ç¬¬2å±¤: ChromaDB API
            layer_results.append(self.analyze_chromadb_api_layer())
            
            # ç¬¬3å±¤: SQLiteå†…éƒ¨
            layer_results.append(self.analyze_sqlite_internal_layer())
            
            # ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿
            layer_results.append(self.analyze_vector_embeddings_layer())
            
            # ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            layer_results.append(self.analyze_data_integrity_layer())
        else:
            print("\nâŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—ã®ãŸã‚ã€ä¸Šä½å±¤ã®åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        # ç·åˆè©•ä¾¡è¨ˆç®—
        scores = [result.get("score", 0) for result in layer_results]
        total_score = sum(scores) / len(scores) if scores else 0
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šï¼ˆè©³ç´°ï¼‰
        if total_score >= 98:
            overall_status = "PERFECT"
            status_emoji = "ğŸ’"
            status_description = "å®Œç’§ãªçŠ¶æ…‹"
        elif total_score >= 95:
            overall_status = "EXCELLENT"
            status_emoji = "ğŸŒŸ"
            status_description = "å„ªç§€ãªçŠ¶æ…‹"
        elif total_score >= 85:
            overall_status = "GOOD"
            status_emoji = "âœ…"
            status_description = "è‰¯å¥½ãªçŠ¶æ…‹"
        elif total_score >= 70:
            overall_status = "WARNING"
            status_emoji = "âš ï¸"
            status_description = "æ³¨æ„ãŒå¿…è¦"
        elif total_score >= 50:
            overall_status = "POOR"
            status_emoji = "ğŸ”¶"
            status_description = "æ”¹å–„ãŒå¿…è¦"
        else:
            overall_status = "CRITICAL"
            status_emoji = "âŒ"
            status_description = "ç·Šæ€¥ä¿®å¾©ãŒå¿…è¦"
        
        # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        health_metrics = self.calculate_comprehensive_metrics(layer_results)
        recommendations = self.generate_expert_recommendations(layer_results, total_score)
        technical_summary = self.generate_technical_summary(layer_results)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆæ§‹ç¯‰
        final_report = {
            "analysis_metadata": {
                "timestamp": datetime.now().isoformat(),
                "analyzer_version": "1.0.0",
                "database_path": self.db_path,
                "analysis_duration": "complete"
            },
            "overall_assessment": {
                "status": overall_status,
                "score": round(total_score, 2),
                "description": status_description,
                "emoji": status_emoji
            },
            "layer_analysis": {
                "total_layers": len(layer_results),
                "completed_layers": len([r for r in layer_results if r.get("score", 0) > 0]),
                "results": layer_results
            },
            "health_metrics": health_metrics,
            "technical_summary": technical_summary,
            "recommendations": recommendations,
            "executive_summary": self.generate_executive_summary(layer_results, total_score)
        }
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        print(f"\n{status_emoji} ğŸ“Š æœ€çµ‚ç·åˆè©•ä¾¡")
        print("="*50)
        print(f"ğŸ† ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {overall_status} ({total_score:.1f}/100)")
        print(f"ğŸ“ è©•ä¾¡: {status_description}")
        print(f"â° åˆ†æå®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ” åˆ†æå±¤æ•°: {len(layer_results)}")
        
        print(f"\nğŸ“‹ å±¤åˆ¥è©³ç´°åˆ†æçµæœ:")
        print("-" * 60)
        for i, result in enumerate(layer_results, 1):
            status_icon = {
                "excellent": "ğŸŒŸ",
                "good": "âœ…", 
                "warning": "âš ï¸",
                "failed": "âŒ",
                "unknown": "â“"
            }.get(result.get("status", "unknown"), "â“")
            
            layer_name = result.get("layer", f"Layer_{i}").replace("_", " ").title()
            score = result.get("score", 0)
            status = result.get("status", "unknown").upper()
            
            print(f"ç¬¬{i}å±¤ {status_icon} {layer_name:20s}: {score:3d}/100 ({status})")
        
        # å¥å…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        print(f"\nğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        print("-" * 40)
        for key, value in health_metrics.items():
            if key not in ["layer_distribution", "technical_details"]:
                display_key = key.replace("_", " ").title()
                print(f"   â€¢ {display_key}: {value}")
        
        # æŠ€è¡“ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        if technical_summary:
            print(f"\nğŸ”§ æŠ€è¡“ã‚µãƒãƒªãƒ¼:")
            print("-" * 30)
            for key, value in technical_summary.items():
                if isinstance(value, (int, float, str)):
                    display_key = key.replace("_", " ").title()
                    print(f"   â€¢ {display_key}: {value}")
        
        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ¨å¥¨äº‹é …
        if recommendations:
            print(f"\nğŸ’¡ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ¨å¥¨äº‹é …:")
            print("-" * 40)
            for i, rec in enumerate(recommendations, 1):
                priority = "ğŸ”´" if "ç·Šæ€¥" in rec or "ä¿®å¾©" in rec else "ğŸŸ¡" if "æ”¹å–„" in rec else "ğŸŸ¢"
                print(f"   {priority} {i}. {rec}")
        
        return final_report
    
    def calculate_comprehensive_metrics(self, layer_results: List[Dict]) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        scores = [result.get("score", 0) for result in layer_results]
        statuses = [result.get("status", "unknown") for result in layer_results]
        
        status_counts = {}
        for status in statuses:
            status_counts[status] = status_counts.get(status, 0) + 1
        
        # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        metrics = {
            "average_score": round(sum(scores) / len(scores) if scores else 0, 2),
            "median_score": sorted(scores)[len(scores)//2] if scores else 0,
            "min_score": min(scores) if scores else 0,
            "max_score": max(scores) if scores else 0,
            "score_variance": round(sum((s - sum(scores)/len(scores))**2 for s in scores) / len(scores), 2) if scores else 0,
            "excellent_layers": status_counts.get("excellent", 0),
            "good_layers": status_counts.get("good", 0),
            "warning_layers": status_counts.get("warning", 0),
            "failed_layers": status_counts.get("failed", 0),
            "healthy_percentage": round((status_counts.get("excellent", 0) + status_counts.get("good", 0)) / len(layer_results) * 100, 1),
            "critical_percentage": round((status_counts.get("failed", 0) + status_counts.get("warning", 0)) / len(layer_results) * 100, 1),
            "total_layers": len(layer_results),
            "consistency_score": 100 - (max(scores) - min(scores)) if scores else 0
        }
        
        return metrics
    
    def generate_technical_summary(self, layer_results: List[Dict]) -> Dict[str, Any]:
        """æŠ€è¡“çš„ãªã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        summary = {}
        
        for result in layer_results:
            layer = result.get("layer", "unknown")
            details = result.get("details", {})
            
            if layer == "filesystem":
                summary["total_size_mb"] = details.get("total_size_mb", 0)
                summary["file_count"] = details.get("file_count", 0)
            elif layer == "chromadb_api":
                main_coll = details.get("main_collection", {})
                summary["document_count"] = main_coll.get("document_count", 0)
                summary["collections_count"] = details.get("collections_count", 0)
            elif layer == "sqlite_internal":
                summary["table_count"] = details.get("table_count", 0)
                summary["index_count"] = details.get("index_count", 0)
            elif layer == "vector_embeddings":
                summary["vector_dimensions"] = details.get("vector_dimensions", 0)
                search_qual = details.get("search_quality", {})
                summary["search_success_rate"] = search_qual.get("success_rate", 0)
            elif layer == "data_integrity":
                summary["analysis_coverage"] = details.get("analysis_coverage", 0)
                doc_int = details.get("document_integrity", {})
                summary["document_validity_rate"] = round(doc_int.get("valid_docs", 0) / max(1, doc_int.get("total_docs", 1)), 3)
        
        return summary
    
    def generate_expert_recommendations(self, layer_results: List[Dict], total_score: float) -> List[str]:
        """ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ã®æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # å„å±¤ã®å•é¡Œã‚’ç‰¹å®š
        for result in layer_results:
            layer = result.get("layer", "unknown")
            status = result.get("status", "unknown")
            score = result.get("score", 0)
            
            if status == "failed":
                if layer == "vector_embeddings":
                    recommendations.append("ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤ã®ç·Šæ€¥ä¿®å¾©: embeddingç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã®è¦‹ç›´ã—ãŒå¿…è¦")
                else:
                    recommendations.append(f"{layer}å±¤ã®ç·Šæ€¥ä¿®å¾©ãŒå¿…è¦ã§ã™ï¼ˆã‚¹ã‚³ã‚¢: {score}ï¼‰")
            elif status == "warning":
                if score < 80:
                    recommendations.append(f"{layer}å±¤ã®æœ€é©åŒ–ã‚’æ¨å¥¨ã—ã¾ã™ï¼ˆç¾åœ¨ã‚¹ã‚³ã‚¢: {score}ï¼‰")
        
        # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …
        if total_score >= 95:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ ã¯æœ€é©ãªçŠ¶æ…‹ã§ã™ã€‚ç¾åœ¨ã®å“è³ªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¦ãã ã•ã„")
        elif total_score >= 85:
            recommendations.append("å®šæœŸçš„ãªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹å“è³ªç¶­æŒã‚’ç¶™ç¶šã—ã¦ãã ã•ã„")
        elif total_score >= 70:
            recommendations.append("ä¸€éƒ¨å±¤ã®æ”¹å–„ã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™")
        elif total_score >= 50:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æœ€é©åŒ–ã¨æ”¹å–„è¨ˆç”»ã®ç­–å®šãŒæ¨å¥¨ã•ã‚Œã¾ã™")
        else:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç·Šæ€¥è¦‹ç›´ã—ã¨ä¿®å¾©ä½œæ¥­ãŒå¿…è¦ã§ã™")
        
        # ç‰¹å®šã®æŠ€è¡“çš„æ¨å¥¨äº‹é …
        vector_layer = next((r for r in layer_results if r.get("layer") == "vector_embeddings"), None)
        if vector_layer and vector_layer.get("status") == "failed":
            recommendations.append("ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†æ§‹ç¯‰ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return recommendations
    
    def generate_executive_summary(self, layer_results: List[Dict], total_score: float) -> Dict[str, Any]:
        """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        passed_layers = [r for r in layer_results if r.get("score", 0) >= 70]
        failed_layers = [r for r in layer_results if r.get("score", 0) < 50]
        
        return {
            "overall_health": "Excellent" if total_score >= 90 else "Good" if total_score >= 75 else "Poor",
            "key_strengths": [r.get("layer") for r in layer_results if r.get("score", 0) >= 90],
            "critical_issues": [r.get("layer") for r in failed_layers],
            "immediate_actions_required": len(failed_layers),
            "system_stability": "High" if len(failed_layers) == 0 else "Medium" if len(failed_layers) <= 1 else "Low",
            "recommendation_priority": "Low" if total_score >= 90 else "Medium" if total_score >= 70 else "High"
        }
    
    def save_comprehensive_report(self, report: Dict[str, Any], output_file: str = None) -> str:
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"chromadb_v4_final_comprehensive_report_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ’¾ æœ€çµ‚åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_file}")
            return output_file
        except Exception as e:
            print(f"\nâŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ChromaDB v4ã®ãƒ‘ã‚¹
    v4_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_v4"
    
    print("ğŸš€ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æã‚’é–‹å§‹ï¼ˆæœ€çµ‚å®Œå…¨ç‰ˆï¼‰")
    print(f"ğŸ“‚ å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {v4_db_path}")
    print(f"â° é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ†æå™¨ã‚’åˆæœŸåŒ–
    analyzer = FinalChromaDBv4Analyzer(v4_db_path)
    
    # åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ
    start_time = time.time()
    report = analyzer.generate_comprehensive_report()
    end_time = time.time()
    
    print(f"\nâ±ï¸ åˆ†ææ‰€è¦æ™‚é–“: {round(end_time - start_time, 2)}ç§’")
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    output_file = r"f:\å‰¯æ¥­\VSC_WorkSpace\MCP_ChromaDB00\chromadb_v4_final_report.json"
    saved_file = analyzer.save_comprehensive_report(report, output_file)
    
    print(f"\nğŸ‰ åŒ…æ‹¬çš„æ·±å±¤åˆ†æå®Œäº†!")
    print(f"ğŸ“Š æœ€çµ‚ç·åˆã‚¹ã‚³ã‚¢: {report['overall_assessment']['score']}/100")
    print(f"ğŸ† æœ€çµ‚è©•ä¾¡: {report['overall_assessment']['status']}")
    print(f"ğŸ“‹ åˆ†æå±¤æ•°: {report['layer_analysis']['total_layers']}")
    
    return report

if __name__ == "__main__":
    report = main()
