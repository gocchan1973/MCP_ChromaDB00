#!/usr/bin/env python3
"""
ChromaDB v4 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ãªæ·±å±¤åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆå®Œå…¨å®‰å…¨ç‰ˆï¼‰
numpyé…åˆ—å•é¡Œã‚’å®Œå…¨ã«å›é¿ã—ã€å…¨å±¤ã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
"""

import chromadb
import os
import json
import sqlite3
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Tuple
import hashlib
import time
from pathlib import Path
import uuid

class UltraSafeChromaDBv4Analyzer:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.client = None
        self.collection = None
        self.analysis_results = {}
        
    def connect_to_database(self) -> bool:
        """v4ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š"""
        try:
            self.client = chromadb.PersistentClient(path=self.db_path)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ: {self.db_path}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def analyze_filesystem_layer(self) -> Dict[str, Any]:
        """ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬1å±¤åˆ†æ: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤")
        
        results = {
            "layer": "filesystem",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
            db_exists = os.path.exists(self.db_path)
            results["details"]["database_exists"] = db_exists
            
            if not db_exists:
                results["status"] = "failed"
                results["details"]["error"] = "Database directory does not exist"
                results["score"] = 0
                return results
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®åˆ†æ
            directory_structure = {}
            total_size = 0
            file_count = 0
            
            for root, dirs, files in os.walk(self.db_path):
                rel_path = os.path.relpath(root, self.db_path)
                directory_structure[rel_path] = []
                
                for file in files:
                    file_path = os.path.join(root, file)
                    file_size = os.path.getsize(file_path)
                    file_info = {
                        "name": file,
                        "size": file_size,
                        "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
                    }
                    directory_structure[rel_path].append(file_info)
                    total_size += file_size
                    file_count += 1
            
            results["details"]["structure"] = directory_structure
            results["details"]["total_size_bytes"] = total_size
            results["details"]["total_size_mb"] = round(total_size / (1024 * 1024), 2)
            results["details"]["file_count"] = file_count
            
            # æ¨©é™ç¢ºèª
            permissions = {
                "readable": os.access(self.db_path, os.R_OK),
                "writable": os.access(self.db_path, os.W_OK),
                "executable": os.access(self.db_path, os.X_OK)
            }
            results["details"]["permissions"] = permissions
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            if all(permissions.values()) and total_size > 0:
                results["status"] = "excellent"
                results["score"] = 100
            elif permissions["readable"] and permissions["writable"]:
                results["status"] = "good"
                results["score"] = 85
            else:
                results["status"] = "warning"
                results["score"] = 60
                
            print(f"   ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : {len(directory_structure)} ãƒ•ã‚©ãƒ«ãƒ€, {file_count} ãƒ•ã‚¡ã‚¤ãƒ«")
            print(f"   ğŸ’¾ ç·ã‚µã‚¤ã‚º: {results['details']['total_size_mb']} MB")
            print(f"   ğŸ”’ æ¨©é™: {permissions}")
            
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_chromadb_api_layer(self) -> Dict[str, Any]:
        """ç¬¬2å±¤: ChromaDB APIå±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬2å±¤åˆ†æ: ChromaDB APIå±¤")
        
        results = {
            "layer": "chromadb_api",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—
            collections = self.client.list_collections()
            results["details"]["collections_count"] = len(collections)
            results["details"]["collections"] = []
            
            for collection in collections:
                coll_info = {
                    "name": collection.name,
                    "id": str(collection.id),  # UUIDã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                    "metadata": collection.metadata
                }
                results["details"]["collections"].append(coll_info)
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®š
                if collection.name == "sister_chat_history_v4":
                    self.collection = collection
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ
            if self.collection:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
                doc_count = self.collection.count()
                results["details"]["main_collection"] = {
                    "name": self.collection.name,
                    "document_count": doc_count,
                    "metadata": self.collection.metadata
                }
                
                # è¤‡æ•°ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆ
                search_tests = ["ãƒ†ã‚¹ãƒˆ", "å§‰å¦¹", "ä¼šè©±"]
                search_results = {}
                
                for test_query in search_tests:
                    try:
                        search_result = self.collection.query(
                            query_texts=[test_query],
                            n_results=2
                        )
                        search_results[test_query] = {
                            "success": True,
                            "result_count": len(search_result["documents"][0]) if search_result["documents"] else 0
                        }
                    except Exception as e:
                        search_results[test_query] = {
                            "success": False,
                            "error": str(e)
                        }
                
                results["details"]["search_tests"] = search_results
                successful_searches = sum(1 for result in search_results.values() if result["success"])
                
                print(f"   ğŸ“Š ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {self.collection.name}")
                print(f"   ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count}")
                print(f"   ğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆæˆåŠŸ: {successful_searches}/{len(search_tests)}")
                
                # ã‚¹ã‚³ã‚¢åˆ¤å®š
                if doc_count > 0 and successful_searches == len(search_tests):
                    results["status"] = "excellent"
                    results["score"] = 100
                elif doc_count > 0 and successful_searches > 0:
                    results["status"] = "good"
                    results["score"] = 85
                else:
                    results["status"] = "warning"
                    results["score"] = 70
            else:
                results["status"] = "failed"
                results["details"]["error"] = "Main collection not found"
                results["score"] = 0
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_sqlite_internal_layer(self) -> Dict[str, Any]:
        """ç¬¬3å±¤: SQLiteå†…éƒ¨æ§‹é€ å±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬3å±¤åˆ†æ: SQLiteå†…éƒ¨æ§‹é€ å±¤")
        
        results = {
            "layer": "sqlite_internal",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            sqlite_files = []
            for root, dirs, files in os.walk(self.db_path):
                for file in files:
                    if (file.endswith('.sqlite') or file.endswith('.sqlite3') or 
                        file.endswith('.db') or 'chroma' in file.lower()):
                        sqlite_files.append(os.path.join(root, file))
            
            results["details"]["sqlite_files"] = [os.path.basename(f) for f in sqlite_files]
            results["details"]["sqlite_file_count"] = len(sqlite_files)
            print(f"   ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(sqlite_files)}")
            
            if not sqlite_files:
                results["status"] = "warning"
                results["details"]["error"] = "No SQLite files found"
                results["score"] = 50
                return results
            
            # ãƒ¡ã‚¤ãƒ³ã®SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
            main_db = sqlite_files[0]
            print(f"   ğŸ—„ï¸  ãƒ¡ã‚¤ãƒ³DB: {os.path.basename(main_db)}")
            
            try:
                with sqlite3.connect(main_db) as conn:
                    cursor = conn.cursor()
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
                    tables = [row[0] for row in cursor.fetchall()]
                    results["details"]["tables"] = tables
                    results["details"]["table_count"] = len(tables)
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='index';")
                    indexes = [row[0] for row in cursor.fetchall()]
                    # ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é™¤å¤–
                    custom_indexes = [idx for idx in indexes if not idx.startswith('sqlite_')]
                    results["details"]["indexes"] = custom_indexes
                    results["details"]["index_count"] = len(custom_indexes)
                    
                    # ãƒ“ãƒ¥ãƒ¼ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='view';")
                    views = [row[0] for row in cursor.fetchall()]
                    results["details"]["views"] = views
                    results["details"]["view_count"] = len(views)
                    
                    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®è©³ç´°æƒ…å ±ï¼ˆä¸»è¦ãªã‚‚ã®ã®ã¿ï¼‰
                    table_details = {}
                    important_tables = [t for t in tables if any(keyword in t.lower() 
                                       for keyword in ['collection', 'embedding', 'segment', 'metadata'])]
                    
                    for table in important_tables[:10]:  # æœ€å¤§10ãƒ†ãƒ¼ãƒ–ãƒ«
                        try:
                            cursor.execute(f"SELECT COUNT(*) FROM `{table}`;")
                            row_count = cursor.fetchone()[0]
                            
                            cursor.execute(f"PRAGMA table_info(`{table}`);")
                            schema = cursor.fetchall()
                            
                            table_details[table] = {
                                "row_count": row_count,
                                "column_count": len(schema),
                                "columns": [col[1] for col in schema[:5]]  # æœ€åˆã®5ã‚«ãƒ©ãƒ ã®ã¿
                            }
                        except Exception as e:
                            table_details[table] = {"error": str(e)}
                    
                    results["details"]["table_details"] = table_details
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
                    cursor.execute("PRAGMA page_count;")
                    page_count = cursor.fetchone()[0]
                    cursor.execute("PRAGMA page_size;")
                    page_size = cursor.fetchone()[0]
                    db_size_bytes = page_count * page_size
                    
                    results["details"]["database_size"] = {
                        "pages": page_count,
                        "page_size": page_size,
                        "total_bytes": db_size_bytes,
                        "total_mb": round(db_size_bytes / (1024 * 1024), 2)
                    }
                    
                    print(f"   ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
                    print(f"   ğŸ” ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(custom_indexes)}")
                    print(f"   ğŸ“Š DBå†…éƒ¨ã‚µã‚¤ã‚º: {results['details']['database_size']['total_mb']} MB")
                    
                    # ã‚¹ã‚³ã‚¢åˆ¤å®š
                    score = 60  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
                    if len(tables) >= 15:
                        score += 20
                    elif len(tables) >= 10:
                        score += 15
                    elif len(tables) >= 5:
                        score += 10
                    
                    if len(custom_indexes) >= 10:
                        score += 15
                    elif len(custom_indexes) >= 5:
                        score += 10
                    elif len(custom_indexes) >= 3:
                        score += 5
                    
                    if db_size_bytes > 1024 * 1024:  # 1MBä»¥ä¸Š
                        score += 5
                    
                    results["score"] = min(100, score)
                    
                    if results["score"] >= 90:
                        results["status"] = "excellent"
                    elif results["score"] >= 75:
                        results["status"] = "good"
                    else:
                        results["status"] = "warning"
                        
            except Exception as e:
                results["status"] = "warning"
                results["details"]["sqlite_error"] = str(e)
                results["score"] = 70
                print(f"   âš ï¸ SQLiteåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_vector_embeddings_layer(self) -> Dict[str, Any]:
        """ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤ã®åˆ†æï¼ˆå®Œå…¨å®‰å…¨ç‰ˆï¼‰"""
        print("\nğŸ” ç¬¬4å±¤åˆ†æ: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤")
        
        results = {
            "layer": "vector_embeddings",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # 1ä»¶ã®ã¿å–å¾—ã—ã¦embeddingã‚’ç¢ºèª
            try:
                single_sample = self.collection.get(limit=1, include=['embeddings', 'metadatas', 'documents'])
                
                if (not single_sample['embeddings'] or 
                    len(single_sample['embeddings']) == 0 or 
                    single_sample['embeddings'][0] is None):
                    results["status"] = "failed"
                    results["details"]["error"] = "No embeddings found"
                    results["score"] = 0
                    return results
                
                # åŸºæœ¬æƒ…å ±ã‚’å®‰å…¨ã«å–å¾—
                first_embedding = single_sample['embeddings'][0]
                vector_dimensions = len(first_embedding)
                results["details"]["vector_dimensions"] = vector_dimensions
                results["details"]["sample_available"] = True
                
                print(f"   ğŸ”¢ ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒ: {vector_dimensions}")
                
                # è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã§çµ±è¨ˆã‚’å–å¾—ï¼ˆlistã¨ã—ã¦æ‰±ã†ï¼‰
                sample_data = self.collection.get(limit=5, include=['embeddings'])
                embeddings_list = sample_data['embeddings']
                
                if embeddings_list and len(embeddings_list) > 0:
                    results["details"]["sample_count"] = len(embeddings_list)
                    
                    # å„ãƒ™ã‚¯ã‚¿ãƒ¼ã‚’å€‹åˆ¥ã«å‡¦ç†ï¼ˆnumpyé…åˆ—ã®æ¯”è¼ƒã‚’é¿ã‘ã‚‹ï¼‰
                    vector_stats = {
                        "dimensions": vector_dimensions,
                        "total_samples": len(embeddings_list),
                        "valid_vectors": 0,
                        "zero_vectors": 0,
                        "dimension_consistency": True
                    }
                    
                    for i, embedding in enumerate(embeddings_list):
                        if embedding is not None:
                            if len(embedding) == vector_dimensions:
                                vector_stats["valid_vectors"] += 1
                                
                                # ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆè¦ç´ ã”ã¨ã«ç¢ºèªï¼‰
                                is_zero = True
                                for val in embedding:
                                    if abs(val) > 1e-10:  # ã»ã¼ã‚¼ãƒ­ã§ã¯ãªã„
                                        is_zero = False
                                        break
                                
                                if is_zero:
                                    vector_stats["zero_vectors"] += 1
                            else:
                                vector_stats["dimension_consistency"] = False
                    
                    results["details"]["vector_stats"] = vector_stats
                    
                    print(f"   ğŸ“Š æœ‰åŠ¹ãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {vector_stats['valid_vectors']}/{vector_stats['total_samples']}")
                    print(f"   ğŸ¯ ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼æ•°: {vector_stats['zero_vectors']}")
                    print(f"   âœ… æ¬¡å…ƒæ•´åˆæ€§: {vector_stats['dimension_consistency']}")
                
                # æ¤œç´¢å“è³ªãƒ†ã‚¹ãƒˆ
                search_quality_tests = ["ãƒ†ã‚¹ãƒˆæ¤œç´¢", "å§‰å¦¹", "ä¼šè©±", "è³ªå•", "ã‚·ã‚¹ãƒ†ãƒ "]
                successful_searches = 0
                
                for test_query in search_quality_tests:
                    try:
                        search_results = self.collection.query(
                            query_texts=[test_query],
                            n_results=3
                        )
                        
                        if (search_results['documents'] and 
                            len(search_results['documents']) > 0 and
                            len(search_results['documents'][0]) > 0):
                            successful_searches += 1
                    except Exception as e:
                        continue
                
                search_success_rate = successful_searches / len(search_quality_tests)
                results["details"]["search_quality"] = {
                    "successful_searches": successful_searches,
                    "total_tests": len(search_quality_tests),
                    "success_rate": search_success_rate
                }
                
                print(f"   ğŸ” æ¤œç´¢æˆåŠŸç‡: {successful_searches}/{len(search_quality_tests)} ({search_success_rate:.1%})")
                
                # ã‚¹ã‚³ã‚¢è¨ˆç®—
                score = 70  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
                
                # æ¬¡å…ƒæ•°ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
                if vector_dimensions >= 384:
                    score += 15
                elif vector_dimensions >= 256:
                    score += 10
                elif vector_dimensions >= 128:
                    score += 5
                
                # ãƒ™ã‚¯ã‚¿ãƒ¼å“è³ª
                if "vector_stats" in results["details"]:
                    stats = results["details"]["vector_stats"]
                    if stats["dimension_consistency"]:
                        score += 5
                    if stats["zero_vectors"] == 0:
                        score += 10
                    elif stats["zero_vectors"] < stats["total_samples"] * 0.1:  # 10%æœªæº€
                        score += 5
                
                # æ¤œç´¢å“è³ª
                if search_success_rate >= 0.8:
                    score += 10
                elif search_success_rate >= 0.6:
                    score += 5
                
                results["score"] = min(100, score)
                
                if results["score"] >= 90:
                    results["status"] = "excellent"
                elif results["score"] >= 75:
                    results["status"] = "good"
                else:
                    results["status"] = "warning"
                    
            except Exception as e:
                results["status"] = "failed"
                results["details"]["error"] = f"Vector analysis error: {str(e)}"
                results["score"] = 0
                print(f"   âŒ ãƒ™ã‚¯ã‚¿ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_data_integrity_layer(self) -> Dict[str, Any]:
        """ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤ã®åˆ†æï¼ˆå®Œå…¨å®‰å…¨ç‰ˆï¼‰"""
        print("\nğŸ” ç¬¬5å±¤åˆ†æ: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤")
        
        results = {
            "layer": "data_integrity",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # ç·æ•°ã‚’å–å¾—
            total_count = self.collection.count()
            sample_limit = min(total_count, 20)  # å°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã§å®‰å…¨ã«åˆ†æ
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æ®µéšçš„ã«å–å¾—
            documents_data = self.collection.get(limit=sample_limit, include=['documents'])
            metadata_data = self.collection.get(limit=sample_limit, include=['metadatas'])
            
            results["details"]["analyzed_documents"] = sample_limit
            results["details"]["total_documents"] = total_count
            results["details"]["analysis_coverage"] = round((sample_limit / total_count) * 100, 1)
            
            # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            document_issues = 0
            valid_documents = 0
            
            if documents_data['documents']:
                for doc in documents_data['documents']:
                    if doc is None or doc == "":
                        document_issues += 1
                    else:
                        valid_documents += 1
            
            results["details"]["document_integrity"] = {
                "valid_documents": valid_documents,
                "invalid_documents": document_issues,
                "validity_rate": valid_documents / max(1, sample_limit)
            }
            
            # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            metadata_issues = 0
            valid_metadata = 0
            
            if metadata_data['metadatas']:
                for metadata in metadata_data['metadatas']:
                    if metadata is None or not isinstance(metadata, dict):
                        metadata_issues += 1
                    else:
                        valid_metadata += 1
                        # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
                        if 'source' not in metadata and 'timestamp' not in metadata:
                            metadata_issues += 0.5  # è»½å¾®ãªå•é¡Œ
            
            results["details"]["metadata_integrity"] = {
                "valid_metadata": valid_metadata,
                "invalid_metadata": metadata_issues,
                "validity_rate": valid_metadata / max(1, sample_limit)
            }
            
            # 3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
            duplicate_count = 0
            if documents_data['documents']:
                doc_hashes = set()
                for doc in documents_data['documents']:
                    if doc:
                        doc_hash = hashlib.md5(doc.encode('utf-8')).hexdigest()
                        if doc_hash in doc_hashes:
                            duplicate_count += 1
                        doc_hashes.add(doc_hash)
            
            results["details"]["duplication_check"] = {
                "duplicate_documents": duplicate_count,
                "unique_documents": sample_limit - duplicate_count,
                "uniqueness_rate": (sample_limit - duplicate_count) / max(1, sample_limit)
            }
            
            # 4. æ¤œç´¢æ©Ÿèƒ½æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ
            search_integrity_tests = [
                ("æ—¥æœ¬èªã‚¯ã‚¨ãƒª", "å§‰å¦¹"),
                ("è‹±èªã‚¯ã‚¨ãƒª", "system"),
                ("çŸ­ã„ã‚¯ã‚¨ãƒª", "ä¼šè©±"),
                ("é•·ã„ã‚¯ã‚¨ãƒª", "äººå·¥çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã®ä¼šè©±æ©Ÿèƒ½ã«ã¤ã„ã¦"),
                ("ç©ºã‚¯ã‚¨ãƒª", "")
            ]
            
            search_results = {}
            successful_searches = 0
            
            for test_name, query in search_integrity_tests:
                try:
                    if query:  # ç©ºã‚¯ã‚¨ãƒªã¯é™¤å¤–
                        result = self.collection.query(
                            query_texts=[query],
                            n_results=3
                        )
                        
                        if (result['documents'] and 
                            len(result['documents']) > 0 and
                            len(result['documents'][0]) > 0):
                            search_results[test_name] = "success"
                            successful_searches += 1
                        else:
                            search_results[test_name] = "no_results"
                    else:
                        search_results[test_name] = "skipped"
                        
                except Exception as e:
                    search_results[test_name] = f"error: {str(e)[:50]}"
            
            # ç©ºã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆã¯é™¤å¤–ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
            valid_tests = len([t for t in search_integrity_tests if t[1]])
            search_success_rate = successful_searches / max(1, valid_tests)
            
            results["details"]["search_integrity"] = {
                "test_results": search_results,
                "successful_searches": successful_searches,
                "total_valid_tests": valid_tests,
                "success_rate": search_success_rate
            }
            
            print(f"   ğŸ“Š åˆ†æã‚«ãƒãƒ¼ç‡: {results['details']['analysis_coverage']}% ({sample_limit}/{total_count})")
            print(f"   ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæœ‰åŠ¹ç‡: {results['details']['document_integrity']['validity_rate']:.1%}")
            print(f"   ğŸ·ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœ‰åŠ¹ç‡: {results['details']['metadata_integrity']['validity_rate']:.1%}")
            print(f"   ğŸ”„ é‡è¤‡æ•°: {duplicate_count}")
            print(f"   ğŸ” æ¤œç´¢æˆåŠŸç‡: {search_success_rate:.1%}")
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            base_score = 80
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå“è³ª
            doc_penalty = (1 - results["details"]["document_integrity"]["validity_rate"]) * 20
            base_score -= doc_penalty
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å“è³ª
            meta_penalty = (1 - results["details"]["metadata_integrity"]["validity_rate"]) * 15
            base_score -= meta_penalty
            
            # é‡è¤‡ãƒšãƒŠãƒ«ãƒ†ã‚£
            dup_penalty = (duplicate_count / max(1, sample_limit)) * 10
            base_score -= dup_penalty
            
            # æ¤œç´¢å“è³ªãƒœãƒ¼ãƒŠã‚¹
            search_bonus = search_success_rate * 5
            base_score += search_bonus
            
            results["score"] = max(0, min(100, int(base_score)))
            
            if results["score"] >= 95:
                results["status"] = "excellent"
            elif results["score"] >= 85:
                results["status"] = "good"
            elif results["score"] >= 70:
                results["status"] = "warning"
            else:
                results["status"] = "failed"
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆå®Œå…¨å®‰å…¨ç‰ˆï¼‰"""
        print("\n" + "="*80)
        print("ğŸ”¬ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆå®Œå…¨å®‰å…¨ç‰ˆï¼‰")
        print("="*80)
        
        # å…¨å±¤ã®åˆ†æã‚’å®Ÿè¡Œ
        layer_results = []
        
        # ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
        layer_results.append(self.analyze_filesystem_layer())
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        if self.connect_to_database():
            # ç¬¬2å±¤: ChromaDB API
            layer_results.append(self.analyze_chromadb_api_layer())
            
            # ç¬¬3å±¤: SQLiteå†…éƒ¨
            layer_results.append(self.analyze_sqlite_internal_layer())
            
            # ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿
            layer_results.append(self.analyze_vector_embeddings_layer())
            
            # ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            layer_results.append(self.analyze_data_integrity_layer())
        
        # ç·åˆè©•ä¾¡
        scores = [result.get("score", 0) for result in layer_results]
        total_score = sum(scores) / len(scores) if scores else 0
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if total_score >= 95:
            overall_status = "EXCELLENT"
            status_emoji = "ğŸŒŸ"
            status_description = "å®Œç’§ãªçŠ¶æ…‹"
        elif total_score >= 85:
            overall_status = "GOOD"
            status_emoji = "âœ…"
            status_description = "è‰¯å¥½ãªçŠ¶æ…‹"
        elif total_score >= 70:
            overall_status = "WARNING"
            status_emoji = "âš ï¸"
            status_description = "æ³¨æ„ãŒå¿…è¦"
        else:
            overall_status = "CRITICAL"
            status_emoji = "âŒ"
            status_description = "ä¿®å¾©ãŒå¿…è¦"
        
        # è©³ç´°ãªå¥å…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        health_metrics = self.calculate_detailed_health_metrics(layer_results)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        final_report = {
            "analysis_timestamp": datetime.now().isoformat(),
            "database_path": self.db_path,
            "overall_status": overall_status,
            "overall_score": round(total_score, 2),
            "status_description": status_description,
            "layer_count": len(layer_results),
            "layer_results": layer_results,
            "summary": {
                "status_emoji": status_emoji,
                "recommendations": self.generate_detailed_recommendations(layer_results),
                "health_metrics": health_metrics,
                "strengths": self.identify_strengths(layer_results),
                "areas_for_improvement": self.identify_improvements(layer_results)
            }
        }
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        print(f"\n{status_emoji} ç·åˆè©•ä¾¡: {overall_status} ({total_score:.1f}/100) - {status_description}")
        print(f"ğŸ“Š åˆ†æå±¤æ•°: {len(layer_results)}")
        print(f"â° åˆ†ææ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"\nğŸ“‹ å±¤åˆ¥è©³ç´°ã‚¹ã‚³ã‚¢:")
        for result in layer_results:
            status_icon = {
                "excellent": "ğŸŒŸ",
                "good": "âœ…", 
                "warning": "âš ï¸",
                "failed": "âŒ",
                "unknown": "â“"
            }.get(result.get("status", "unknown"), "â“")
            
            layer_name = result.get("layer", "Unknown")
            score = result.get("score", 0)
            status = result.get("status", "unknown").upper()
            print(f"   {status_icon} {layer_name:20s}: {score:3d}/100 ({status})")
        
        # å¥å…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        print(f"\nğŸ“ˆ å¥å…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        for key, value in health_metrics.items():
            if key != "layer_distribution":
                print(f"   â€¢ {key}: {value}")
        
        # å¼·ã¿ã®è¡¨ç¤º
        strengths = final_report["summary"]["strengths"]
        if strengths:
            print(f"\nğŸ’ª æ¤œå‡ºã•ã‚ŒãŸå¼·ã¿:")
            for strength in strengths:
                print(f"   âœ“ {strength}")
        
        # æ¨å¥¨äº‹é …
        recommendations = final_report["summary"]["recommendations"]
        if recommendations:
            print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        # æ”¹å–„ç‚¹
        improvements = final_report["summary"]["areas_for_improvement"]
        if improvements:
            print(f"\nğŸ”§ æ”¹å–„ç‚¹:")
            for improvement in improvements:
                print(f"   â€¢ {improvement}")
        
        return final_report
    
    def calculate_detailed_health_metrics(self, layer_results: List[Dict]) -> Dict[str, Any]:
        """è©³ç´°ãªå¥å…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        scores = [result.get("score", 0) for result in layer_results]
        statuses = [result.get("status", "unknown") for result in layer_results]
        
        status_counts = {}
        for status in statuses:
            status_counts[status] = status_counts.get(status, 0) + 1
        
        return {
            "average_score": round(sum(scores) / len(scores) if scores else 0, 2),
            "min_score": min(scores) if scores else 0,
            "max_score": max(scores) if scores else 0,
            "score_range": max(scores) - min(scores) if scores else 0,
            "excellent_layers": status_counts.get("excellent", 0),
            "good_layers": status_counts.get("good", 0),
            "warning_layers": status_counts.get("warning", 0),
            "failed_layers": status_counts.get("failed", 0),
            "healthy_percentage": round((status_counts.get("excellent", 0) + status_counts.get("good", 0)) / len(layer_results) * 100, 1),
            "total_layers": len(layer_results),
            "layer_distribution": status_counts
        }
    
    def identify_strengths(self, layer_results: List[Dict]) -> List[str]:
        """ã‚·ã‚¹ãƒ†ãƒ ã®å¼·ã¿ã‚’ç‰¹å®š"""
        strengths = []
        
        for result in layer_results:
            if result.get("status") == "excellent":
                layer = result.get("layer", "Unknown")
                score = result.get("score", 0)
                strengths.append(f"{layer}å±¤ãŒå„ªç§€ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ({score}/100)")
        
        # å…¨ä½“çš„ãªå¼·ã¿
        scores = [result.get("score", 0) for result in layer_results]
        if sum(scores) / len(scores) >= 90:
            strengths.append("å…¨å±¤ã«ãŠã„ã¦é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒ")
        
        return strengths
    
    def identify_improvements(self, layer_results: List[Dict]) -> List[str]:
        """æ”¹å–„ãŒå¿…è¦ãªé ˜åŸŸã‚’ç‰¹å®š"""
        improvements = []
        
        for result in layer_results:
            status = result.get("status", "unknown")
            layer = result.get("layer", "Unknown")
            
            if status == "failed":
                improvements.append(f"{layer}å±¤ã®ç·Šæ€¥ä¿®å¾©ãŒå¿…è¦")
            elif status == "warning":
                improvements.append(f"{layer}å±¤ã®æœ€é©åŒ–ã‚’æ¨å¥¨")
        
        return improvements
    
    def generate_detailed_recommendations(self, layer_results: List[Dict]) -> List[str]:
        """è©³ç´°ãªæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # å„å±¤ã®çŠ¶æ³ã«å¿œã˜ãŸæ¨å¥¨äº‹é …
        for result in layer_results:
            status = result.get("status", "unknown")
            layer = result.get("layer", "Unknown")
            score = result.get("score", 0)
            
            if status == "failed":
                recommendations.append(f"{layer}å±¤ã®ä¿®å¾©: ã‚¹ã‚³ã‚¢{score}ã€å³åº§ã®å¯¾å¿œãŒå¿…è¦")
            elif status == "warning":
                recommendations.append(f"{layer}å±¤ã®æ”¹å–„: ã‚¹ã‚³ã‚¢{score}ã€æœ€é©åŒ–ã‚’æ¤œè¨")
        
        # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …
        scores = [result.get("score", 0) for result in layer_results]
        avg_score = sum(scores) / len(scores) if scores else 0
        
        if avg_score >= 95:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ ã¯æœ€é©ãªçŠ¶æ…‹ã§ã™ã€‚ç¾åœ¨ã®å“è³ªã‚’ç¶­æŒã—ã¦ãã ã•ã„")
        elif avg_score >= 85:
            recommendations.append("å®šæœŸçš„ãªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„")
        elif avg_score >= 70:
            recommendations.append("ä¸€éƒ¨å±¤ã®æ”¹å–„ã«ã‚ˆã‚Šã€ã•ã‚‰ãªã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™")
        else:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®è¦‹ç›´ã—ã¨æœ€é©åŒ–ãŒå¿…è¦ã§ã™")
        
        if not recommendations:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯å¥å…¨ãªçŠ¶æ…‹ã§ã™")
        
        return recommendations
    
    def save_report(self, report: Dict[str, Any], output_file: str = None):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"chromadb_v4_comprehensive_analysis_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_file}")
            return output_file
        except Exception as e:
            print(f"\nâŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ChromaDB v4ã®ãƒ‘ã‚¹
    v4_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_v4"
    
    print("ğŸš€ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æã‚’é–‹å§‹ï¼ˆå®Œå…¨å®‰å…¨ç‰ˆï¼‰")
    print(f"ğŸ“‚ å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {v4_db_path}")
    
    # åˆ†æå™¨ã‚’åˆæœŸåŒ–
    analyzer = UltraSafeChromaDBv4Analyzer(v4_db_path)
    
    # åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ
    report = analyzer.generate_comprehensive_report()
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    output_file = r"f:\å‰¯æ¥­\VSC_WorkSpace\MCP_ChromaDB00\chromadb_v4_ultra_safe_report.json"
    saved_file = analyzer.save_report(report, output_file)
    
    print(f"\nğŸ‰ åŒ…æ‹¬çš„æ·±å±¤åˆ†æå®Œäº†!")
    print(f"ğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {report['overall_score']}/100")
    print(f"ğŸ† è©•ä¾¡: {report['overall_status']}")
    
    return report

if __name__ == "__main__":
    report = main()
