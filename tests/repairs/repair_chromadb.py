#!/usr/bin/env python3
"""
ChromaDBã®ä¸æ•´åˆä¿®å¾©ãƒ„ãƒ¼ãƒ«
"""
import chromadb
from chromadb.config import Settings
from pathlib import Path
import json
from datetime import datetime
import sqlite3
import shutil

def repair_chromadb_inconsistencies(db_path: str, create_backup: bool = True):
    """ChromaDBã®ä¸æ•´åˆã‚’ä¿®å¾©"""
    print(f"ğŸ”§ ChromaDBä¸æ•´åˆä¿®å¾©é–‹å§‹: {db_path}")
    print("=" * 70)
    
    if create_backup:
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        backup_path = f"{db_path}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆä¸­: {backup_path}")
        shutil.copytree(db_path, backup_path)
        print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†")
    
    try:
        repair_log = []
        
        # 1. å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        print(f"\nğŸ§¹ å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
        print("-" * 40)
        
        sqlite_file = Path(db_path) / "chroma.sqlite3"
        if sqlite_file.exists():
            with sqlite3.connect(sqlite_file) as conn:
                cursor = conn.cursor()
                
                # å­¤ç«‹ã—ãŸåŸ‹ã‚è¾¼ã¿ã‚’å‰Šé™¤
                cursor.execute("DELETE FROM embeddings WHERE segment_id NOT IN (SELECT id FROM segments);")
                deleted_embeddings = cursor.rowcount
                
                # å­¤ç«‹ã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                cursor.execute("DELETE FROM embedding_metadata WHERE id NOT IN (SELECT id FROM embeddings);")
                deleted_metadata = cursor.rowcount
                
                conn.commit()
                
                print(f"âœ… å­¤ç«‹ã—ãŸåŸ‹ã‚è¾¼ã¿å‰Šé™¤: {deleted_embeddings}ä»¶")
                print(f"âœ… å­¤ç«‹ã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‰Šé™¤: {deleted_metadata}ä»¶")
                
                repair_log.append({
                    'action': 'cleanup_orphaned_records',
                    'deleted_embeddings': deleted_embeddings,
                    'deleted_metadata': deleted_metadata
                })
        
        # 2. ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
        print(f"\nğŸ”§ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ æ­£è¦åŒ–")
        print("-" * 40)
        
        client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        collections = client.list_collections()
        
        for collection in collections:
            print(f"ğŸ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collection.name}")
            
            if collection.name == "sister_chat_history":
                # sister_chat_historyã®ä¸æ•´åˆä¿®å¾©
                all_docs = collection.get()
                ids = all_docs.get('ids', [])
                documents = all_docs.get('documents', [])
                metadatas = all_docs.get('metadatas', [])
                
                fixed_metadatas = []
                updated_count = 0
                
                for i, (doc_id, metadata) in enumerate(zip(ids, metadatas)):
                    if metadata is None:
                        metadata = {}
                    
                    # æ¨™æº–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã®ç¢ºä¿
                    fixed_metadata = metadata.copy()
                      # æ¬ æã‚­ãƒ¼ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                    if 'updated_timestamp' not in fixed_metadata or fixed_metadata['updated_timestamp'] is None:
                        fixed_metadata['updated_timestamp'] = ""  # ç©ºæ–‡å­—åˆ—ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
                    
                    if 'update_reason' not in fixed_metadata or fixed_metadata['update_reason'] is None:
                        fixed_metadata['update_reason'] = ""  # ç©ºæ–‡å­—åˆ—ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
                    
                    # å¿…é ˆã‚­ãƒ¼ã®æ¤œè¨¼ã¨ä¿®æ­£
                    required_keys = ['timestamp', 'type', 'genres', 'summary_length', 'original_length']
                    for key in required_keys:
                        if key not in fixed_metadata or fixed_metadata[key] is None:
                            if key == 'type':
                                fixed_metadata[key] = 'conversation_summary'
                            elif key == 'genres':
                                fixed_metadata[key] = 'ãã®ä»–'
                            elif key in ['summary_length', 'original_length']:
                                fixed_metadata[key] = 0
                            elif key == 'timestamp':
                                fixed_metadata[key] = datetime.now().isoformat()
                    
                    # Noneã®å€¤ã‚’é©åˆ‡ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«å¤‰æ›
                    for key, value in fixed_metadata.items():
                        if value is None:
                            if key in ['summary_length', 'original_length']:
                                fixed_metadata[key] = 0
                            else:
                                fixed_metadata[key] = ""
                    
                    fixed_metadatas.append(fixed_metadata)
                    
                    # å¤‰æ›´ãŒã‚ã£ãŸã‹ãƒã‚§ãƒƒã‚¯
                    if fixed_metadata != metadata:
                        updated_count += 1
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¿®æ­£ã•ã‚ŒãŸå ´åˆã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ›´æ–°
                if updated_count > 0:
                    print(f"   ğŸ“ {updated_count}ä»¶ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿®æ­£ä¸­...")
                    
                    # æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¦å†è¿½åŠ 
                    # ChromaDBã§ã¯ç›´æ¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãŒã§ããªã„ãŸã‚
                    temp_collection_name = f"{collection.name}_temp"
                    
                    # ä¸€æ™‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
                    temp_collection = client.create_collection(temp_collection_name)
                    
                    # ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
                    temp_collection.add(
                        ids=ids,
                        documents=documents,
                        metadatas=fixed_metadatas
                    )
                    
                    # å…ƒã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
                    client.delete_collection(collection.name)
                    
                    # ä¸€æ™‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®åå‰ã‚’å¤‰æ›´ï¼ˆå†ä½œæˆï¼‰
                    final_collection = client.create_collection(collection.name)
                    
                    # ä¸€æ™‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œ
                    temp_data = temp_collection.get()
                    final_collection.add(
                        ids=temp_data['ids'],
                        documents=temp_data['documents'],
                        metadatas=temp_data['metadatas']
                    )
                    
                    # ä¸€æ™‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
                    client.delete_collection(temp_collection_name)
                    
                    print(f"   âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿®å¾©å®Œäº†: {updated_count}ä»¶")
                    
                    repair_log.append({
                        'action': 'normalize_metadata',
                        'collection': collection.name,
                        'updated_documents': updated_count
                    })
                else:
                    print(f"   âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«æ­£è¦åŒ–æ¸ˆã¿")
            
            elif collection.name == "my_sister_context":
                # my_sister_contextã¯æ­£å¸¸ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
                print(f"   âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ æ­£å¸¸")
        
        # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
        print(f"\nâš¡ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–")
        print("-" * 40)
        
        with sqlite3.connect(sqlite_file) as conn:
            cursor = conn.cursor()
            
            # VACUUMå®Ÿè¡Œã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æœ€é©åŒ–
            print(f"ğŸ”„ VACUUMå®Ÿè¡Œä¸­...")
            cursor.execute("VACUUM;")
            
            # ANALYZEå®Ÿè¡Œã§ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚’æœ€é©åŒ–
            print(f"ğŸ”„ ANALYZEå®Ÿè¡Œä¸­...")
            cursor.execute("ANALYZE;")
            
            conn.commit()
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†")
            
            repair_log.append({
                'action': 'database_optimization',
                'completed': True
            })
        
        # 4. ä¿®å¾©å¾Œã®æ¤œè¨¼
        print(f"\nğŸ” ä¿®å¾©çµæœæ¤œè¨¼")
        print("-" * 40)
        
        # å†åº¦ä¸æ•´åˆãƒã‚§ãƒƒã‚¯
        collections_after = client.list_collections()
        total_issues_after = 0
        
        for collection in collections_after:
            if collection.count() > 0:
                all_docs = collection.get()
                metadatas = all_docs.get('metadatas', [])
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
                if metadatas:
                    first_meta_keys = set(metadatas[0].keys()) if metadatas[0] else set()
                    inconsistent = 0
                    
                    for metadata in metadatas:
                        if metadata is None:
                            inconsistent += 1
                            continue
                        current_keys = set(metadata.keys())
                        if current_keys != first_meta_keys:
                            inconsistent += 1
                    
                    if inconsistent == 0:
                        print(f"   âœ… {collection.name}: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ ä¸€è²«")
                    else:
                        print(f"   âš ï¸  {collection.name}: {inconsistent}ä»¶ã®ä¸æ•´åˆæ®‹å­˜")
                        total_issues_after += inconsistent
        
        # SQLiteãƒ¬ãƒ™ãƒ«æ¤œè¨¼
        with sqlite3.connect(sqlite_file) as conn:
            cursor = conn.cursor()
            
            # å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰å†ãƒã‚§ãƒƒã‚¯
            cursor.execute("SELECT COUNT(*) FROM embeddings WHERE segment_id NOT IN (SELECT id FROM segments);")
            orphaned_embeddings = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM embedding_metadata WHERE id NOT IN (SELECT id FROM embeddings);")
            orphaned_metadata = cursor.fetchone()[0]
            
            if orphaned_embeddings == 0 and orphaned_metadata == 0:
                print(f"   âœ… SQLite: å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãªã—")
            else:
                print(f"   âš ï¸  SQLite: å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰æ®‹å­˜ (åŸ‹ã‚è¾¼ã¿:{orphaned_embeddings}, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:{orphaned_metadata})")
                total_issues_after += orphaned_embeddings + orphaned_metadata
        
        # ä¿®å¾©çµæœã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“ˆ ä¿®å¾©çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        print(f"   ä¿®å¾©å‰ã®å•é¡Œ: 99ä»¶ + 7ä»¶å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰")
        print(f"   ä¿®å¾©å¾Œã®å•é¡Œ: {total_issues_after}ä»¶")
        
        if total_issues_after == 0:
            print(f"   ğŸ‰ ä¿®å¾©å®Œäº†: å…¨ã¦ã®ä¸æ•´åˆãŒè§£æ±ºã•ã‚Œã¾ã—ãŸï¼")
        else:
            print(f"   âš ï¸  éƒ¨åˆ†ä¿®å¾©: {total_issues_after}ä»¶ã®å•é¡ŒãŒæ®‹å­˜")
        
        print(f"   ä¿®å¾©æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        if create_backup:
            print(f"   ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")
        
        return {
            "success": True,
            "total_issues_before": 106,  # 99 + 7
            "total_issues_after": total_issues_after,
            "repair_log": repair_log,
            "backup_path": backup_path if create_backup else None,
            "repair_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ ä¿®å¾©ã‚¨ãƒ©ãƒ¼: {e}")
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    target_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB"
    
    print(f"âš ï¸  ä¿®å¾©ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ")
    print(f"   å¯¾è±¡: {target_path}")
    print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè‡ªå‹•ä½œæˆã•ã‚Œã¾ã™")
    print(f"   ã“ã®æ“ä½œã¯ä¸å¯é€†ã§ã™")
    
    # ä¿®å¾©å®Ÿè¡Œ
    result = repair_chromadb_inconsistencies(target_path, create_backup=True)
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = Path(__file__).parent / f"chromadb_repair_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ ä¿®å¾©ãƒ­ã‚°ã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
