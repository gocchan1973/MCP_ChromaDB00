#!/usr/bin/env python3
"""
ChromaDB v4 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ãªæ·±å±¤åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆçµ¶å¯¾å®‰å…¨ç‰ˆï¼‰
å…¨ã¦ã®numpyã‚¨ãƒ©ãƒ¼ã¨ChromaDBã®ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã‚’å®Œå…¨ã«å›é¿
"""

import chromadb
import os
import json
import sqlite3
from datetime import datetime
from typing import Dict, List, Any, Tuple
import hashlib
import time
from pathlib import Path
import uuid
import math

class AbsoluteSafeChromaDBv4Analyzer:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.client = None
        self.collection = None
        self.analysis_results = {}
        
    def connect_to_database(self) -> bool:
        """v4ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š"""
        try:
            self.client = chromadb.PersistentClient(path=self.db_path)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ: {self.db_path}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def safe_get_data(self, include_types: List[str], limit: int = 10) -> Dict[str, Any]:
        """ChromaDBã‹ã‚‰å®‰å…¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        results = {}
        
        # æœ‰åŠ¹ãªã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—
        valid_includes = ['documents', 'embeddings', 'metadatas']
        
        for inc_type in include_types:
            if inc_type in valid_includes:
                try:
                    data = self.collection.get(limit=limit, include=[inc_type])
                    results[inc_type] = data.get(inc_type, [])
                except Exception as e:
                    print(f"   âš ï¸ {inc_type}å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    results[inc_type] = []
        
        return results
    
    def safe_math_stats(self, values: List[float]) -> Dict[str, float]:
        """æ•°å­¦çµ±è¨ˆã‚’å®‰å…¨ã«è¨ˆç®—ï¼ˆnumpyã‚’ä½¿ç”¨ã—ãªã„ï¼‰"""
        if not values:
            return {"mean": 0, "std": 0, "min": 0, "max": 0, "count": 0}
        
        n = len(values)
        mean_val = sum(values) / n
        variance = sum((x - mean_val) ** 2 for x in values) / n
        std_val = math.sqrt(variance)
        
        return {
            "mean": round(mean_val, 6),
            "std": round(std_val, 6),
            "min": round(min(values), 6),
            "max": round(max(values), 6),
            "count": n
        }
    
    def analyze_filesystem_layer(self) -> Dict[str, Any]:
        """ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬1å±¤åˆ†æ: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ å±¤")
        
        results = {
            "layer": "filesystem",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
            db_exists = os.path.exists(self.db_path)
            results["details"]["database_exists"] = db_exists
            
            if not db_exists:
                results["status"] = "failed"
                results["details"]["error"] = "Database directory does not exist"
                results["score"] = 0
                return results
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®åˆ†æ
            directory_structure = {}
            total_size = 0
            file_count = 0
            
            for root, dirs, files in os.walk(self.db_path):
                rel_path = os.path.relpath(root, self.db_path)
                directory_structure[rel_path] = []
                
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        file_size = os.path.getsize(file_path)
                        file_info = {
                            "name": file,
                            "size": file_size,
                            "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
                        }
                        directory_structure[rel_path].append(file_info)
                        total_size += file_size
                        file_count += 1
                    except Exception as e:
                        print(f"   âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {file}: {e}")
            
            results["details"]["structure"] = directory_structure
            results["details"]["total_size_bytes"] = total_size
            results["details"]["total_size_mb"] = round(total_size / (1024 * 1024), 2)
            results["details"]["file_count"] = file_count
            
            # æ¨©é™ç¢ºèª
            permissions = {
                "readable": os.access(self.db_path, os.R_OK),
                "writable": os.access(self.db_path, os.W_OK),
                "executable": os.access(self.db_path, os.X_OK)
            }
            results["details"]["permissions"] = permissions
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            if all(permissions.values()) and total_size > 0:
                results["status"] = "excellent"
                results["score"] = 100
            elif permissions["readable"] and permissions["writable"]:
                results["status"] = "good"
                results["score"] = 85
            else:
                results["status"] = "warning"
                results["score"] = 60
                
            print(f"   ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : {len(directory_structure)} ãƒ•ã‚©ãƒ«ãƒ€, {file_count} ãƒ•ã‚¡ã‚¤ãƒ«")
            print(f"   ğŸ’¾ ç·ã‚µã‚¤ã‚º: {results['details']['total_size_mb']} MB")
            print(f"   ğŸ”’ æ¨©é™: {permissions}")
            
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_chromadb_api_layer(self) -> Dict[str, Any]:
        """ç¬¬2å±¤: ChromaDB APIå±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬2å±¤åˆ†æ: ChromaDB APIå±¤")
        
        results = {
            "layer": "chromadb_api",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—
            collections = self.client.list_collections()
            results["details"]["collections_count"] = len(collections)
            results["details"]["collections"] = []
            
            for collection in collections:
                coll_info = {
                    "name": collection.name,
                    "id": str(collection.id),
                    "metadata": collection.metadata
                }
                results["details"]["collections"].append(coll_info)
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®š
                if collection.name == "sister_chat_history_v4":
                    self.collection = collection
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°åˆ†æ
            if self.collection:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
                doc_count = self.collection.count()
                results["details"]["main_collection"] = {
                    "name": self.collection.name,
                    "document_count": doc_count,
                    "metadata": self.collection.metadata
                }
                
                # æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚’æ®µéšçš„ã«å®Ÿè¡Œ
                search_tests = [
                    ("åŸºæœ¬æ¤œç´¢", "ãƒ†ã‚¹ãƒˆ"),
                    ("æ—¥æœ¬èªæ¤œç´¢", "å§‰å¦¹"),
                    ("è¤‡åˆæ¤œç´¢", "ä¼šè©± ã‚·ã‚¹ãƒ†ãƒ "),
                    ("è‹±èªæ¤œç´¢", "system"),
                    ("å°‚é–€ç”¨èª", "äººå·¥çŸ¥èƒ½")
                ]
                
                search_results = {}
                successful_searches = 0
                
                for test_name, query in search_tests:
                    try:
                        # æœ€å°é™ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æ¤œç´¢
                        search_result = self.collection.query(
                            query_texts=[query],
                            n_results=2
                        )
                        
                        result_count = 0
                        if (search_result and 'documents' in search_result and 
                            search_result['documents'] and len(search_result['documents']) > 0):
                            result_count = len(search_result['documents'][0])
                        
                        search_results[test_name] = {
                            "success": result_count > 0,
                            "result_count": result_count,
                            "query": query
                        }
                        
                        if result_count > 0:
                            successful_searches += 1
                            
                    except Exception as e:
                        search_results[test_name] = {
                            "success": False,
                            "error": str(e)[:100],
                            "query": query
                        }
                
                results["details"]["search_tests"] = search_results
                results["details"]["search_success_rate"] = successful_searches / len(search_tests)
                
                print(f"   ğŸ“Š ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {self.collection.name}")
                print(f"   ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count}")
                print(f"   ğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆæˆåŠŸ: {successful_searches}/{len(search_tests)}")
                
                # ã‚¹ã‚³ã‚¢åˆ¤å®š
                score = 60
                
                if doc_count > 0:
                    score += 20
                if doc_count >= 100:
                    score += 10
                    
                success_rate = successful_searches / len(search_tests)
                if success_rate >= 1.0:
                    score += 10
                elif success_rate >= 0.8:
                    score += 7
                elif success_rate >= 0.6:
                    score += 3
                
                results["score"] = min(100, score)
                
                if results["score"] >= 95:
                    results["status"] = "excellent"
                elif results["score"] >= 80:
                    results["status"] = "good"
                else:
                    results["status"] = "warning"
            else:
                results["status"] = "failed"
                results["details"]["error"] = "Main collection not found"
                results["score"] = 0
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_sqlite_internal_layer(self) -> Dict[str, Any]:
        """ç¬¬3å±¤: SQLiteå†…éƒ¨æ§‹é€ å±¤ã®åˆ†æ"""
        print("\nğŸ” ç¬¬3å±¤åˆ†æ: SQLiteå†…éƒ¨æ§‹é€ å±¤")
        
        results = {
            "layer": "sqlite_internal",
            "status": "unknown",
            "details": {}
        }
        
        try:
            # SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            sqlite_files = []
            for root, dirs, files in os.walk(self.db_path):
                for file in files:
                    if (file.endswith('.sqlite') or file.endswith('.sqlite3') or 
                        file.endswith('.db') or 'chroma' in file.lower()):
                        sqlite_files.append(os.path.join(root, file))
            
            results["details"]["sqlite_files"] = [os.path.basename(f) for f in sqlite_files]
            results["details"]["sqlite_file_count"] = len(sqlite_files)
            print(f"   ğŸ—„ï¸  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(sqlite_files)}")
            
            if not sqlite_files:
                results["status"] = "warning"
                results["details"]["error"] = "No SQLite files found"
                results["score"] = 50
                return results
            
            # ãƒ¡ã‚¤ãƒ³ã®SQLiteãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
            main_db = sqlite_files[0]
            print(f"   ğŸ—„ï¸  ãƒ¡ã‚¤ãƒ³DB: {os.path.basename(main_db)}")
            
            try:
                with sqlite3.connect(main_db) as conn:
                    cursor = conn.cursor()
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
                    tables = [row[0] for row in cursor.fetchall()]
                    results["details"]["tables"] = tables
                    results["details"]["table_count"] = len(tables)
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='index';")
                    indexes = [row[0] for row in cursor.fetchall()]
                    custom_indexes = [idx for idx in indexes if not idx.startswith('sqlite_')]
                    results["details"]["indexes"] = custom_indexes
                    results["details"]["index_count"] = len(custom_indexes)
                    
                    # é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç‰¹å®š
                    segment_tables = [t for t in tables if 'segment' in t.lower()]
                    collection_tables = [t for t in tables if 'collection' in t.lower()]
                    embedding_tables = [t for t in tables if 'embedding' in t.lower()]
                    
                    results["details"]["key_tables"] = {
                        "segments": len(segment_tables),
                        "collections": len(collection_tables),
                        "embeddings": len(embedding_tables)
                    }
                    
                    print(f"   ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
                    print(f"   ğŸ” ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(custom_indexes)}")
                    print(f"   ğŸ”§ é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ={len(segment_tables)}, ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³={len(collection_tables)}")
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—
                    score = 50
                    
                    if len(tables) >= 20:
                        score += 25
                    elif len(tables) >= 15:
                        score += 20
                    elif len(tables) >= 10:
                        score += 15
                    
                    if len(custom_indexes) >= 10:
                        score += 15
                    elif len(custom_indexes) >= 5:
                        score += 10
                    elif len(custom_indexes) >= 3:
                        score += 5
                    
                    if len(segment_tables) > 0:
                        score += 5
                    if len(collection_tables) > 0:
                        score += 5
                    
                    results["score"] = min(100, score)
                    
                    if results["score"] >= 90:
                        results["status"] = "excellent"
                    elif results["score"] >= 75:
                        results["status"] = "good"
                    else:
                        results["status"] = "warning"
                        
            except Exception as e:
                results["status"] = "warning"
                results["details"]["sqlite_error"] = str(e)
                results["score"] = 60
                print(f"   âš ï¸ SQLiteåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_vector_embeddings_layer(self) -> Dict[str, Any]:
        """ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤ã®åˆ†æï¼ˆçµ¶å¯¾å®‰å…¨ç‰ˆï¼‰"""
        print("\nğŸ” ç¬¬4å±¤åˆ†æ: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿å±¤")
        
        results = {
            "layer": "vector_embeddings",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: embeddingsãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«å–å¾—
            try:
                embeddings_data = self.safe_get_data(['embeddings'], limit=2)
                embeddings_list = embeddings_data.get('embeddings', [])
                
                if not embeddings_list or len(embeddings_list) == 0:
                    results["status"] = "failed"
                    results["details"]["error"] = "No embeddings data found"
                    results["score"] = 0
                    return results
                
                first_embedding = embeddings_list[0]
                if first_embedding is None or len(first_embedding) == 0:
                    results["status"] = "failed"
                    results["details"]["error"] = "First embedding is empty or null"
                    results["score"] = 0
                    return results
                
                # åŸºæœ¬æƒ…å ±
                vector_dimensions = len(first_embedding)
                results["details"]["vector_dimensions"] = vector_dimensions
                results["details"]["embeddings_available"] = True
                results["details"]["sample_count"] = len(embeddings_list)
                
                print(f"   ğŸ”¢ ãƒ™ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒ: {vector_dimensions}")
                print(f"   ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(embeddings_list)}")
                
                # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ™ã‚¯ã‚¿ãƒ¼å“è³ªåˆ†æï¼ˆæ‰‹å‹•è¨ˆç®—ï¼‰
                vector_quality = {
                    "dimensions": vector_dimensions,
                    "total_samples": len(embeddings_list),
                    "valid_vectors": 0,
                    "zero_vectors": 0,
                    "vector_norms": []
                }
                
                for embedding in embeddings_list:
                    if embedding is not None and len(embedding) == vector_dimensions:
                        vector_quality["valid_vectors"] += 1
                        
                        # ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆè¦ç´ å˜ä½ã§ç¢ºèªï¼‰
                        is_zero_vector = True
                        norm_squared = 0.0
                        
                        for val in embedding:
                            if abs(val) > 1e-10:
                                is_zero_vector = False
                            norm_squared += val * val
                        
                        if is_zero_vector:
                            vector_quality["zero_vectors"] += 1
                        
                        # ãƒãƒ«ãƒ è¨ˆç®—
                        norm = math.sqrt(norm_squared)
                        vector_quality["vector_norms"].append(norm)
                
                results["details"]["vector_quality"] = vector_quality
                
                print(f"   âœ… æœ‰åŠ¹ãƒ™ã‚¯ã‚¿ãƒ¼: {vector_quality['valid_vectors']}/{vector_quality['total_samples']}")
                print(f"   ğŸ¯ ã‚¼ãƒ­ãƒ™ã‚¯ã‚¿ãƒ¼: {vector_quality['zero_vectors']}")
                
                # ã‚¹ãƒ†ãƒƒãƒ—3: æ¤œç´¢å“è³ªãƒ†ã‚¹ãƒˆ
                search_tests = [
                    "å§‰å¦¹", "ä¼šè©±", "ã‚·ã‚¹ãƒ†ãƒ ", "ãƒ†ã‚¹ãƒˆ", "AI"
                ]
                
                search_results = {}
                successful_searches = 0
                
                for query in search_tests:
                    try:
                        result = self.collection.query(
                            query_texts=[query],
                            n_results=2
                        )
                        
                        result_count = 0
                        if (result and 'documents' in result and result['documents'] and 
                            len(result['documents']) > 0 and result['documents'][0]):
                            result_count = len(result['documents'][0])
                        
                        search_results[query] = {
                            "success": result_count > 0,
                            "count": result_count
                        }
                        
                        if result_count > 0:
                            successful_searches += 1
                            
                    except Exception as e:
                        search_results[query] = {
                            "success": False,
                            "error": str(e)[:50]
                        }
                
                search_success_rate = successful_searches / len(search_tests)
                results["details"]["search_quality"] = {
                    "results": search_results,
                    "success_rate": search_success_rate,
                    "successful_count": successful_searches,
                    "total_tests": len(search_tests)
                }
                
                print(f"   ğŸ” æ¤œç´¢æˆåŠŸç‡: {successful_searches}/{len(search_tests)} ({search_success_rate:.1%})")
                
                # ã‚¹ã‚³ã‚¢è¨ˆç®—
                score = 40  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
                
                # æ¬¡å…ƒãƒœãƒ¼ãƒŠã‚¹
                if vector_dimensions >= 384:
                    score += 20
                elif vector_dimensions >= 256:
                    score += 15
                elif vector_dimensions >= 128:
                    score += 10
                
                # ãƒ™ã‚¯ã‚¿ãƒ¼å“è³ªãƒœãƒ¼ãƒŠã‚¹
                if vector_quality["valid_vectors"] == vector_quality["total_samples"]:
                    score += 15
                elif vector_quality["valid_vectors"] > 0:
                    score += 10
                
                if vector_quality["zero_vectors"] == 0:
                    score += 15
                elif vector_quality["zero_vectors"] <= 1:
                    score += 10
                
                # æ¤œç´¢å“è³ªãƒœãƒ¼ãƒŠã‚¹
                if search_success_rate >= 1.0:
                    score += 10
                elif search_success_rate >= 0.8:
                    score += 8
                elif search_success_rate >= 0.6:
                    score += 5
                
                results["score"] = min(100, score)
                
                if results["score"] >= 90:
                    results["status"] = "excellent"
                elif results["score"] >= 75:
                    results["status"] = "good"
                elif results["score"] >= 60:
                    results["status"] = "warning"
                else:
                    results["status"] = "failed"
                    
            except Exception as e:
                results["status"] = "failed"
                results["details"]["embedding_error"] = str(e)
                results["score"] = 0
                print(f"   âŒ åŸ‹ã‚è¾¼ã¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def analyze_data_integrity_layer(self) -> Dict[str, Any]:
        """ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤ã®åˆ†æï¼ˆçµ¶å¯¾å®‰å…¨ç‰ˆï¼‰"""
        print("\nğŸ” ç¬¬5å±¤åˆ†æ: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§å±¤")
        
        results = {
            "layer": "data_integrity",
            "status": "unknown",
            "details": {}
        }
        
        try:
            if not self.collection:
                results["status"] = "failed"
                results["details"]["error"] = "No collection available"
                results["score"] = 0
                return results
            
            # ç·æ•°å–å¾—
            total_count = self.collection.count()
            sample_limit = min(total_count, 15)  # ã‚ˆã‚Šå°ã•ãªã‚µãƒ³ãƒ—ãƒ«
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æ®µéšçš„ã«å–å¾—
            documents_data = self.safe_get_data(['documents'], limit=sample_limit)
            metadata_data = self.safe_get_data(['metadatas'], limit=sample_limit)
            
            results["details"]["analyzed_documents"] = sample_limit
            results["details"]["total_documents"] = total_count
            results["details"]["analysis_coverage"] = round((sample_limit / total_count) * 100, 1)
            
            # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´åˆæ€§åˆ†æ
            documents = documents_data.get('documents', [])
            doc_analysis = {
                "total": len(documents),
                "valid": 0,
                "empty": 0,
                "null": 0
            }
            
            for doc in documents:
                if doc is None:
                    doc_analysis["null"] += 1
                elif doc == "":
                    doc_analysis["empty"] += 1
                else:
                    doc_analysis["valid"] += 1
            
            doc_validity_rate = doc_analysis["valid"] / max(1, doc_analysis["total"])
            results["details"]["document_analysis"] = doc_analysis
            results["details"]["document_validity_rate"] = doc_validity_rate
            
            # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§åˆ†æ
            metadatas = metadata_data.get('metadatas', [])
            meta_analysis = {
                "total": len(metadatas),
                "valid": 0,
                "null": 0,
                "invalid": 0
            }
            
            for metadata in metadatas:
                if metadata is None:
                    meta_analysis["null"] += 1
                elif not isinstance(metadata, dict):
                    meta_analysis["invalid"] += 1
                else:
                    meta_analysis["valid"] += 1
            
            meta_validity_rate = meta_analysis["valid"] / max(1, meta_analysis["total"])
            results["details"]["metadata_analysis"] = meta_analysis
            results["details"]["metadata_validity_rate"] = meta_validity_rate
            
            # 3. é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
            duplicate_count = 0
            if documents:
                seen_hashes = set()
                for doc in documents:
                    if doc and doc.strip():
                        doc_hash = hashlib.md5(doc.encode('utf-8')).hexdigest()
                        if doc_hash in seen_hashes:
                            duplicate_count += 1
                        seen_hashes.add(doc_hash)
            
            uniqueness_rate = (len(documents) - duplicate_count) / max(1, len(documents))
            results["details"]["duplicate_count"] = duplicate_count
            results["details"]["uniqueness_rate"] = uniqueness_rate
            
            # 4. æ¤œç´¢æ©Ÿèƒ½æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ
            search_tests = ["å§‰å¦¹", "ä¼šè©±", "ãƒ†ã‚¹ãƒˆ", "ã‚·ã‚¹ãƒ†ãƒ "]
            successful_searches = 0
            
            for query in search_tests:
                try:
                    result = self.collection.query(
                        query_texts=[query],
                        n_results=2
                    )
                    if (result and 'documents' in result and result['documents'] and 
                        len(result['documents']) > 0 and result['documents'][0]):
                        successful_searches += 1
                except:
                    continue
            
            search_success_rate = successful_searches / len(search_tests)
            results["details"]["search_integrity"] = {
                "success_rate": search_success_rate,
                "successful_tests": successful_searches,
                "total_tests": len(search_tests)
            }
            
            print(f"   ğŸ“Š åˆ†æã‚«ãƒãƒ¼ç‡: {results['details']['analysis_coverage']}%")
            print(f"   ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæœ‰åŠ¹ç‡: {doc_validity_rate:.1%}")
            print(f"   ğŸ·ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœ‰åŠ¹ç‡: {meta_validity_rate:.1%}")
            print(f"   ğŸ”„ é‡è¤‡æ•°: {duplicate_count}")
            print(f"   ğŸ” æ¤œç´¢æˆåŠŸç‡: {search_success_rate:.1%}")
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            score = 50  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
            
            # å“è³ªãƒœãƒ¼ãƒŠã‚¹
            score += doc_validity_rate * 20
            score += meta_validity_rate * 15
            score += uniqueness_rate * 10
            score += search_success_rate * 5
            
            results["score"] = max(0, min(100, int(score)))
            
            if results["score"] >= 95:
                results["status"] = "excellent"
            elif results["score"] >= 85:
                results["status"] = "good"
            elif results["score"] >= 70:
                results["status"] = "warning"
            else:
                results["status"] = "failed"
                
        except Exception as e:
            results["status"] = "failed"
            results["details"]["error"] = str(e)
            results["score"] = 0
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆçµ¶å¯¾å®‰å…¨ç‰ˆï¼‰"""
        print("\n" + "="*80)
        print("ğŸ”¬ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆçµ¶å¯¾å®‰å…¨ç‰ˆï¼‰")
        print("="*80)
        
        # å…¨å±¤ã®åˆ†æã‚’å®Ÿè¡Œ
        layer_results = []
        
        print("\nğŸš€ 5å±¤æ·±å±¤åˆ†æã‚’é–‹å§‹...")
        
        # ç¬¬1å±¤: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
        layer_results.append(self.analyze_filesystem_layer())
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        if self.connect_to_database():
            # ç¬¬2å±¤: ChromaDB API
            layer_results.append(self.analyze_chromadb_api_layer())
            
            # ç¬¬3å±¤: SQLiteå†…éƒ¨
            layer_results.append(self.analyze_sqlite_internal_layer())
            
            # ç¬¬4å±¤: ãƒ™ã‚¯ã‚¿ãƒ¼åŸ‹ã‚è¾¼ã¿
            layer_results.append(self.analyze_vector_embeddings_layer())
            
            # ç¬¬5å±¤: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            layer_results.append(self.analyze_data_integrity_layer())
        else:
            print("\nâŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—ã®ãŸã‚ã€ä¸Šä½å±¤ã®åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        # ç·åˆè©•ä¾¡è¨ˆç®—
        scores = [result.get("score", 0) for result in layer_results]
        total_score = sum(scores) / len(scores) if scores else 0
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šï¼ˆè©³ç´°ï¼‰
        if total_score >= 98:
            overall_status = "PERFECT"
            status_emoji = "ğŸ’"
            status_description = "å®Œç’§ãªçŠ¶æ…‹"
        elif total_score >= 95:
            overall_status = "EXCELLENT"
            status_emoji = "ğŸŒŸ"
            status_description = "å„ªç§€ãªçŠ¶æ…‹"
        elif total_score >= 85:
            overall_status = "GOOD"
            status_emoji = "âœ…"
            status_description = "è‰¯å¥½ãªçŠ¶æ…‹"
        elif total_score >= 70:
            overall_status = "WARNING"
            status_emoji = "âš ï¸"
            status_description = "æ³¨æ„ãŒå¿…è¦"
        elif total_score >= 50:
            overall_status = "POOR"
            status_emoji = "ğŸ”¶"
            status_description = "æ”¹å–„ãŒå¿…è¦"
        else:
            overall_status = "CRITICAL"
            status_emoji = "âŒ"
            status_description = "ç·Šæ€¥ä¿®å¾©ãŒå¿…è¦"
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        excellent_count = sum(1 for r in layer_results if r.get("status") == "excellent")
        good_count = sum(1 for r in layer_results if r.get("status") == "good")
        warning_count = sum(1 for r in layer_results if r.get("status") == "warning")
        failed_count = sum(1 for r in layer_results if r.get("status") == "failed")
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆæ§‹ç¯‰
        final_report = {
            "analysis_metadata": {
                "timestamp": datetime.now().isoformat(),
                "analyzer_version": "1.0.0-absolute-safe",
                "database_path": self.db_path,
                "analysis_type": "comprehensive_5_layer"
            },
            "overall_assessment": {
                "status": overall_status,
                "score": round(total_score, 2),
                "description": status_description,
                "emoji": status_emoji
            },
            "layer_analysis": {
                "total_layers": len(layer_results),
                "completed_layers": len([r for r in layer_results if "error" not in r.get("details", {})]),
                "results": layer_results
            },
            "summary_metrics": {
                "excellent_layers": excellent_count,
                "good_layers": good_count,
                "warning_layers": warning_count,
                "failed_layers": failed_count,
                "healthy_percentage": round((excellent_count + good_count) / len(layer_results) * 100, 1),
                "critical_percentage": round((warning_count + failed_count) / len(layer_results) * 100, 1)
            }
        }
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        print(f"\n{status_emoji} ğŸ“Š æœ€çµ‚ç·åˆè©•ä¾¡")
        print("="*50)
        print(f"ğŸ† ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {overall_status} ({total_score:.1f}/100)")
        print(f"ğŸ“ è©•ä¾¡: {status_description}")
        print(f"â° åˆ†æå®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ” åˆ†æå±¤æ•°: {len(layer_results)}")
        
        print(f"\nğŸ“‹ å±¤åˆ¥è©³ç´°åˆ†æçµæœ:")
        print("-" * 60)
        for i, result in enumerate(layer_results, 1):
            status_icon = {
                "excellent": "ğŸŒŸ",
                "good": "âœ…", 
                "warning": "âš ï¸",
                "failed": "âŒ",
                "unknown": "â“"
            }.get(result.get("status", "unknown"), "â“")
            
            layer_name = result.get("layer", f"Layer_{i}").replace("_", " ").title()
            score = result.get("score", 0)
            status = result.get("status", "unknown").upper()
            
            print(f"ç¬¬{i}å±¤ {status_icon} {layer_name:20s}: {score:3d}/100 ({status})")
        
        # ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        print(f"\nğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ã‚µãƒãƒªãƒ¼:")
        print("-" * 40)
        print(f"   ğŸŒŸ å„ªç§€ãªå±¤: {excellent_count}")
        print(f"   âœ… è‰¯å¥½ãªå±¤: {good_count}")
        print(f"   âš ï¸ æ³¨æ„ãŒå¿…è¦ãªå±¤: {warning_count}")
        print(f"   âŒ ä¿®å¾©ãŒå¿…è¦ãªå±¤: {failed_count}")
        print(f"   ğŸ’š å¥å…¨æ€§: {final_report['summary_metrics']['healthy_percentage']}%")
        
        # æ¨å¥¨äº‹é …
        recommendations = []
        if failed_count > 0:
            recommendations.append(f"{failed_count}å±¤ã®ç·Šæ€¥ä¿®å¾©ãŒå¿…è¦ã§ã™")
        if warning_count > 0:
            recommendations.append(f"{warning_count}å±¤ã®æœ€é©åŒ–ã‚’æ¨å¥¨ã—ã¾ã™")
        if excellent_count == len(layer_results):
            recommendations.append("å…¨å±¤ãŒå„ªç§€ãªçŠ¶æ…‹ã§ã™ã€‚ç¾åœ¨ã®å“è³ªã‚’ç¶­æŒã—ã¦ãã ã•ã„")
        
        if recommendations:
            print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        return final_report
    
    def save_report(self, report: Dict[str, Any], output_file: str = None) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"chromadb_v4_absolute_safe_report_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ’¾ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_file}")
            return output_file
        except Exception as e:
            print(f"\nâŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å¤±æ•—: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ChromaDB v4ã®ãƒ‘ã‚¹
    v4_db_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_v4"
    
    print("ğŸš€ ChromaDB v4 åŒ…æ‹¬çš„æ·±å±¤åˆ†æã‚’é–‹å§‹ï¼ˆçµ¶å¯¾å®‰å…¨ç‰ˆï¼‰")
    print(f"ğŸ“‚ å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {v4_db_path}")
    print(f"â° é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ†æå™¨ã‚’åˆæœŸåŒ–
    analyzer = AbsoluteSafeChromaDBv4Analyzer(v4_db_path)
    
    # åŒ…æ‹¬çš„ãªåˆ†æã‚’å®Ÿè¡Œ
    start_time = time.time()
    report = analyzer.generate_comprehensive_report()
    end_time = time.time()
    
    print(f"\nâ±ï¸ åˆ†ææ‰€è¦æ™‚é–“: {round(end_time - start_time, 2)}ç§’")
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    output_file = r"f:\å‰¯æ¥­\VSC_WorkSpace\MCP_ChromaDB00\chromadb_v4_absolute_safe_report.json"
    saved_file = analyzer.save_report(report, output_file)
    
    print(f"\nğŸ‰ çµ¶å¯¾å®‰å…¨ç‰ˆåŒ…æ‹¬çš„æ·±å±¤åˆ†æå®Œäº†!")
    print(f"ğŸ“Š æœ€çµ‚ç·åˆã‚¹ã‚³ã‚¢: {report['overall_assessment']['score']}/100")
    print(f"ğŸ† æœ€çµ‚è©•ä¾¡: {report['overall_assessment']['status']}")
    print(f"ğŸ“‹ åˆ†æå±¤æ•°: {report['layer_analysis']['total_layers']}")
    
    return report

if __name__ == "__main__":
    report = main()
