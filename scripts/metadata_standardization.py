#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ChromaDB ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
=================================

ç›®çš„: mcp_production_knowledgeã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’çµ±ä¸€å½¢å¼ã«å¤‰æ›

æ¨™æº–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ:
- project (å¿…é ˆ): ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
- source (å¿…é ˆ): ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
- timestamp (å¿…é ˆ): ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
- content_type (å¿…é ˆ): ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ— (document, chunk, report, manual)
- category (å¿…é ˆ): ã‚«ãƒ†ã‚´ãƒª (technical, documentation, report, other)
- source_type (å¿…é ˆ): ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ— (pdf, md, txt, html, manual)
- document_id (å¿…é ˆ): ä¸€æ„ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
- content_length (å¿…é ˆ): ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·
- file_path (ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
- chunk_info (ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒãƒ£ãƒ³ã‚¯æƒ…å ± {"index": 0, "total": 1, "size": 1000}
- version (ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
- status (ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
"""

import chromadb
import json
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
import uuid

class MetadataStandardizer:
    def __init__(self, collection_name: str = "mcp_production_knowledge"):
        # ç›´æ¥ChromaDBã«æ¥ç¶šï¼ˆæ—¢å­˜ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼‰
        self.client = chromadb.PersistentClient(
            path="F:/å‰¯æ¥­/VSC_WorkSpace/MCP_ChromaDB00/chroma"
        )
        self.collection = self.client.get_collection(collection_name)
        self.collection_name = collection_name
        
        # æ¨™æº–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
        self.required_fields = {
            'project': str,
            'source': str, 
            'timestamp': str,
            'content_type': str,  # document, chunk, report, manual
            'category': str,      # technical, documentation, report, other
            'source_type': str,   # pdf, md, txt, html, manual
            'document_id': str,
            'content_length': int
        }
        
        self.optional_fields = {
            'file_path': str,
            'chunk_info': dict,   # {"index": 0, "total": 1, "size": 1000}
            'version': str,
            'status': str
        }

    def analyze_current_metadata(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ"""
        print("ğŸ“Š ç¾åœ¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­...")
        
        all_docs = self.collection.get()
        total_docs = len(all_docs['ids'])
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®çµ±è¨ˆ
        field_stats = {}
        content_type_analysis = {}
        
        for i, metadata in enumerate(all_docs['metadatas']):
            if metadata:
                for field, value in metadata.items():
                    if field not in field_stats:
                        field_stats[field] = {
                            'count': 0, 
                            'types': set(),
                            'samples': []
                        }
                    field_stats[field]['count'] += 1
                    field_stats[field]['types'].add(type(value).__name__)
                    if len(field_stats[field]['samples']) < 5:
                        field_stats[field]['samples'].append(str(value)[:50])
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã®æ¨å®š
                content_type = self._infer_content_type(metadata, all_docs['documents'][i])
                if content_type not in content_type_analysis:
                    content_type_analysis[content_type] = 0
                content_type_analysis[content_type] += 1
        
        return {
            'total_documents': total_docs,
            'field_statistics': {
                field: {
                    'count': stats['count'],
                    'frequency': stats['count'] / total_docs,
                    'types': list(stats['types']),
                    'samples': stats['samples']
                }
                for field, stats in field_stats.items()
            },
            'content_type_distribution': content_type_analysis
        }

    def _infer_content_type(self, metadata: Dict, content: str) -> str:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
        if 'chunk_index' in metadata:
            return 'chunk'
        elif 'document_type' in metadata and 'report' in metadata.get('document_type', '').lower():
            return 'report'
        elif metadata.get('source') == 'manual_entry':
            return 'manual'
        else:
            return 'document'

    def _infer_category(self, metadata: Dict, content: str) -> str:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š"""
        content_lower = content.lower()
        
        if 'report' in content_lower or 'ãƒ¬ãƒãƒ¼ãƒˆ' in content:
            return 'report'
        elif any(word in content_lower for word in ['æŠ€è¡“', 'technical', 'implementation', 'å®Ÿè£…']):
            return 'technical'
        elif any(word in content_lower for word in ['readme', 'documentation', 'manual', 'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«']):
            return 'documentation'
        else:
            return 'other'

    def _infer_source_type(self, metadata: Dict) -> str:
        """ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
        if 'source_type' in metadata:
            return metadata['source_type']
        elif 'file_path' in metadata:
            file_path = metadata['file_path'].lower()
            if file_path.endswith('.pdf'):
                return 'pdf'
            elif file_path.endswith('.md'):
                return 'md'
            elif file_path.endswith('.txt'):
                return 'txt'
            elif file_path.endswith('.html'):
                return 'html'
        return 'manual'

    def create_standardized_metadata(self, original_metadata: Dict, content: str, doc_id: str) -> Dict[str, Any]:
        """æ¨™æº–åŒ–ã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ä½œæˆ
        standardized = {
            'project': original_metadata.get('project', 'MCP_ChromaDB_Documentation'),
            'source': original_metadata.get('source', 'unknown'),
            'timestamp': original_metadata.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
            'content_type': self._infer_content_type(original_metadata, content),
            'category': original_metadata.get('category') or self._infer_category(original_metadata, content),
            'source_type': self._infer_source_type(original_metadata),
            'document_id': doc_id,
            'content_length': len(content)
        }
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è¿½åŠ 
        if 'file_path' in original_metadata:
            standardized['file_path'] = original_metadata['file_path']
            
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã®çµ±åˆ
        if any(field in original_metadata for field in ['chunk_index', 'chunk_size', 'total_chunks']):
            standardized['chunk_info'] = {
                'index': original_metadata.get('chunk_index', 0),
                'total': original_metadata.get('total_chunks', 1), 
                'size': original_metadata.get('chunk_size', len(content))
            }
            
        if 'version' in original_metadata:
            standardized['version'] = original_metadata['version']
            
        if 'status' in original_metadata:
            standardized['status'] = original_metadata['status']
            
        return standardized

    def standardize_collection(self, dry_run: bool = True) -> Dict[str, Any]:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–"""
        print(f"ğŸ”„ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–é–‹å§‹ (dry_run={dry_run})...")
        
        all_docs = self.collection.get()
        total_docs = len(all_docs['ids'])
        
        standardization_report = {
            'total_documents': total_docs,
            'processed': 0,
            'errors': [],
            'changes_summary': {},
            'dry_run': dry_run
        }
        
        updated_metadatas = []
        
        for i, (doc_id, metadata, content) in enumerate(zip(
            all_docs['ids'], all_docs['metadatas'], all_docs['documents']
        )):
            try:
                if metadata is None:
                    metadata = {}
                    
                # æ¨™æº–åŒ–ã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                standardized_metadata = self.create_standardized_metadata(metadata, content, doc_id)
                updated_metadatas.append(standardized_metadata)
                
                # å¤‰æ›´ç‚¹ã®è¨˜éŒ²
                changes = []
                for field in self.required_fields:
                    if field not in metadata or metadata.get(field) != standardized_metadata[field]:
                        changes.append(f"+ {field}")
                        
                if changes:
                    standardization_report['changes_summary'][doc_id] = changes
                    
                standardization_report['processed'] += 1
                
                if i % 50 == 0:
                    print(f"   å‡¦ç†æ¸ˆã¿: {i}/{total_docs}")
                    
            except Exception as e:
                standardization_report['errors'].append({
                    'document_id': doc_id,
                    'error': str(e)
                })
        
        # å®Ÿéš›ã®æ›´æ–°å®Ÿè¡Œ
        if not dry_run and updated_metadatas:
            print("ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚’å®Ÿè¡Œä¸­...")
            try:
                self.collection.update(
                    ids=all_docs['ids'],
                    metadatas=updated_metadatas
                )
                print("âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº†")
            except Exception as e:
                standardization_report['errors'].append({
                    'operation': 'bulk_update',
                    'error': str(e)
                })
        
        return standardization_report

    def validate_standardized_metadata(self) -> Dict[str, Any]:
        """æ¨™æº–åŒ–å¾Œã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼"""
        print("ğŸ” æ¨™æº–åŒ–çµæœã‚’æ¤œè¨¼ä¸­...")
        
        all_docs = self.collection.get()
        validation_report = {
            'total_documents': len(all_docs['ids']),
            'valid_documents': 0,
            'missing_required_fields': {},
            'type_mismatches': {},
            'overall_score': 0.0
        }
        
        for doc_id, metadata in zip(all_docs['ids'], all_docs['metadatas']):
            if metadata is None:
                continue
                
            doc_valid = True
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
            for field, expected_type in self.required_fields.items():
                if field not in metadata:
                    if field not in validation_report['missing_required_fields']:
                        validation_report['missing_required_fields'][field] = 0
                    validation_report['missing_required_fields'][field] += 1
                    doc_valid = False
                elif not isinstance(metadata[field], expected_type):
                    if field not in validation_report['type_mismatches']:
                        validation_report['type_mismatches'][field] = 0
                    validation_report['type_mismatches'][field] += 1
                    doc_valid = False
            
            if doc_valid:
                validation_report['valid_documents'] += 1
        
        # å…¨ä½“ã‚¹ã‚³ã‚¢è¨ˆç®—
        if validation_report['total_documents'] > 0:
            validation_report['overall_score'] = (
                validation_report['valid_documents'] / validation_report['total_documents']
            )
        
        return validation_report


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ ChromaDB ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
    print("=" * 50)
    
    standardizer = MetadataStandardizer()
    
    # ç¾åœ¨ã®çŠ¶æ…‹åˆ†æ
    print("\nğŸ“Š STEP 1: ç¾åœ¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ")
    current_analysis = standardizer.analyze_current_metadata()
    print(f"ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {current_analysis['total_documents']}")
    print(f"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(current_analysis['field_statistics'])}")
    
    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ
    print("\nğŸ§ª STEP 2: æ¨™æº–åŒ–ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ")
    dry_run_result = standardizer.standardize_collection(dry_run=True)
    print(f"å‡¦ç†å¯¾è±¡: {dry_run_result['processed']} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
    print(f"ã‚¨ãƒ©ãƒ¼æ•°: {len(dry_run_result['errors'])}")
    print(f"å¤‰æ›´äºˆå®š: {len(dry_run_result['changes_summary'])} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
    
    # å®Ÿè¡Œç¢ºèª
    if dry_run_result['errors']:
        print("\nâš ï¸  ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ:")
        for error in dry_run_result['errors'][:3]:
            print(f"  - {error}")
        print(f"  ... ä»– {len(dry_run_result['errors'])-3} ä»¶" if len(dry_run_result['errors']) > 3 else "")
        
        response = input("\nã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Šã¾ã™ãŒç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").lower()
        if response != 'y':
            print("å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return
    
    # å®Ÿéš›ã®æ¨™æº–åŒ–å®Ÿè¡Œ
    print("\nğŸ”„ STEP 3: å®Ÿéš›ã®æ¨™æº–åŒ–å®Ÿè¡Œ")
    final_result = standardizer.standardize_collection(dry_run=False)
    
    # æ¤œè¨¼
    print("\nğŸ” STEP 4: æ¨™æº–åŒ–çµæœã®æ¤œè¨¼")
    validation_result = standardizer.validate_standardized_metadata()
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 50)
    print("ğŸ“‹ æ¨™æº–åŒ–å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 50)
    print(f"å‡¦ç†æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {final_result['processed']}")
    print(f"ã‚¨ãƒ©ãƒ¼æ•°: {len(final_result['errors'])}")
    print(f"æœ‰åŠ¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {validation_result['valid_documents']}")
    print(f"å…¨ä½“ã‚¹ã‚³ã‚¢: {validation_result['overall_score']:.2%}")
    
    if validation_result['missing_required_fields']:
        print("\nä¸è¶³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:")
        for field, count in validation_result['missing_required_fields'].items():
            print(f"  - {field}: {count} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    report_file = f"metadata_standardization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump({
            'analysis': current_analysis,
            'standardization': final_result,
            'validation': validation_result
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
    print("âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–å®Œäº†!")


if __name__ == "__main__":
    main()
