#!/usr/bin/env python3
"""
ChromaDB ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–ã‚·ã‚¹ãƒ†ãƒ  v2.1 (APIå®‰å…¨ç‰ˆ)
ChromaDB APIäº’æ›æ€§å¯¾å¿œç‰ˆ
"""

import os
import sys
import json
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import re

# ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import chromadb
    from chromadb.config import Settings
    print("âœ“ ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âœ— ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

class SafeMetadataUnifier:
    """å®‰å…¨ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, chromadb_path: str):
        self.chromadb_path = Path(chromadb_path)
        self.client = None
        self.collection = None
        
        # çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ
        self.required_fields = {
            "document_id": "string",
            "content_hash": "string", 
            "project": "string",
            "source": "string",
            "timestamp": "string",
            "content_type": "string",
            "category": "string",
            "source_type": "string",
            "content_length": "number",
            "version": "string"
        }
        
    def initialize(self) -> bool:
        """åˆæœŸåŒ–"""
        try:
            self.client = chromadb.PersistentClient(
                path=str(self.chromadb_path),
                settings=Settings(anonymized_telemetry=False)
            )
            print(f"âœ“ ChromaDBæ¥ç¶šæˆåŠŸ: {self.chromadb_path}")
            return True
        except Exception as e:
            print(f"âœ— ChromaDBæ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def get_collection(self, collection_name: str) -> bool:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—"""
        try:
            self.collection = self.client.get_collection(collection_name)
            count = self.collection.count()
            print(f"âœ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—: {collection_name} ({count}ä»¶)")
            return True
        except Exception as e:
            print(f"âœ— ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—å¤±æ•—: {e}")
            return False
    
    def create_backup(self, collection_name: str) -> str:
        """ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = f"metadata_unify_backup_{timestamp}.json"
        backup_path = Path(self.chromadb_path).parent / "scripts" / backup_file
        
        try:
            # å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿å–å¾—
            result = self.collection.get(include=['documents', 'metadatas'])
            documents = result.get('documents', [])
            metadatas = result.get('metadatas', [])
            
            backup_data = {
                "backup_info": {
                    "collection_name": collection_name,
                    "timestamp": timestamp,
                    "document_count": len(documents),
                    "schema_version": "v2.1"
                },
                "data": {
                    "documents": documents,
                    "metadatas": metadatas
                }
            }
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            backup_path.parent.mkdir(exist_ok=True)
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2, ensure_ascii=False)
            
            file_size = backup_path.stat().st_size / (1024*1024)
            print(f"âœ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_file} ({file_size:.2f}MB)")
            return backup_file
            
        except Exception as e:
            print(f"âœ— ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            return None
    
    def analyze_metadata(self) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        try:
            result = self.collection.get(include=['metadatas'])
            metadatas = result.get('metadatas', [])
            
            if not metadatas:
                return {"error": "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
            # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ†æ
            field_counts = {}
            total_docs = len(metadatas)
            
            for metadata in metadatas:
                if metadata:
                    for field in metadata.keys():
                        field_counts[field] = field_counts.get(field, 0) + 1
            
            # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢
            consistency_scores = {
                field: (count / total_docs) * 100 
                for field, count in field_counts.items()
            }
            
            avg_consistency = sum(consistency_scores.values()) / len(consistency_scores) if consistency_scores else 0
            
            analysis = {
                "total_documents": total_docs,
                "unique_fields": len(field_counts),
                "field_usage": field_counts,
                "consistency_scores": consistency_scores,
                "average_consistency": avg_consistency
            }
            
            print(f"ğŸ“Š åˆ†æçµæœ:")
            print(f"   - ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {total_docs}")
            print(f"   - ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(field_counts)}")
            print(f"   - å¹³å‡ä¸€è²«æ€§: {avg_consistency:.1f}%")
            
            return analysis
            
        except Exception as e:
            print(f"âœ— åˆ†æå¤±æ•—: {e}")
            return {"error": str(e)}
    
    def generate_content_hash(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
    
    def unify_single_metadata(self, old_metadata: Dict, content: str, index: int) -> Dict[str, Any]:
        """å˜ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€"""
        unified = {}
        
        # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        unified["document_id"] = f"doc_{index:06d}"
        unified["content_hash"] = self.generate_content_hash(content or "")
        unified["project"] = old_metadata.get("project", "unknown_project")
        unified["source"] = old_metadata.get("source", "unknown_source")
        unified["timestamp"] = old_metadata.get("timestamp", datetime.now().isoformat())
        unified["content_length"] = len(content or "")
        unified["version"] = "v2.1"
        
        # content_typeçµ±ä¸€
        if "type" in old_metadata:
            unified["content_type"] = old_metadata["type"]
        elif "source_type" in old_metadata:
            unified["content_type"] = old_metadata["source_type"]
        elif "document_type" in old_metadata:
            unified["content_type"] = old_metadata["document_type"]
        else:
            unified["content_type"] = "unknown"
        
        # categoryè¨­å®š
        content_type = unified["content_type"]
        if content_type in ["PDF", "pdf"]:
            unified["category"] = "document"
        elif content_type in ["MD", "markdown"]:
            unified["category"] = "documentation"
        elif "report" in content_type.lower():
            unified["category"] = "system_data"
        else:
            unified["category"] = "general"
        
        # source_typeè¨­å®š
        if old_metadata.get("file_path"):
            unified["source_type"] = "file"
        elif old_metadata.get("document_type"):
            unified["source_type"] = "document"
        else:
            unified["source_type"] = "manual"
        
        return unified
    
    def execute_dry_run(self) -> Dict[str, Any]:
        """ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ"""
        print("ğŸ§ª ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³é–‹å§‹...")
        
        try:
            result = self.collection.get(include=['documents', 'metadatas'])
            documents = result.get('documents', [])
            metadatas = result.get('metadatas', [])
            
            changes = 0
            errors = 0
            
            for i, (doc, old_meta) in enumerate(zip(documents, metadatas)):
                try:
                    unified_meta = self.unify_single_metadata(old_meta or {}, doc or "", i)
                    
                    # å¤‰æ›´ãƒã‚§ãƒƒã‚¯
                    if old_meta != unified_meta:
                        changes += 1
                        
                except Exception as e:
                    errors += 1
            
            result_summary = {
                "processed": len(documents),
                "changes": changes,
                "errors": errors,
                "success_rate": ((len(documents) - errors) / len(documents)) * 100 if documents else 0
            }
            
            print(f"âœ“ ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†:")
            print(f"   - å‡¦ç†å¯¾è±¡: {len(documents)}")
            print(f"   - å¤‰æ›´äºˆå®š: {changes}")
            print(f"   - ã‚¨ãƒ©ãƒ¼: {errors}")
            
            return result_summary
            
        except Exception as e:
            print(f"âœ— ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def execute_unification(self) -> Dict[str, Any]:
        """å®Ÿéš›ã®çµ±ä¸€åŒ–å®Ÿè¡Œ"""
        print("ğŸ”„ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–å®Ÿè¡Œ...")
        
        try:
            result = self.collection.get(include=['documents', 'metadatas'])
            documents = result.get('documents', [])
            metadatas = result.get('metadatas', [])
            
            processed = 0
            updated = 0
            errors = 0
            
            # ãƒãƒƒãƒæ›´æ–°ç”¨ãƒªã‚¹ãƒˆ
            update_metadatas = []
            
            for i, (doc, old_meta) in enumerate(zip(documents, metadatas)):
                try:
                    unified_meta = self.unify_single_metadata(old_meta or {}, doc or "", i)
                    update_metadatas.append(unified_meta)
                    
                    if old_meta != unified_meta:
                        updated += 1
                    
                    processed += 1
                    
                    if processed % 50 == 0:
                        print(f"   å‡¦ç†ä¸­: {processed}/{len(documents)}")
                        
                except Exception as e:
                    print(f"   ã‚¨ãƒ©ãƒ¼ at {i}: {e}")
                    update_metadatas.append(old_meta or {})
                    errors += 1
            
            # ãƒãƒƒãƒæ›´æ–°å®Ÿè¡Œ
            print("ğŸ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°ä¸­...")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚’å†è¿½åŠ ã™ã‚‹æ–¹å¼
            # ã¾ãšæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            ids_to_delete = [f"doc_{i:06d}" for i in range(len(documents))]
            try:
                self.collection.delete(ids=ids_to_delete)
            except:
                pass  # å­˜åœ¨ã—ãªã„IDãŒã‚ã£ã¦ã‚‚ç„¡è¦–
            
            # æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§è¿½åŠ 
            new_ids = [f"doc_{i:06d}" for i in range(len(documents))]
            self.collection.add(
                ids=new_ids,
                documents=documents,
                metadatas=update_metadatas
            )
            
            result_summary = {
                "processed": processed,
                "updated": updated,
                "errors": errors,
                "success_rate": (processed / len(documents)) * 100 if documents else 0
            }
            
            print(f"âœ… çµ±ä¸€åŒ–å®Œäº†:")
            print(f"   - å‡¦ç†æ¸ˆã¿: {processed}")
            print(f"   - æ›´æ–°æ¸ˆã¿: {updated}")
            print(f"   - ã‚¨ãƒ©ãƒ¼: {errors}")
            print(f"   - æˆåŠŸç‡: {result_summary['success_rate']:.1f}%")
            
            return result_summary
            
        except Exception as e:
            print(f"âœ— çµ±ä¸€åŒ–å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def validate_result(self) -> Dict[str, Any]:
        """çµæœæ¤œè¨¼"""
        print("ğŸ” çµæœæ¤œè¨¼ä¸­...")
        
        try:
            result = self.collection.get(include=['metadatas'])
            metadatas = result.get('metadatas', [])
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
            field_completeness = {}
            for field in self.required_fields.keys():
                count = sum(1 for meta in metadatas if meta and field in meta)
                field_completeness[field] = (count / len(metadatas)) * 100 if metadatas else 0
            
            avg_completeness = sum(field_completeness.values()) / len(field_completeness)
            
            validation = {
                "field_completeness": field_completeness,
                "average_completeness": avg_completeness,
                "schema_compliance": avg_completeness >= 95.0,
                "total_documents": len(metadatas)
            }
            
            print(f"âœ“ æ¤œè¨¼å®Œäº†:")
            print(f"   - å®Œå…¨æ€§: {avg_completeness:.1f}%")
            print(f"   - æº–æ‹ æ€§: {'âœ“' if validation['schema_compliance'] else 'âœ—'}")
            
            return validation
            
        except Exception as e:
            print(f"âœ— æ¤œè¨¼å¤±æ•—: {e}")
            return {"error": str(e)}

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ ChromaDB ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–ã‚·ã‚¹ãƒ†ãƒ  v2.1 èµ·å‹•")
    print("=" * 60)
    
    # è¨­å®š
    collection_name = "mcp_production_knowledge"
    chromadb_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_"
    
    # ãƒ‘ã‚¹ç¢ºèª
    if not Path(chromadb_path).exists():
        print(f"âœ— ChromaDBãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {chromadb_path}")
        sys.exit(1)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    unifier = SafeMetadataUnifier(chromadb_path)
    
    if not unifier.initialize():
        sys.exit(1)
    
    if not unifier.get_collection(collection_name):
        sys.exit(1)
    
    try:
        # STEP 1: ç¾çŠ¶åˆ†æ
        print("\nğŸ“Š STEP 1: ç¾çŠ¶åˆ†æ")
        analysis = unifier.analyze_metadata()
        if "error" in analysis:
            print(f"åˆ†æå¤±æ•—: {analysis['error']}")
            sys.exit(1)
        
        # STEP 2: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        print("\nğŸ’¾ STEP 2: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
        backup_file = unifier.create_backup(collection_name)
        if not backup_file:
            print("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ä¸­æ­¢")
            sys.exit(1)
        
        # STEP 3: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³
        print("\nğŸ§ª STEP 3: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³")
        dry_result = unifier.execute_dry_run()
        if "error" in dry_result:
            print(f"ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å¤±æ•—: {dry_result['error']}")
            sys.exit(1)
        
        # ç¢ºèª
        print(f"\nâ“ {dry_result['changes']}ä»¶ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’çµ±ä¸€åŒ–ã—ã¾ã™ã‹ï¼Ÿ")
        print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¸ˆã¿: {backup_file}")
        
        confirm = input("å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        
        if confirm == 'y':
            # STEP 4: å®Ÿè¡Œ
            print("\nğŸ”„ STEP 4: çµ±ä¸€åŒ–å®Ÿè¡Œ")
            exec_result = unifier.execute_unification()
            
            if "error" in exec_result:
                print(f"å®Ÿè¡Œå¤±æ•—: {exec_result['error']}")
                sys.exit(1)
            
            # STEP 5: æ¤œè¨¼
            print("\nğŸ” STEP 5: çµæœæ¤œè¨¼")
            validation = unifier.validate_result()
            
            # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
            print("\n" + "=" * 60)
            print("ğŸ“‹ çµ±ä¸€åŒ–å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 60)
            print(f"å‡¦ç†æ¸ˆã¿: {exec_result['processed']}")
            print(f"æ›´æ–°æ¸ˆã¿: {exec_result['updated']}")
            print(f"ã‚¨ãƒ©ãƒ¼æ•°: {exec_result['errors']}")
            print(f"æˆåŠŸç‡: {exec_result['success_rate']:.1f}%")
            
            if "error" not in validation:
                print(f"ã‚¹ã‚­ãƒ¼ãƒæº–æ‹ ç‡: {validation['average_completeness']:.1f}%")
                print(f"æº–æ‹ åˆ¤å®š: {'âœ… åˆæ ¼' if validation['schema_compliance'] else 'âŒ ä¸åˆæ ¼'}")
            
            print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
            print("âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–å®Œäº†!")
        else:
            print("æ“ä½œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
    
    except KeyboardInterrupt:
        print("\næ“ä½œãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
