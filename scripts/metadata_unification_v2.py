#!/usr/bin/env python3
"""
ChromaDB ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–ã‚·ã‚¹ãƒ†ãƒ  v2.0
å¤šæ§˜ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¯¾å¿œã—ãŸå°†æ¥å¯¾å¿œç‰ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–

å¯¾å¿œäºˆå®šå½¢å¼:
- PDF, MD, TXT, HTML, DOCX, XLSX, JSON, XML, YAML, CSV
- æ‰‹å‹•ã‚¨ãƒ³ãƒˆãƒªã€ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒãƒ¼ãƒˆã€ãƒãƒ£ãƒƒãƒˆå±¥æ­´
"""

import os
import sys
import json
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import re

# ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import chromadb
    from chromadb.config import Settings
    print("âœ“ ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âœ— ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

class MetadataUnificationSystem:
    """å¤šæ§˜ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼å¯¾å¿œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, chromadb_path: str):
        self.chromadb_path = Path(chromadb_path)
        self.client = None
        self.collection = None
        self.backup_file = None
        
        # çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ v2.0
        self.unified_schema = {
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆå…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…±é€šï¼‰
            "required_fields": {
                "document_id": "string",      # ä¸€æ„è­˜åˆ¥å­
                "content_hash": "string",     # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥
                "project": "string",          # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
                "source": "string",           # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
                "timestamp": "string",        # ä½œæˆãƒ»æ›´æ–°æ—¥æ™‚
                "content_type": "string",     # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼/ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥
                "category": "string",         # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
                "source_type": "string",      # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
                "content_length": "number",   # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·
                "version": "string"           # ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³
            },
            
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦ï¼‰
            "optional_fields": {
                "language": "string",         # è¨€èªæ¤œå‡º
                "complexity_score": "number", # è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢
                "importance_score": "number", # é‡è¦åº¦ã‚¹ã‚³ã‚¢
                "freshness_score": "number",  # æ–°é®®åº¦ã‚¹ã‚³ã‚¢
                "file_path": "string",        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
                "file_size": "number",        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
                "chunk_info": "object",       # ãƒãƒ£ãƒ³ã‚¯æƒ…å ± {index, total, size}
                "quality_score": "number",    # å“è³ªã‚¹ã‚³ã‚¢
                "validation_status": "string", # æ¤œè¨¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                "related_documents": "array", # é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
                "tags": "array",              # ã‚¿ã‚°
                "keywords": "array"           # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            }
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼åˆ¥ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°
        self.format_category_mapping = {
            "PDF": "document",
            "MD": "documentation", 
            "TXT": "text",
            "HTML": "web_content",
            "DOCX": "document",
            "XLSX": "spreadsheet",
            "JSON": "data",
            "XML": "data",
            "YAML": "configuration",
            "CSV": "data",
            "manual_entry": "user_input",
            "system_report": "system_data",
            "chat_history": "conversation"
        }
        
    def initialize_chromadb(self) -> bool:
        """ChromaDBæ¥ç¶šåˆæœŸåŒ–"""
        try:
            self.client = chromadb.PersistentClient(
                path=str(self.chromadb_path),
                settings=Settings(anonymized_telemetry=False)
            )
            print(f"âœ“ ChromaDBæ¥ç¶šæˆåŠŸ: {self.chromadb_path}")
            return True
        except Exception as e:
            print(f"âœ— ChromaDBæ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def get_collection(self, collection_name: str):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—"""
        try:
            if self.client is None:
                print("âœ— ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
            self.collection = self.client.get_collection(collection_name)
            print(f"âœ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—: {collection_name} ({self.collection.count()}ä»¶)")
            return True
        except Exception as e:
            print(f"âœ— ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—å¤±æ•—: {e}")
            return False
    
    def create_backup(self, collection_name: str) -> str:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"metadata_unification_backup_{timestamp}.json"
        self.backup_file = Path(self.chromadb_path).parent / "scripts" / backup_filename
        try:
            # å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—
            if self.collection is None:
                print("âœ— ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return ""
            all_data = self.collection.get(include=['documents', 'metadatas'])
            
            backup_data = {
                "backup_info": {
                    "collection_name": collection_name,
                    "timestamp": timestamp,
                    "document_count": len(all_data.get('documents') or []),
                    "schema_version": "v2.0"
                },
                "documents": all_data.get('documents', []),
                "metadatas": all_data.get('metadatas', [])
            }
            
            with open(self.backup_file, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2, ensure_ascii=False)
            
            file_size = self.backup_file.stat().st_size / (1024*1024)
            print(f"âœ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†: {backup_filename} ({file_size:.2f}MB)")
            return backup_filename
            
        except Exception as e:
            print(f"âœ— ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¤±æ•—: {e}")
            return ""
    def analyze_current_metadata(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        try:
            if self.collection is None:
                print("âœ— ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return {}
            all_data = self.collection.get(include=['metadatas'])
            metadatas = all_data.get('metadatas') or []
            
            # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä½¿ç”¨çŠ¶æ³åˆ†æ
            field_usage = {}
            total_docs = len(metadatas)
            
            for metadata in metadatas:
                if metadata:
                    for field in metadata.keys():
                        if field not in field_usage:
                            field_usage[field] = 0
                        field_usage[field] += 1
            
            # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            consistency_scores = {}
            for field, count in field_usage.items():
                consistency_scores[field] = (count / total_docs) * 100
            
            analysis = {
                "total_documents": total_docs,
                "unique_fields": len(field_usage),
                "field_usage": field_usage,
                "consistency_scores": consistency_scores,
                "average_consistency": sum(consistency_scores.values()) / len(consistency_scores) if consistency_scores else 0
            }
            
            print(f"ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†:")
            print(f"   - ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {total_docs}")
            print(f"   - ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {len(field_usage)}")
            print(f"   - å¹³å‡ä¸€è²«æ€§: {analysis['average_consistency']:.1f}%")
            
            return analysis
            
        except Exception as e:
            print(f"âœ— ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æå¤±æ•—: {e}")
            return {}
    
    def generate_content_hash(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
    
    def detect_language(self, content: str) -> str:
        """ç°¡æ˜“è¨€èªæ¤œå‡º"""
        japanese_chars = len(re.findall(r'[ã‚-ã‚“ã‚¢-ãƒ³ãƒ¼ä¸€-é¾¯]', content))
        english_chars = len(re.findall(r'[a-zA-Z]', content))
        
        if japanese_chars > english_chars:
            return "ja"
        elif english_chars > 0:
            return "en"
        else:
            return "unknown"
    
    def calculate_complexity_score(self, content: str) -> float:
        """è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not content:
            return 0.0
        
        # åŸºæœ¬çš„ãªè¤‡é›‘åº¦æŒ‡æ¨™
        unique_words = len(set(content.split()))
        total_words = len(content.split())
        avg_word_length = sum(len(word) for word in content.split()) / total_words if total_words > 0 else 0
        
        complexity = (unique_words / total_words if total_words > 0 else 0) * avg_word_length / 10
        return min(complexity, 1.0)
    
    def calculate_importance_score(self, metadata: Dict, content: str) -> float:
        """é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.5  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹é‡è¦åº¦èª¿æ•´
        if metadata.get('source_type') == 'PDF':
            score += 0.2
        elif metadata.get('document_type') in ['system_report', 'specification']:
            score += 0.3
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·ã«ã‚ˆã‚‹èª¿æ•´
        content_length = len(content)
        if content_length > 2000:
            score += 0.2
        elif content_length < 100:
            score -= 0.2
        
        return min(max(score, 0.0), 1.0)
    
    def unify_metadata(self, old_metadata: Dict, content: str, doc_id: str) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–å‡¦ç†"""
        unified = {}
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è¨­å®š
        unified["document_id"] = doc_id
        unified["content_hash"] = self.generate_content_hash(content)
        unified["project"] = old_metadata.get("project", "unknown_project")
        unified["source"] = old_metadata.get("source", "unknown_source")
        unified["timestamp"] = old_metadata.get("timestamp", datetime.now().isoformat())
        unified["content_length"] = len(content)
        unified["version"] = "v2.0"
        
        # content_type ã®çµ±ä¸€
        if "type" in old_metadata:
            unified["content_type"] = old_metadata["type"]
        elif "source_type" in old_metadata:
            unified["content_type"] = old_metadata["source_type"]
        elif "document_type" in old_metadata:
            unified["content_type"] = old_metadata["document_type"]
        else:
            unified["content_type"] = "unknown"
        
        # category ã®çµ±ä¸€
        content_type = unified["content_type"]
        unified["category"] = self.format_category_mapping.get(content_type, "general")
        
        # source_type ã®çµ±ä¸€
        if old_metadata.get("file_path"):
            unified["source_type"] = "file"
        elif old_metadata.get("document_type"):
            unified["source_type"] = "document"
        else:
            unified["source_type"] = "manual"
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è¨­å®š
        unified["language"] = self.detect_language(content)
        unified["complexity_score"] = self.calculate_complexity_score(content)
        unified["importance_score"] = self.calculate_importance_score(old_metadata, content)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ä¿æŒï¼ˆé©åˆ‡ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
        if old_metadata.get("file_path"):
            unified["file_path"] = old_metadata["file_path"]
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã®çµ±ä¸€
        if any(key in old_metadata for key in ["chunk_index", "total_chunks", "chunk_size"]):
            unified["chunk_info"] = {
                "index": old_metadata.get("chunk_index", 0),
                "total": old_metadata.get("total_chunks", 1),
                "size": old_metadata.get("chunk_size", len(content))
            }
        
        # å“è³ªã‚¹ã‚³ã‚¢
        unified["quality_score"] = 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        unified["validation_status"] = "validated"
        
        return unified
    
    def execute_unification(self, dry_run: bool = True) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–å®Ÿè¡Œ"""
        print(f"ğŸ”„ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–é–‹å§‹ (dry_run={dry_run})")
        
        try:
            # å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—
            if self.collection is None:
                print("âœ— ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return {"error": "collection is not initialized"}
            all_data = self.collection.get(include=['documents', 'metadatas'])
            
            processed = 0
            errors = 0
            changes = 0
            
            ids = all_data.get('ids') or []
            documents = all_data.get('documents') or []
            metadatas = all_data.get('metadatas') or []
            for i, (doc_id, content, old_metadata) in enumerate(zip(ids, documents, metadatas)):
                try:
                    # çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                    unified_metadata = self.unify_metadata(dict(old_metadata) if old_metadata else {}, content or "", doc_id)
                    
                    # å¤‰æ›´ãƒã‚§ãƒƒã‚¯
                    if old_metadata != unified_metadata:
                        changes += 1
                        
                        if not dry_run:
                            # å®Ÿéš›ã®æ›´æ–°
                            self.collection.update(
                                ids=[doc_id],
                                metadatas=[unified_metadata]
                            )
                    
                    processed += 1
                    
                    if processed % 50 == 0:
                        print(f"   å‡¦ç†ä¸­: {processed}/{len(all_data['ids'])}")
                        
                except Exception as e:
                    print(f"   ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ {doc_id}: {e}")
                    errors += 1
            
            result = {
                "processed": processed,
                "errors": errors,
                "changes": changes,
                "total": len(all_data['ids']),
                "success_rate": (processed / len(all_data['ids'])) * 100 if all_data['ids'] else 0
            }
            
            print(f"âœ“ çµ±ä¸€åŒ–{'ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³' if dry_run else 'å®Ÿè¡Œ'}å®Œäº†:")
            print(f"   - å‡¦ç†æ¸ˆã¿: {processed}")
            print(f"   - å¤‰æ›´å¯¾è±¡: {changes}")
            print(f"   - ã‚¨ãƒ©ãƒ¼: {errors}")
            print(f"   - æˆåŠŸç‡: {result['success_rate']:.1f}%")
            
            return result
            
        except Exception as e:
            print(f"âœ— çµ±ä¸€åŒ–å®Ÿè¡Œå¤±æ•—: {e}")
            return {"error": str(e)}
    
    def validate_result(self) -> Dict[str, Any]:
        """çµ±ä¸€åŒ–çµæœæ¤œè¨¼"""
        print("ğŸ” çµ±ä¸€åŒ–çµæœã‚’æ¤œè¨¼ä¸­...")
        
        try:
            analysis = self.analyze_current_metadata()
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            if self.collection is None:
                print("âœ— ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return {"error": "collection is not initialized"}
            all_data = self.collection.get(include=['metadatas'])
            metadatas = all_data.get('metadatas') or []
            
            required_fields = self.unified_schema["required_fields"].keys()
            field_completeness = {}
            
            for field in required_fields:
                count = sum(1 for metadata in metadatas if metadata and field in metadata)
                field_completeness[field] = (count / len(metadatas)) * 100 if metadatas else 0
            
            avg_completeness = sum(field_completeness.values()) / len(field_completeness)
            
            validation = {
                "field_completeness": field_completeness,
                "average_completeness": avg_completeness,
                "schema_compliance": avg_completeness >= 95.0,
                "total_documents": len(metadatas)
            }
            
            print(f"âœ“ æ¤œè¨¼å®Œäº†:")
            print(f"   - å¹³å‡å®Œå…¨æ€§: {avg_completeness:.1f}%")
            print(f"   - ã‚¹ã‚­ãƒ¼ãƒæº–æ‹ : {'âœ“' if validation['schema_compliance'] else 'âœ—'}")
            
            return validation
            
        except Exception as e:
            print(f"âœ— æ¤œè¨¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def generate_report(self, analysis: Dict, unification_result: Dict, validation: Dict) -> str:
        """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = Path(self.chromadb_path).parent / "scripts" / f"metadata_unification_report_{timestamp}.json"
        
        report = {
            "report_info": {
                "timestamp": timestamp,
                "schema_version": "v2.0",
                "operation": "metadata_unification"
            },
            "before_analysis": analysis,
            "unification_result": unification_result,
            "after_validation": validation,
            "schema_definition": self.unified_schema
        }
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file.name}")
            return str(report_file)
            
        except Exception as e:
            print(f"âœ— ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå¤±æ•—: {e}")
            return ""

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ ChromaDB ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–ã‚·ã‚¹ãƒ†ãƒ  v2.0 èµ·å‹•")
    print("=" * 60)
    
    # è¨­å®š
    collection_name = "mcp_production_knowledge"
      # ChromaDBãƒ‘ã‚¹ã‚’ç’°å¢ƒã«å¿œã˜ã¦å‹•çš„å–å¾—
    possible_paths = [
        r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_",
        r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared_Chromadb\chromadb_data",
        r"F:\å‰¯æ¥­\VSC_WorkSpace\MySisterDB\chromadb_data", 
        r"F:\å‰¯æ¥­\VSC_WorkSpace\MCP_ChromaDB00\chromadb_data"
    ]
    
    chromadb_path = None
    for path in possible_paths:
        if Path(path).exists():
            chromadb_path = path
            break
    
    if not chromadb_path:
        print("âœ— ChromaDBãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = MetadataUnificationSystem(chromadb_path)
    
    if not system.initialize_chromadb():
        sys.exit(1)
    
    if not system.get_collection(collection_name):
        sys.exit(1)
    
    try:
        # STEP 1: ç¾åœ¨ã®åˆ†æ
        print("\nğŸ“Š STEP 1: ç¾åœ¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ")
        analysis = system.analyze_current_metadata()
        
        # STEP 2: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        print("\nğŸ’¾ STEP 2: ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
        backup_file = system.create_backup(collection_name)
        if not backup_file:
            print("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™")
            sys.exit(1)
        
        # STEP 3: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ
        print("\nğŸ§ª STEP 3: çµ±ä¸€åŒ–ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ")
        dry_result = system.execute_unification(dry_run=True)
        
        # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        print(f"\nğŸ¤” å®Ÿéš›ã«{dry_result.get('changes', 0)}ä»¶ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã™ã‹ï¼Ÿ")
        print("   - ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¸ˆã¿")
        print("   - å¤‰æ›´ã¯å¯é€†çš„ã§ã™")
        
        confirm = input("å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        
        if confirm == 'y':
            # STEP 4: å®Ÿéš›ã®çµ±ä¸€åŒ–å®Ÿè¡Œ
            print("\nğŸ”„ STEP 4: å®Ÿéš›ã®çµ±ä¸€åŒ–å®Ÿè¡Œ")
            unification_result = system.execute_unification(dry_run=False)
            
            # STEP 5: çµæœæ¤œè¨¼
            print("\nğŸ” STEP 5: çµ±ä¸€åŒ–çµæœã®æ¤œè¨¼")
            validation = system.validate_result()
            
            # STEP 6: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            print("\nğŸ“„ STEP 6: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
            report_file = system.generate_report(analysis, unification_result, validation)
            
            print("\n" + "=" * 60)
            print("ğŸ“‹ çµ±ä¸€åŒ–å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 60)
            print(f"å‡¦ç†æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {unification_result.get('processed', 0)}")
            print(f"æ›´æ–°ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {unification_result.get('changes', 0)}")
            print(f"ã‚¨ãƒ©ãƒ¼æ•°: {unification_result.get('errors', 0)}")
            print(f"æˆåŠŸç‡: {unification_result.get('success_rate', 0):.1f}%")
            print(f"ã‚¹ã‚­ãƒ¼ãƒæº–æ‹ ç‡: {validation.get('average_completeness', 0):.1f}%")
            print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
            print(f"ãƒ¬ãƒãƒ¼ãƒˆ: {Path(report_file).name if report_file else 'ãªã—'}")
            print("âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€åŒ–å®Œäº†!")
            
        else:
            print("æ“ä½œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
    
    except KeyboardInterrupt:
        print("\næ“ä½œãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
