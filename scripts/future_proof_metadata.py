#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœªæ¥å¯¾å¿œå‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
===============================

ç›®çš„: å¤šç¨®å¤šæ§˜ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‹ã‚‰ã®å­¦ç¿’ã«å¯¾å¿œã—ãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±ä¸€ç®¡ç†
å¯¾å¿œäºˆå®š: PDF, MD, TXT, HTML, DOCX, XLSX, JSON, XML, YAML, CSV, ãªã©

ç‰¹å¾´:
1. å‹•çš„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒå¯¾å¿œ
2. ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼åˆ¥ã®è‡ªå‹•ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¨å®š
3. ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ 
4. é‡è¤‡æ¤œå‡ºãƒ»çµ±åˆæ©Ÿèƒ½
5. ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–
"""

import chromadb
import json
import os
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import mimetypes

class FutureProofMetadataManager:
    """æœªæ¥å¯¾å¿œå‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, collection_name: str = "mcp_production_knowledge"):
        """åˆæœŸåŒ–"""
        self.client = chromadb.PersistentClient(
            path="F:/å‰¯æ¥­/VSC_WorkSpace/MCP_ChromaDB00/chroma"
        )
        self.collection_name = collection_name
        
        try:
            self.collection = self.client.get_collection(collection_name)
        except:
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            self.collection = self.client.create_collection(collection_name)
        
        # çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒï¼ˆv2.0ï¼‰
        self.unified_schema = {
            # === å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ ===
            'document_id': str,           # ä¸€æ„è­˜åˆ¥å­
            'content_hash': str,          # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰
            'project': str,               # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
            'source': str,                # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
            'timestamp': str,             # ç™»éŒ²ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            'content_type': str,          # document/chunk/report/manual/system
            'category': str,              # auto-classified category
            'source_type': str,           # pdf/md/txt/html/docx/xlsx/json/etc
            'content_length': int,        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·
            'version': str,               # ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³
            
            # === åˆ†æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ ===
            'language': str,              # è¨€èªï¼ˆja/en/mixedï¼‰
            'complexity_score': float,    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¤‡é›‘åº¦ï¼ˆ0-1ï¼‰
            'importance_score': float,    # é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
            'freshness_score': float,     # é®®åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
            
            # === ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ± ===
            'file_path': str,            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            'file_size': int,            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
            'file_created': str,         # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ—¥æ™‚
            'file_modified': str,        # ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥æ™‚
            
            # === ãƒãƒ£ãƒ³ã‚¯æƒ…å ± ===
            'chunk_info': dict,          # {"index": 0, "total": 1, "size": 1000, "overlap": 200}
            
            # === å“è³ªç®¡ç† ===
            'quality_score': float,      # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
            'validation_status': str,    # validated/pending/failed
            'last_validated': str,       # æœ€çµ‚æ¤œè¨¼æ—¥æ™‚
            
            # === é–¢é€£æ€§ ===
            'related_documents': list,   # é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
            'tags': list,               # ã‚¿ã‚°ãƒªã‚¹ãƒˆ
            'keywords': list,           # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼åˆ¥å‡¦ç†å®šç¾©
        self.file_type_handlers = {
            '.pdf': self._handle_pdf_metadata,
            '.md': self._handle_markdown_metadata,
            '.txt': self._handle_text_metadata,
            '.html': self._handle_html_metadata,
            '.docx': self._handle_docx_metadata,
            '.xlsx': self._handle_excel_metadata,
            '.json': self._handle_json_metadata,
            '.xml': self._handle_xml_metadata,
            '.yaml': self._handle_yaml_metadata,
            '.yml': self._handle_yaml_metadata,
            '.csv': self._handle_csv_metadata,
        }
        
        # ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•åˆ†é¡ãƒ«ãƒ¼ãƒ«
        self.category_rules = {
            'technical': [
                'æŠ€è¡“', 'technical', 'implementation', 'å®Ÿè£…', 'code', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ',
                'api', 'ã‚·ã‚¹ãƒ†ãƒ ', 'architecture', 'design', 'è¨­è¨ˆ'
            ],
            'documentation': [
                'readme', 'doc', 'manual', 'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«', 'guide', 'ã‚¬ã‚¤ãƒ‰',
                'spec', 'ä»•æ§˜', 'instruction', 'æ‰‹é †'
            ],
            'report': [
                'report', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'å ±å‘Š', 'åˆ†æ', 'analysis', 'è©•ä¾¡',
                'assessment', 'summary', 'ã¾ã¨ã‚'
            ],
            'business': [
                'business', 'ãƒ“ã‚¸ãƒã‚¹', 'æ¥­å‹™', 'å–¶æ¥­', 'sales', 'é¡§å®¢',
                'customer', 'requirement', 'è¦ä»¶'
            ],
            'data': [
                'data', 'ãƒ‡ãƒ¼ã‚¿', 'çµ±è¨ˆ', 'statistics', 'metrics', 'æŒ‡æ¨™',
                'csv', 'excel', 'database', 'db'
            ],
            'media': [
                'image', 'ç”»åƒ', 'video', 'å‹•ç”»', 'audio', 'éŸ³å£°',
                'multimedia', 'ãƒ¡ãƒ‡ã‚£ã‚¢'
            ]
        }

    def create_document_hash(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’ä½œæˆ"""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]

    def analyze_content_complexity(self, content: str) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¤‡é›‘åº¦ã‚’åˆ†æ"""
        if not content:
            return 0.0
        
        # åŸºæœ¬çš„ãªè¤‡é›‘åº¦æŒ‡æ¨™
        factors = {
            'length': min(len(content) / 10000, 1.0) * 0.3,
            'lines': min(content.count('\n') / 100, 1.0) * 0.2,
            'words': min(len(content.split()) / 1000, 1.0) * 0.2,
            'code': (content.count('{') + content.count('def ') + content.count('class ')) / 50 * 0.3
        }
        
        return min(sum(factors.values()), 1.0)

    def analyze_content_importance(self, content: str, metadata: Dict) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¦åº¦ã‚’åˆ†æ"""
        importance = 0.0
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦‹å‡ºã—ã®å­˜åœ¨
        if any(marker in content for marker in ['#', '==', '--', 'Title:', 'ã‚¿ã‚¤ãƒˆãƒ«:']):
            importance += 0.2
        
        # æŠ€è¡“çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨
        tech_keywords = ['API', 'ã‚·ã‚¹ãƒ†ãƒ ', 'implementation', 'å®Ÿè£…', 'database']
        importance += min(sum(1 for kw in tech_keywords if kw in content) / 10, 0.3)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼ˆå¤§ãã„ã»ã©é‡è¦ï¼‰
        content_length = len(content)
        if content_length > 5000:
            importance += 0.2
        elif content_length > 1000:
            importance += 0.1
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£æ€§
        if metadata.get('project', '').startswith('MCP_'):
            importance += 0.3
        
        return min(importance, 1.0)

    def classify_category(self, content: str, metadata: Dict) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•åˆ†é¡"""
        content_lower = content.lower()
        
        # ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚³ã‚¢è¨ˆç®—
        category_scores = {}
        for category, keywords in self.category_rules.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            if score > 0:
                category_scores[category] = score
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™
        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        
        return 'other'

    def detect_language(self, content: str) -> str:
        """è¨€èªã‚’æ¤œå‡º"""
        if not content:
            return 'unknown'
        
        # æ—¥æœ¬èªæ–‡å­—ã®å‰²åˆ
        japanese_chars = sum(1 for char in content if '\u3040' <= char <= '\u309F' or 
                           '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF')
        
        japanese_ratio = japanese_chars / len(content) if content else 0
        
        if japanese_ratio > 0.1:
            return 'ja' if japanese_ratio > 0.3 else 'mixed'
        else:
            return 'en'

    def _handle_pdf_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """PDFå›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'is_chunked': 'chunk_index' in metadata,
            'pdf_pages': metadata.get('total_chunks', 1),
            'extraction_method': 'pypdf'
        })
        return enhanced

    def _handle_markdown_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """Markdownå›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'has_headers': True,  # å®Ÿéš›ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æã§åˆ¤å®š
            'is_documentation': 'readme' in file_path.lower(),
            'markup_type': 'markdown'
        })
        return enhanced

    def _handle_text_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å›ºæœ‰ã®å‡¦ç†"""
        return metadata

    def _handle_html_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """HTMLå›ºæœ‰ã®å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'markup_type': 'html',
            'is_web_content': True
        })
        return enhanced

    def _handle_docx_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """Wordæ–‡æ›¸å›ºæœ‰ã®å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'document_format': 'docx',
            'is_office_document': True
        })
        return enhanced

    def _handle_excel_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """Excelå›ºæœ‰ã®å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'document_format': 'xlsx',
            'is_spreadsheet': True,
            'data_type': 'structured'
        })
        return enhanced

    def _handle_json_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """JSONå›ºæœ‰ã®å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'data_format': 'json',
            'is_structured_data': True
        })
        return enhanced

    def _handle_xml_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """XMLå›ºæœ‰ã®å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'data_format': 'xml',
            'is_structured_data': True
        })
        return enhanced

    def _handle_yaml_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """YAMLå›ºæœ‰ã®å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'data_format': 'yaml',
            'is_config_file': True
        })
        return enhanced

    def _handle_csv_metadata(self, file_path: str, metadata: Dict) -> Dict:
        """CSVå›ºæœ‰ã®å‡¦ç†"""
        enhanced = metadata.copy()
        enhanced.update({
            'data_format': 'csv',
            'is_tabular_data': True
        })
        return enhanced

    def create_unified_metadata(self, content: str, original_metadata: Dict, 
                              document_id: Optional[str] = None) -> Dict[str, Any]:
        """çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        
        if document_id is None:
            document_id = original_metadata.get('document_id', f"doc_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{self.create_document_hash(content)[:8]}")
        
        # åŸºæœ¬åˆ†æ
        content_hash = self.create_document_hash(content)
        complexity = self.analyze_content_complexity(content)
        importance = self.analyze_content_importance(content, original_metadata)
        category = self.classify_category(content, original_metadata)
        language = self.detect_language(content)
        
        # é®®åº¦ã‚¹ã‚³ã‚¢ï¼ˆæ–°ã—ã„ã»ã©é«˜ã„ï¼‰
        timestamp_str = original_metadata.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        try:
            doc_time = datetime.strptime(timestamp_str.split('.')[0], '%Y-%m-%d %H:%M:%S')
            days_old = (datetime.now() - doc_time).days
            freshness = max(0.0, 1.0 - (days_old / 365))  # 1å¹´ã§0ã«ãªã‚‹
        except:
            freshness = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        quality_factors = {
            'has_content': 1.0 if len(content.strip()) > 50 else 0.5,
            'has_metadata': 0.8 if len(original_metadata) > 3 else 0.3,
            'complexity': complexity * 0.5,
            'importance': importance * 0.7
        }
        quality_score = sum(quality_factors.values()) / len(quality_factors)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®å‡¦ç†
        file_path = original_metadata.get('file_path', '')
        file_ext = Path(file_path).suffix.lower() if file_path else ''
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼åˆ¥ã®æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        if file_ext in self.file_type_handlers:
            enhanced_metadata = self.file_type_handlers[file_ext](file_path, original_metadata)
        else:
            enhanced_metadata = original_metadata.copy()
        
        # çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
        unified = {
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            'document_id': document_id,
            'content_hash': content_hash,
            'project': original_metadata.get('project', 'MCP_ChromaDB_Documentation'),
            'source': original_metadata.get('source', 'unknown'),
            'timestamp': timestamp_str,
            'content_type': self._determine_content_type(original_metadata, content),
            'category': category,
            'source_type': self._determine_source_type(original_metadata, file_path),
            'content_length': len(content),
            'version': '2.0',
            
            # åˆ†æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            'language': language,
            'complexity_score': round(complexity, 3),
            'importance_score': round(importance, 3),
            'freshness_score': round(freshness, 3),
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            'file_path': file_path,
            'file_size': original_metadata.get('file_size', len(content.encode('utf-8'))),
            'file_created': original_metadata.get('file_created', timestamp_str),
            'file_modified': original_metadata.get('file_modified', timestamp_str),
            
            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±
            'chunk_info': self._extract_chunk_info(original_metadata, content),
            
            # å“è³ªç®¡ç†
            'quality_score': round(quality_score, 3),
            'validation_status': 'validated',
            'last_validated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            
            # é–¢é€£æ€§
            'related_documents': [],
            'tags': self._extract_tags(content, original_metadata),
            'keywords': self._extract_keywords(content),
        }
        
        # å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’ä¿æŒ
        for key, value in enhanced_metadata.items():
            if key not in unified and value is not None:
                unified[f'original_{key}'] = value
        
        return unified

    def _determine_content_type(self, metadata: Dict, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š"""
        if 'chunk_index' in metadata:
            return 'chunk'
        elif 'document_type' in metadata and 'report' in str(metadata.get('document_type', '')).lower():
            return 'report'
        elif metadata.get('source') == 'manual_entry':
            return 'manual'
        elif 'system' in content.lower() and len(content) < 500:
            return 'system'
        else:
            return 'document'

    def _determine_source_type(self, metadata: Dict, file_path: str) -> str:
        """ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š"""
        if 'source_type' in metadata:
            return metadata['source_type']
        elif file_path:
            ext = Path(file_path).suffix.lower()
            return ext[1:] if ext else 'unknown'
        else:
            return 'manual'

    def _extract_chunk_info(self, metadata: Dict, content: str) -> Dict:
        """ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’æŠ½å‡º"""
        if any(field in metadata for field in ['chunk_index', 'chunk_size', 'total_chunks']):
            return {
                'index': metadata.get('chunk_index', 0),
                'total': metadata.get('total_chunks', 1),
                'size': metadata.get('chunk_size', len(content)),
                'overlap': metadata.get('overlap', 0)
            }
        return {'index': 0, 'total': 1, 'size': len(content), 'overlap': 0}

    def _extract_tags(self, content: str, metadata: Dict) -> List[str]:
        """ã‚¿ã‚°ã‚’æŠ½å‡º"""
        tags = []
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚°
        category = self.classify_category(content, metadata)
        tags.append(f"category:{category}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚¿ã‚°
        source_type = self._determine_source_type(metadata, metadata.get('file_path', ''))
        tags.append(f"format:{source_type}")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚°
        project = metadata.get('project', '')
        if project:
            tags.append(f"project:{project}")
        
        # ç‰¹æ®Šã‚¿ã‚°
        if 'chunk_index' in metadata:
            tags.append("chunked")
        if len(content) > 10000:
            tags.append("large-content")
        if any(word in content.lower() for word in ['api', 'ã‚·ã‚¹ãƒ†ãƒ ', 'database']):
            tags.append("technical")
        
        return list(set(tags))

    def _extract_keywords(self, content: str) -> List[str]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šé«˜åº¦ãªå‡¦ç†ãŒå¿…è¦ï¼‰
        import re
        
        # å¤§æ–‡å­—ã§å§‹ã¾ã‚‹å˜èªã‚„æŠ€è¡“ç”¨èªã‚’æŠ½å‡º
        keywords = re.findall(r'\b[A-Z][a-zA-Z]{2,}\b', content)
        
        # æ—¥æœ¬èªã®æŠ€è¡“ç”¨èª
        jp_tech_terms = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{3,}|ã‚·ã‚¹ãƒ†ãƒ |ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹|ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³|ãƒ—ãƒ­ã‚°ãƒ©ãƒ ', content)
        
        all_keywords = keywords + jp_tech_terms
        
        # é »åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½10å€‹ã‚’è¿”ã™
        from collections import Counter
        keyword_counts = Counter(all_keywords)
        
        return [kw for kw, count in keyword_counts.most_common(10)]

    def migrate_all_metadata(self, dry_run: bool = True) -> Dict[str, Any]:
        """å…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ–°å½¢å¼ã«ç§»è¡Œ"""
        print(f"ğŸš€ çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å½¢å¼ã¸ã®ç§»è¡Œé–‹å§‹ (dry_run={dry_run})")
        
        all_docs = self.collection.get()
        # Ensure all values are lists to avoid iteration errors
        ids = all_docs.get('ids') or []
        metadatas = all_docs.get('metadatas') or []
        documents = all_docs.get('documents') or []
        total_docs = len(ids)
        
        migration_report = {
            'total_documents': total_docs,
            'processed': 0,
            'upgraded': 0,
            'errors': [],
            'duplicate_hashes': {},
            'quality_distribution': {'high': 0, 'medium': 0, 'low': 0},
            'dry_run': dry_run
        }
        
        new_metadatas = []
        
        print(f"ğŸ“Š å‡¦ç†å¯¾è±¡: {total_docs} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
        
        for i, (doc_id, metadata, content) in enumerate(zip(
            ids, metadatas, documents
        )):
            try:
                if metadata is None:
                    metadata = {}
                
                # æ—¢ã«æ–°å½¢å¼ã‹ãƒã‚§ãƒƒã‚¯
                if metadata.get('version') == '2.0':
                    print(f"   ã‚¹ã‚­ãƒƒãƒ—: {doc_id} (æ—¢ã«æ–°å½¢å¼)")
                    new_metadatas.append(metadata)
                    continue
                
                # çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                unified_metadata = self.create_unified_metadata(content, dict(metadata), doc_id)
                new_metadatas.append(unified_metadata)
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                content_hash = unified_metadata['content_hash']
                if content_hash in migration_report['duplicate_hashes']:
                    migration_report['duplicate_hashes'][content_hash].append(doc_id)
                else:
                    migration_report['duplicate_hashes'][content_hash] = [doc_id]
                
                # å“è³ªåˆ†å¸ƒ
                quality = unified_metadata['quality_score']
                if quality >= 0.8:
                    migration_report['quality_distribution']['high'] += 1
                elif quality >= 0.5:
                    migration_report['quality_distribution']['medium'] += 1
                else:
                    migration_report['quality_distribution']['low'] += 1
                
                migration_report['processed'] += 1
                migration_report['upgraded'] += 1
                
                if i % 50 == 0:
                    print(f"   é€²æ—: {i}/{total_docs} ({i/total_docs*100:.1f}%)")
                    
            except Exception as e:
                migration_report['errors'].append({
                    'document_id': doc_id,
                    'error': str(e)
                })
                new_metadatas.append(metadata)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
        
        # é‡è¤‡ã®çµ±è¨ˆ
        duplicates = {hash_val: ids for hash_val, ids in migration_report['duplicate_hashes'].items() if len(ids) > 1}
        migration_report['duplicates_found'] = len(duplicates)
        migration_report['duplicate_groups'] = duplicates
        
        # å®Ÿéš›ã®æ›´æ–°å®Ÿè¡Œ
        if not dry_run and new_metadatas:
            print("ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚’å®Ÿè¡Œä¸­...")
            try:
                self.collection.update(
                    ids=all_docs['ids'],
                    metadatas=new_metadatas
                )
                print("âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº†")
            except Exception as e:
                migration_report['errors'].append({
                    'operation': 'bulk_update',
                    'error': str(e)
                })
        
        return migration_report

    def analyze_collection_health(self) -> Dict[str, Any]:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å¥å…¨æ€§ã‚’åˆ†æ"""
        print("ğŸ” ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¥å…¨æ€§åˆ†æä¸­...")
        
        all_docs = self.collection.get()
        
        health_report = {
            'total_documents': len(all_docs['ids']),
            'schema_versions': {},
            'quality_stats': {
                'average_quality': 0.0,
                'high_quality_docs': 0,
                'medium_quality_docs': 0,
                'low_quality_docs': 0
            },
            'content_analysis': {
                'average_length': 0,
                'languages': {},
                'categories': {},
                'source_types': {}
            },
            'data_integrity': {
                'missing_fields': {},
                'inconsistent_types': {},
                'duplicate_content': 0
            }
        }
        
        quality_scores = []
        content_lengths = []
        content_hashes = set()
        
        for metadata in (all_docs.get('metadatas') or []):
            if not metadata:
                continue
            
            # ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³çµ±è¨ˆ
            version = metadata.get('version', 'unknown')
            health_report['schema_versions'][version] = health_report['schema_versions'].get(version, 0) + 1
            
            # å“è³ªçµ±è¨ˆ
            quality = metadata.get('quality_score', 0.0)
            try:
                if quality is None:
                    quality_val = 0.0
                else:
                    quality_val = float(quality)
            except (TypeError, ValueError):
                quality_val = 0.0
            quality_scores.append(quality_val)
            
            if quality_val >= 0.8:
                health_report['quality_stats']['high_quality_docs'] += 1
            elif quality_val >= 0.5:
                health_report['quality_stats']['medium_quality_docs'] += 1
            else:
                health_report['quality_stats']['low_quality_docs'] += 1
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ
            content_lengths.append(metadata.get('content_length', 0))
            
            language = metadata.get('language', 'unknown')
            health_report['content_analysis']['languages'][language] = health_report['content_analysis']['languages'].get(language, 0) + 1
            
            category = metadata.get('category', 'unknown')
            health_report['content_analysis']['categories'][category] = health_report['content_analysis']['categories'].get(category, 0) + 1
            
            source_type = metadata.get('source_type', 'unknown')
            health_report['content_analysis']['source_types'][source_type] = health_report['content_analysis']['source_types'].get(source_type, 0) + 1
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            content_hash = metadata.get('content_hash')
            if content_hash:
                if content_hash in content_hashes:
                    health_report['data_integrity']['duplicate_content'] += 1
                content_hashes.add(content_hash)
        
        # çµ±è¨ˆè¨ˆç®—
        if quality_scores:
            health_report['quality_stats']['average_quality'] = sum(quality_scores) / len(quality_scores)
        
        if content_lengths:
            health_report['content_analysis']['average_length'] = sum(content_lengths) / len(content_lengths)
        
        return health_report


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ æœªæ¥å¯¾å¿œå‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
    print("=" * 60)
    
    manager = FutureProofMetadataManager()
    
    # Step 1: ç¾åœ¨ã®å¥å…¨æ€§åˆ†æ
    print("\nğŸ“Š STEP 1: ç¾åœ¨ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¥å…¨æ€§åˆ†æ")
    health_report = manager.analyze_collection_health()
    
    print(f"ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {health_report['total_documents']}")
    print(f"ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³: {health_report['schema_versions']}")
    print(f"å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {health_report['quality_stats']['average_quality']:.3f}")
    print(f"è¨€èªåˆ†å¸ƒ: {health_report['content_analysis']['languages']}")
    print(f"ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ: {health_report['content_analysis']['categories']}")
    
    # Step 2: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ç§»è¡Œ
    print("\nğŸ§ª STEP 2: çµ±ä¸€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç§»è¡Œï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼‰")
    dry_run_result = manager.migrate_all_metadata(dry_run=True)
    
    print(f"ç§»è¡Œå¯¾è±¡: {dry_run_result['upgraded']} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
    print(f"é‡è¤‡ç™ºè¦‹: {dry_run_result['duplicates_found']} ã‚°ãƒ«ãƒ¼ãƒ—")
    print(f"å“è³ªåˆ†å¸ƒ: {dry_run_result['quality_distribution']}")
    
    if dry_run_result['errors']:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {len(dry_run_result['errors'])} ä»¶")
        for error in dry_run_result['errors'][:3]:
            print(f"  - {error}")
    
    # Step 3: å®Ÿè¡Œç¢ºèª
    if dry_run_result['upgraded'] > 0:
        print(f"\nâ“ {dry_run_result['upgraded']} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ–°å½¢å¼ã«ç§»è¡Œã—ã¾ã™ã‹ï¼Ÿ")
        print("   ã“ã®æ“ä½œã«ã‚ˆã‚Šã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå¤§å¹…ã«æ‹¡å¼µã•ã‚Œã€")
        print("   å°†æ¥ã®å¤šæ§˜ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚")
        
        response = input("\nå®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").lower()
        
        if response == 'y':
            print("\nğŸ”„ STEP 3: å®Ÿéš›ã®ç§»è¡Œå®Ÿè¡Œ")
            final_result = manager.migrate_all_metadata(dry_run=False)
            
            print("\nğŸ“‹ ç§»è¡Œå®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 40)
            print(f"å‡¦ç†æ¸ˆã¿: {final_result['processed']}")
            print(f"ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰: {final_result['upgraded']}")
            print(f"ã‚¨ãƒ©ãƒ¼: {len(final_result['errors'])}")
            
            # Step 4: ç§»è¡Œå¾Œã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            print("\nğŸ” STEP 4: ç§»è¡Œå¾Œã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯")
            final_health = manager.analyze_collection_health()
            print(f"æ–°ã—ã„å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {final_health['quality_stats']['average_quality']:.3f}")
            print(f"v2.0ã‚¹ã‚­ãƒ¼ãƒ: {final_health['schema_versions'].get('2.0', 0)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
            
        else:
            print("ç§»è¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_data = {
        'health_analysis': health_report,
        'migration_preview': dry_run_result,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    report_file = f"metadata_future_proof_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
    print("âœ… æœªæ¥å¯¾å¿œå‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†!")


if __name__ == "__main__":
    main()
