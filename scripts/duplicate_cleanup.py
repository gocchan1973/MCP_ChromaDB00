#!/usr/bin/env python3
"""
ChromaDB é‡è¤‡å‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ 
çµ±ä¸€åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã§ç™ºç”Ÿã—ãŸé‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å®‰å…¨ãªå‰Šé™¤
"""

import os
import sys
import json
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict

# ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import chromadb
    from chromadb.config import Settings
    print("âœ“ ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âœ— ChromaDB ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

class DuplicateCleanupSystem:
    """é‡è¤‡å‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, chromadb_path: str):
        self.chromadb_path = Path(chromadb_path)
        self.client = None
        self.collection = None
        
    def initialize(self) -> bool:
        """åˆæœŸåŒ–"""
        try:
            self.client = chromadb.PersistentClient(
                path=str(self.chromadb_path),
                settings=Settings(anonymized_telemetry=False)
            )
            print(f"âœ“ ChromaDBæ¥ç¶šæˆåŠŸ: {self.chromadb_path}")
            return True
        except Exception as e:
            print(f"âœ— ChromaDBæ¥ç¶šå¤±æ•—: {e}")
            return False
    
    def get_collection(self, collection_name: str) -> bool:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—"""
        try:
            self.collection = self.client.get_collection(collection_name)
            count = self.collection.count()
            print(f"âœ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—: {collection_name} ({count}ä»¶)")
            return True
        except Exception as e:
            print(f"âœ— ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—å¤±æ•—: {e}")
            return False
    
    def create_backup(self, collection_name: str) -> str:
        """é‡è¤‡å‰Šé™¤å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = f"duplicate_cleanup_backup_{timestamp}.json"
        backup_path = Path(self.chromadb_path).parent / "scripts" / backup_file
        
        try:
            result = self.collection.get(include=['documents', 'metadatas'])
            documents = result.get('documents', [])
            metadatas = result.get('metadatas', [])
            
            backup_data = {
                "backup_info": {
                    "collection_name": collection_name,
                    "timestamp": timestamp,
                    "document_count": len(documents),
                    "operation": "duplicate_cleanup"
                },
                "data": {
                    "documents": documents,
                    "metadatas": metadatas
                }
            }
            
            backup_path.parent.mkdir(exist_ok=True)
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2, ensure_ascii=False)
            
            file_size = backup_path.stat().st_size / (1024*1024)
            print(f"âœ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_file} ({file_size:.2f}MB)")
            return backup_file
            
        except Exception as e:
            print(f"âœ— ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            return None
    
    def detect_duplicates(self) -> Dict[str, Any]:
        """é‡è¤‡æ¤œå‡º"""
        print("ğŸ” é‡è¤‡æ¤œå‡ºé–‹å§‹...")
        
        try:
            result = self.collection.get(include=['documents', 'metadatas'])
            documents = result.get('documents', [])
            metadatas = result.get('metadatas', [])
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ã§é‡è¤‡æ¤œå‡º
            content_groups = defaultdict(list)
            
            for i, (doc, meta) in enumerate(zip(documents, metadatas)):
                content_hash = hashlib.md5((doc or "").encode('utf-8')).hexdigest()
                content_groups[content_hash].append({
                    'index': i,
                    'document': doc,
                    'metadata': meta,
                    'content_hash': content_hash
                })
            
            # é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç‰¹å®š
            duplicate_groups = {}
            total_duplicates = 0
            
            for content_hash, items in content_groups.items():
                if len(items) > 1:
                    duplicate_groups[content_hash] = items
                    total_duplicates += len(items) - 1  # 1ã¤ã¯æ®‹ã™ã®ã§-1
            
            analysis = {
                "total_documents": len(documents),
                "unique_content_hashes": len(content_groups),
                "duplicate_groups": len(duplicate_groups),
                "total_duplicates": total_duplicates,
                "duplicate_groups_detail": duplicate_groups
            }
            
            print(f"ğŸ“Š é‡è¤‡æ¤œå‡ºçµæœ:")
            print(f"   - ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(documents)}")
            print(f"   - ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(content_groups)}")
            print(f"   - é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—: {len(duplicate_groups)}")
            print(f"   - é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {total_duplicates}")
            
            return analysis
            
        except Exception as e:
            print(f"âœ— é‡è¤‡æ¤œå‡ºå¤±æ•—: {e}")
            return {"error": str(e)}
    
    def execute_cleanup(self, dry_run: bool = True) -> Dict[str, Any]:
        """é‡è¤‡å‰Šé™¤å®Ÿè¡Œ"""
        print(f"ğŸ§¹ é‡è¤‡å‰Šé™¤é–‹å§‹ (dry_run={dry_run})")
        
        try:
            # é‡è¤‡æ¤œå‡º
            dup_analysis = self.detect_duplicates()
            if "error" in dup_analysis:
                return dup_analysis
            
            duplicate_groups = dup_analysis.get("duplicate_groups_detail", {})
            
            if not duplicate_groups:
                print("âœ“ é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {"processed": 0, "removed": 0, "kept": 0}
            
            # å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆIDã‚‚å«ã‚€ï¼‰
            result = self.collection.get(include=['documents', 'metadatas'])
            documents = result.get('documents', [])
            metadatas = result.get('metadatas', [])
            
            # æ–°ã—ã„IDã‚’ç”Ÿæˆï¼ˆdoc_000001å½¢å¼ï¼‰
            ids_to_remove = []
            docs_to_keep = []
            metas_to_keep = []
            removed_count = 0
            kept_count = 0
            
            # å„é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰1ã¤ã ã‘ä¿æŒ
            processed_indices = set()
            
            for content_hash, items in duplicate_groups.items():
                # æœ€åˆã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä¿æŒã€æ®‹ã‚Šã‚’å‰Šé™¤å¯¾è±¡ã«
                keep_item = items[0]
                docs_to_keep.append(keep_item['document'])
                metas_to_keep.append(keep_item['metadata'])
                processed_indices.add(keep_item['index'])
                kept_count += 1
                
                # æ®‹ã‚Šã¯å‰Šé™¤å¯¾è±¡
                for item in items[1:]:
                    processed_indices.add(item['index'])
                    removed_count += 1
            
            # é‡è¤‡ã—ã¦ã„ãªã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ä¿æŒ
            for i, (doc, meta) in enumerate(zip(documents, metadatas)):
                if i not in processed_indices:
                    docs_to_keep.append(doc)
                    metas_to_keep.append(meta)
                    kept_count += 1
            
            if not dry_run:
                # å®Ÿéš›ã®å‰Šé™¤ãƒ»å†æ§‹ç¯‰
                print("ğŸ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å†æ§‹ç¯‰ä¸­...")
                
                # å…ƒã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
                collection_name = self.collection.name
                self.client.delete_collection(collection_name)
                
                # æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
                self.collection = self.client.create_collection(collection_name)
                
                # é‡è¤‡é™¤å»å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                if docs_to_keep:
                    new_ids = [f"doc_{i:06d}" for i in range(len(docs_to_keep))]
                    self.collection.add(
                        ids=new_ids,
                        documents=docs_to_keep,
                        metadatas=metas_to_keep
                    )
            
            result_summary = {
                "processed": len(documents),
                "removed": removed_count,
                "kept": kept_count,
                "final_count": len(docs_to_keep),
                "duplicate_groups": len(duplicate_groups)
            }
            
            print(f"âœ“ é‡è¤‡å‰Šé™¤{'ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³' if dry_run else 'å®Ÿè¡Œ'}å®Œäº†:")
            print(f"   - å‡¦ç†å¯¾è±¡: {len(documents)}")
            print(f"   - å‰Šé™¤å¯¾è±¡: {removed_count}")
            print(f"   - ä¿æŒå¯¾è±¡: {kept_count}")
            print(f"   - æœ€çµ‚ä»¶æ•°: {len(docs_to_keep)}")
            
            return result_summary
            
        except Exception as e:
            print(f"âœ— é‡è¤‡å‰Šé™¤å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def validate_cleanup(self) -> Dict[str, Any]:
        """å‰Šé™¤çµæœæ¤œè¨¼"""
        print("ğŸ” å‰Šé™¤çµæœæ¤œè¨¼ä¸­...")
        
        try:
            # å†åº¦é‡è¤‡ãƒã‚§ãƒƒã‚¯
            dup_check = self.detect_duplicates()
            
            if "error" in dup_check:
                return dup_check
            
            validation = {
                "final_document_count": dup_check["total_documents"],
                "remaining_duplicates": dup_check["total_duplicates"],
                "unique_content_count": dup_check["unique_content_hashes"],
                "cleanup_successful": dup_check["total_duplicates"] == 0
            }
            
            print(f"âœ“ æ¤œè¨¼å®Œäº†:")
            print(f"   - æœ€çµ‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {validation['final_document_count']}")
            print(f"   - æ®‹å­˜é‡è¤‡: {validation['remaining_duplicates']}")
            print(f"   - ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {validation['unique_content_count']}")
            print(f"   - å‰Šé™¤æˆåŠŸ: {'âœ“' if validation['cleanup_successful'] else 'âœ—'}")
            
            return validation
            
        except Exception as e:
            print(f"âœ— æ¤œè¨¼å¤±æ•—: {e}")
            return {"error": str(e)}

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ§¹ ChromaDB é‡è¤‡å‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ  èµ·å‹•")
    print("=" * 50)
    
    # è¨­å®š
    collection_name = "mcp_production_knowledge"
    chromadb_path = r"F:\å‰¯æ¥­\VSC_WorkSpace\IrukaWorkspace\shared__ChromaDB_"
    
    # ãƒ‘ã‚¹ç¢ºèª
    if not Path(chromadb_path).exists():
        print(f"âœ— ChromaDBãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {chromadb_path}")
        sys.exit(1)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    cleanup_system = DuplicateCleanupSystem(chromadb_path)
    
    if not cleanup_system.initialize():
        sys.exit(1)
    
    if not cleanup_system.get_collection(collection_name):
        sys.exit(1)
    
    try:
        # STEP 1: é‡è¤‡æ¤œå‡º
        print("\nğŸ” STEP 1: é‡è¤‡æ¤œå‡º")
        dup_analysis = cleanup_system.detect_duplicates()
        if "error" in dup_analysis:
            print(f"é‡è¤‡æ¤œå‡ºå¤±æ•—: {dup_analysis['error']}")
            sys.exit(1)
        
        if dup_analysis["total_duplicates"] == 0:
            print("âœ… é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            sys.exit(0)
        
        # STEP 2: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        print("\nğŸ’¾ STEP 2: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
        backup_file = cleanup_system.create_backup(collection_name)
        if not backup_file:
            print("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ä¸­æ­¢")
            sys.exit(1)
        
        # STEP 3: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³
        print("\nğŸ§ª STEP 3: å‰Šé™¤ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³")
        dry_result = cleanup_system.execute_cleanup(dry_run=True)
        if "error" in dry_result:
            print(f"ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å¤±æ•—: {dry_result['error']}")
            sys.exit(1)
        
        # ç¢ºèª
        print(f"\nâ“ {dry_result['removed']}ä»¶ã®é‡è¤‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
        print(f"   å‰Šé™¤å¾Œ: {dry_result['final_count']}ä»¶")
        print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
        
        confirm = input("å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        
        if confirm == 'y':
            # STEP 4: å®Ÿéš›ã®å‰Šé™¤
            print("\nğŸ§¹ STEP 4: é‡è¤‡å‰Šé™¤å®Ÿè¡Œ")
            cleanup_result = cleanup_system.execute_cleanup(dry_run=False)
            
            if "error" in cleanup_result:
                print(f"å‰Šé™¤å¤±æ•—: {cleanup_result['error']}")
                sys.exit(1)
            
            # STEP 5: æ¤œè¨¼
            print("\nğŸ” STEP 5: å‰Šé™¤çµæœæ¤œè¨¼")
            validation = cleanup_system.validate_cleanup()
            
            # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
            print("\n" + "=" * 50)
            print("ğŸ“‹ é‡è¤‡å‰Šé™¤å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 50)
            print(f"å‰Šé™¤å‰: {cleanup_result['processed']}ä»¶")
            print(f"å‰Šé™¤æ¸ˆã¿: {cleanup_result['removed']}ä»¶")
            print(f"ä¿æŒæ¸ˆã¿: {cleanup_result['kept']}ä»¶")
            print(f"æœ€çµ‚ä»¶æ•°: {cleanup_result['final_count']}ä»¶")
            
            if "error" not in validation:
                print(f"æ®‹å­˜é‡è¤‡: {validation['remaining_duplicates']}ä»¶")
                print(f"å‰Šé™¤æˆåŠŸ: {'âœ… æˆåŠŸ' if validation['cleanup_successful'] else 'âŒ å¤±æ•—'}")
            
            print(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
            print("âœ… é‡è¤‡å‰Šé™¤å®Œäº†!")
        else:
            print("æ“ä½œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
    
    except KeyboardInterrupt:
        print("\næ“ä½œãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
